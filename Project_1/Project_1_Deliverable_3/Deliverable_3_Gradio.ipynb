{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcLNOHClOMTWWn0dCz8+W3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BodrulJalal/CS676-Algorithm-of-Data-Science/blob/main/Deliverable_3_Gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Installations:\n",
        "# ‚úÖ Install all required packages (including the latest Gemini SDK)\n",
        "# !pip install -U google-generativeai requests beautifulsoup4 tldextract python-dateutil lxml scikit-learn\n",
        "# ‚úÖ Install all required packages (silently)\n",
        "!pip install -q -U google-generativeai gradio_client requests beautifulsoup4 tldextract python-dateutil lxml scikit-learn > /dev/null 2>&1\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OHwpiPTOZ7z9",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Environment API Keys:\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"SERP_API_KEY\"] = \"\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"\"\n",
        "\n",
        "#real SERP_API_KEY\n",
        "# os.environ[\"SERP_API_KEY\"] = userdata.get('SERP_API_KEY')\n",
        "# os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "kzfWaqdIcaMG",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Credibility Scoring Functions:\n",
        "# ============================================================\n",
        "# ==========  C R E D I B I L I T Y   S C O R I N G  =========\n",
        "# ============================================================\n",
        "# This section is the *deliverable_1* rule-based scorer\n",
        "\n",
        "import re, time, json, math, tldextract, requests, random, hashlib\n",
        "from urllib.parse import urlparse\n",
        "from datetime import datetime\n",
        "from dateutil import parser as dateparser\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- Networking defaults ---\n",
        "USER_AGENT = 'Mozilla/5.0 (CredibilityPOC/0.1)'   # Browser-y UA to avoid blocks\n",
        "DEFAULT_TIMEOUT = 12                               # Seconds\n",
        "\n",
        "# --- Heuristic signals ---\n",
        "CLICKBAIT_TERMS = [\n",
        "    \"you won't believe\", 'shocking', 'jaw-dropping', 'what happened next',\n",
        "    'unbelievable', 'miracle', 'exposed', \"secret they don't want you to know\"\n",
        "]\n",
        "TRANSPARENCY_HINTS = [\n",
        "    'author','byline','by ','by:','written by','editor','editorial',\n",
        "    'fact-check','fact check','sources','references','citations',\n",
        "    'methodology','about us','about the author','corrections','disclosures'\n",
        "]\n",
        "INSTITUTIONAL_TLDS = {'edu','gov','ac','sch','mil'}\n",
        "\n",
        "def fetch_html(url: str):\n",
        "    \"\"\"\n",
        "    Fetch raw HTML for a URL with retries and basic anti-block heuristics.\n",
        "    - Rotates realistic headers\n",
        "    - Exponential backoff on 403/429/5xx\n",
        "    - Fallback for Reddit: try old.reddit.com if www.reddit.com blocks\n",
        "    Returns (html_text, None) on success, or (None, 'error message') on failure.\n",
        "    \"\"\"\n",
        "    import random\n",
        "    import time\n",
        "    import requests\n",
        "    from urllib.parse import urlparse\n",
        "\n",
        "    # Primary + backup user-agent/header sets\n",
        "    header_candidates = [\n",
        "        {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                          \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                          \"Chrome/126.0 Safari/537.36\",\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "            \"Connection\": \"keep-alive\",\n",
        "        },\n",
        "        {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
        "                          \"AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\",\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        },\n",
        "        {\n",
        "            \"User-Agent\": USER_AGENT,  # your original UA, as a last resort\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    def reddit_alt(u: str) -> list[str]:\n",
        "        \"\"\"If it's a Reddit URL, also try old.reddit.com.\"\"\"\n",
        "        try:\n",
        "            parsed = urlparse(u)\n",
        "            if parsed.netloc.endswith(\"reddit.com\") and not parsed.netloc.startswith(\"old.\"):\n",
        "                alt = u.replace(\"//www.reddit.com\", \"//old.reddit.com\")\n",
        "                if alt == u:  # if it wasn't www, still try old.\n",
        "                    alt = u.replace(\"//reddit.com\", \"//old.reddit.com\")\n",
        "                return [u, alt]\n",
        "        except Exception:\n",
        "            pass\n",
        "        return [u]\n",
        "\n",
        "    urls_to_try = reddit_alt(url)\n",
        "    last_err = None\n",
        "\n",
        "    for candidate_url in urls_to_try:\n",
        "        # up to 3 attempts per candidate url, with exponential backoff\n",
        "        backoff = 1.0\n",
        "        for attempt in range(3):\n",
        "            headers = header_candidates[min(attempt, len(header_candidates)-1)]\n",
        "            try:\n",
        "                resp = requests.get(candidate_url, headers=headers, timeout=DEFAULT_TIMEOUT, allow_redirects=True)\n",
        "                status = resp.status_code\n",
        "\n",
        "                # Retry on \"blocked\"/rate-limited or transient server errors\n",
        "                if status in (403, 429) or 500 <= status < 600:\n",
        "                    last_err = f\"{status} {resp.reason}\"\n",
        "                    time.sleep(backoff)\n",
        "                    backoff *= 2.0\n",
        "                    continue\n",
        "\n",
        "                resp.raise_for_status()\n",
        "\n",
        "                # Prefer HTML content; some sites return other types\n",
        "                ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
        "                if \"text/html\" not in ctype:\n",
        "                    last_err = f\"Non-HTML content-type: {ctype}\"\n",
        "                    # Don‚Äôt retry endlessly for non-HTML; move to next candidate/final\n",
        "                    break\n",
        "\n",
        "                return resp.text, None\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                last_err = str(e)\n",
        "                time.sleep(backoff)\n",
        "                backoff *= 2.0\n",
        "                continue\n",
        "\n",
        "    return None, f\"Fetch error: {last_err or 'unknown error'}\"\n",
        "\n",
        "\n",
        "def extract_article_fields(html: str, url: str):\n",
        "    \"\"\"\n",
        "    Parse HTML to extract: title, author, published date, body text (paragraphs),\n",
        "    and link counts (total + external), plus transparency hint flag.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    text_chunks, title, author, published = [], None, None, None\n",
        "\n",
        "    # --- Title from <title> or OG meta ---\n",
        "    if soup.title and soup.title.string:\n",
        "        title = soup.title.string.strip()\n",
        "    mt = soup.find('meta', attrs={'property':'og:title'}) or soup.find('meta', attrs={'name':'title'})\n",
        "    if not title and mt and mt.get('content'):\n",
        "        title = mt['content'].strip()\n",
        "\n",
        "    # --- Author / byline in common locations ---\n",
        "    for selector in [\n",
        "        {'name':'meta','attrs':{'name':'author'}},\n",
        "        {'name':'meta','attrs':{'property':'article:author'}},\n",
        "        {'name':'span','class_':re.compile('author|byline', re.I)},\n",
        "        {'name':'div','class_':re.compile('author|byline', re.I)},\n",
        "        {'name':'a','class_':re.compile('author', re.I)},\n",
        "    ]:\n",
        "        if selector['name']=='meta':\n",
        "            node = soup.find('meta', attrs=selector['attrs'])\n",
        "            if node and node.get('content'):\n",
        "                author = node['content'].strip(); break\n",
        "        else:\n",
        "            node = soup.find(selector['name'], class_=selector.get('class_'))\n",
        "            if node and node.get_text(strip=True):\n",
        "                candidate = node.get_text(' ', strip=True)\n",
        "                if len(candidate) >= 3:\n",
        "                    author = candidate; break\n",
        "\n",
        "    # --- Publish date in common meta/time/span patterns ---\n",
        "    for date_sel in [\n",
        "        {'name':'meta','attrs':{'property':'article:published_time'}},\n",
        "        {'name':'meta','attrs':{'name':'date'}},\n",
        "        {'name':'time','attrs':{}},\n",
        "        {'name':'span','class_':re.compile('date|time', re.I)},\n",
        "    ]:\n",
        "        if date_sel['name']=='meta':\n",
        "            node = soup.find('meta', attrs=date_sel['attrs'])\n",
        "            if node and node.get('content'):\n",
        "                try:\n",
        "                    published = dateparser.parse(node['content'], fuzzy=True); break\n",
        "                except Exception:\n",
        "                    pass\n",
        "        else:\n",
        "            node = soup.find(date_sel['name'], class_=date_sel.get('class_'))\n",
        "            if node and node.get_text(strip=True):\n",
        "                try:\n",
        "                    published = dateparser.parse(node.get_text(strip=True), fuzzy=True); break\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    # --- Body text: prefer a likely article container, else all <p> ---\n",
        "    main_container = None\n",
        "    for cls in ['article','post','story','content','entry-content','article-body']:\n",
        "        mc = soup.find(True, class_=re.compile(cls, re.I))\n",
        "        if mc: main_container = mc; break\n",
        "    paragraphs = (main_container.find_all('p') if main_container else soup.find_all('p'))\n",
        "    for p in paragraphs:\n",
        "        t = p.get_text(' ', strip=True)\n",
        "        if t and len(t) > 40: text_chunks.append(t)\n",
        "    article_text = '\\n\\n'.join(text_chunks)[:100000]  # cap to avoid huge pages\n",
        "\n",
        "    # --- Link counts: total & external ---\n",
        "    all_links, external_links = [], []\n",
        "    base_host = urlparse(url).netloc.lower()\n",
        "    for a in soup.find_all('a', href=True):\n",
        "        href = a['href']\n",
        "        if href.startswith('http://') or href.startswith('https://'):\n",
        "            all_links.append(href)\n",
        "            if urlparse(href).netloc.lower() != base_host:\n",
        "                external_links.append(href)\n",
        "\n",
        "    # --- Transparency hint flag (string match) ---\n",
        "    full_text_for_hints = (article_text + ' ' + ' '.join(TRANSPARENCY_HINTS)).lower()\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'author': author,\n",
        "        'published': published.isoformat() if published else None,\n",
        "        'text': article_text,\n",
        "        'num_paragraphs': len(text_chunks),\n",
        "        'all_links': all_links,\n",
        "        'external_links': external_links,\n",
        "        'has_transparency_hints': any(h in full_text_for_hints for h in TRANSPARENCY_HINTS),\n",
        "    }\n",
        "\n",
        "def score_url(url: str, fields: dict):\n",
        "    \"\"\"\n",
        "    Apply heuristic scoring rules ‚Üí (score 0‚Äì100, explanation string).\n",
        "    Starts at 50 and adds/subtracts per signal.\n",
        "    \"\"\"\n",
        "    explanation_bits = []\n",
        "    score = 50  # neutral baseline\n",
        "\n",
        "    # HTTPS\n",
        "    if url.lower().startswith('https://'):\n",
        "        score += 12; explanation_bits.append('+12: uses HTTPS')\n",
        "    else:\n",
        "        score -= 10; explanation_bits.append('-10: not using HTTPS')\n",
        "\n",
        "    # Institutional TLD\n",
        "    ext = tldextract.extract(url)\n",
        "    tld_last = (ext.suffix.split('.')[-1] if ext.suffix else '')\n",
        "    if tld_last in INSTITUTIONAL_TLDS:\n",
        "        score += 14; explanation_bits.append(f'+14: institutional TLD ({tld_last})')\n",
        "\n",
        "    # Author/byline\n",
        "    if fields.get('author'):\n",
        "        score += 10; explanation_bits.append('+10: author/byline found')\n",
        "    else:\n",
        "        score -= 6; explanation_bits.append('-6: no clear author/byline')\n",
        "\n",
        "    # Published recency (NOTE: uses datetime.utcnow(), may warn in 3.12+)\n",
        "    published = fields.get('published')\n",
        "    if published:\n",
        "        try:\n",
        "            dt = dateparser.parse(published)\n",
        "            if (datetime.utcnow() - dt).days <= 3650:\n",
        "                score += 6; explanation_bits.append('+6: reasonably recent publication date')\n",
        "            else:\n",
        "                score -= 4; explanation_bits.append('-4: appears quite old')\n",
        "        except Exception:\n",
        "            explanation_bits.append('0: could not parse publication date reliably')\n",
        "    else:\n",
        "        explanation_bits.append('0: no publication date found')\n",
        "\n",
        "    # References\n",
        "    total_links = len(fields.get('all_links', []))\n",
        "    external_links_count = len(fields.get('external_links', []))\n",
        "    if total_links >= 5 and external_links_count >= 3:\n",
        "        score += 10; explanation_bits.append(f'+10: provides references (links: {total_links}, external: {external_links_count})')\n",
        "    elif total_links >= 2:\n",
        "        score += 4; explanation_bits.append(f'+4: some references (links: {total_links})')\n",
        "    else:\n",
        "        score -= 6; explanation_bits.append(f'-6: minimal/no references (links: {total_links})')\n",
        "\n",
        "    # Length (by paragraph count)\n",
        "    num_paras = fields.get('num_paragraphs', 0)\n",
        "    if num_paras >= 8:\n",
        "        score += 6; explanation_bits.append('+6: substantive article length')\n",
        "    elif num_paras >= 3:\n",
        "        score += 2; explanation_bits.append('+2: moderate article length')\n",
        "    else:\n",
        "        score -= 6; explanation_bits.append('-6: very short article text')\n",
        "\n",
        "    # Clickbait language\n",
        "    text_lower = (fields.get('text') or '').lower()\n",
        "    clickbait_hits = sum(1 for term in CLICKBAIT_TERMS if term in text_lower)\n",
        "    if clickbait_hits >= 2:\n",
        "        score -= 10; explanation_bits.append('-10: strong clickbait indicators')\n",
        "    elif clickbait_hits == 1:\n",
        "        score -= 4; explanation_bits.append('-4: mild clickbait indicators')\n",
        "\n",
        "    # Advertising/sponsor cues\n",
        "    ad_signals = len(re.findall(r\"advertis(e|ement)|sponsor(ed|ship)\", text_lower))\n",
        "    iframes_penalty = min(8, math.floor(ad_signals / 5) * 2)\n",
        "    if iframes_penalty:\n",
        "        score -= iframes_penalty; explanation_bits.append(f'-{iframes_penalty}: advertising/sponsorship language')\n",
        "\n",
        "    # Clamp score and join explanation\n",
        "    score = max(0, min(100, int(round(score))))\n",
        "    explanation = '; '.join(explanation_bits)\n",
        "    return score, explanation\n",
        "\n",
        "def evaluate_url(url: str):\n",
        "    \"\"\"\n",
        "    Orchestrator: fetch ‚Üí parse ‚Üí score.\n",
        "    Returns a dict with 'score', 'explanation', and 'details' on success.\n",
        "    Returns None if the page cannot be accessed (STRICT EXCLUDE).\n",
        "    \"\"\"\n",
        "    if not isinstance(url, str) or not url.strip():\n",
        "        return None  # exclude invalid\n",
        "\n",
        "    html, err = fetch_html(url)\n",
        "    if err:\n",
        "        return None  # STRICT EXCLUDE: do not return a low score or an error object\n",
        "\n",
        "    fields = extract_article_fields(html, url)\n",
        "    score, explanation = score_url(url, fields)\n",
        "\n",
        "    return {\n",
        "        'score': score,\n",
        "        'explanation': explanation,\n",
        "        'details': {\n",
        "            'url': url,\n",
        "            'title': fields.get('title'),\n",
        "            'author': fields.get('author'),\n",
        "            'published': fields.get('published'),\n",
        "            'num_paragraphs': fields.get('num_paragraphs'),\n",
        "            'total_links': len(fields.get('all_links', [])),\n",
        "            'external_links': len(fields.get('external_links', [])),\n",
        "        },\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# ======  H Y B R I D   ( R u l e s  +  L i n e a r  R e g ) =\n",
        "# ============================================================\n",
        "# This section:\n",
        "# 1) builds features from evaluate_url() outputs\n",
        "# 2) creates a labeled dataset of 20 real URLs (0‚Äì100 labels)\n",
        "# 3) runs 5-fold cross-validation for LinearRegression\n",
        "# 4) fits a final model on ALL available rows\n",
        "# 5) exposes hybrid_score(url, alpha) for inference\n",
        "#\n",
        "# Notes:\n",
        "# - If some URLs fail to fetch, they're STRICTLY EXCLUDED (no score, not shown).\n",
        "# - CV uses MAE and R^2 to give you both error and fit quality.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from datetime import datetime, timezone\n",
        "from dateutil import parser as dateparser\n",
        "\n",
        "# ---------- Feature Engineering ----------\n",
        "def _safe(value, default=0):\n",
        "    return value if value is not None else default\n",
        "\n",
        "def _https_flag(url: str) -> int:\n",
        "    return 1 if isinstance(url, str) and url.lower().startswith('https://') else 0\n",
        "\n",
        "def _institutional_tld_flag(url: str) -> int:\n",
        "    try:\n",
        "        ext = tldextract.extract(url)\n",
        "        tld_last = (ext.suffix.split('.')[-1] if ext.suffix else '')\n",
        "        return 1 if tld_last in {'edu','gov','ac','sch','mil'} else 0\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "def _days_since(published_iso: str) -> int:\n",
        "    if not published_iso:\n",
        "        return 99999  # treat unknown/absent as very old\n",
        "    try:\n",
        "        dt = dateparser.parse(published_iso)\n",
        "        return max(0, (datetime.utcnow() - dt).days)  # OK to mirror your original utc usage\n",
        "    except Exception:\n",
        "        return 99999\n",
        "\n",
        "def features_from_eval(eval_obj: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Build a feature dict using ONLY fields the original scorer returns.\n",
        "    These features are model-agnostic and cheap to compute.\n",
        "    \"\"\"\n",
        "    d = eval_obj.get('details', {})\n",
        "    url = d.get('url') or ''\n",
        "    feats = {\n",
        "        \"https\": _https_flag(url),\n",
        "        \"inst_tld\": _institutional_tld_flag(url),\n",
        "        \"has_author\": 1 if d.get('author') else 0,\n",
        "        \"num_paragraphs\": _safe(d.get('num_paragraphs'), 0),\n",
        "        \"total_links\": _safe(d.get('total_links'), 0),\n",
        "        \"external_links\": _safe(d.get('external_links'), 0),\n",
        "        \"days_since_pub\": _days_since(d.get('published')),\n",
        "    }\n",
        "    return feats\n",
        "\n",
        "FEATURE_ORDER = [\n",
        "    \"https\",\n",
        "    \"inst_tld\",\n",
        "    \"has_author\",\n",
        "    \"num_paragraphs\",\n",
        "    \"total_links\",\n",
        "    \"external_links\",\n",
        "    \"days_since_pub\",\n",
        "]\n",
        "\n",
        "def vectorize_features(feat_dict: dict, feature_order=FEATURE_ORDER):\n",
        "    \"\"\"Convert a feature dict to a fixed-order numeric vector.\"\"\"\n",
        "    return np.array([feat_dict.get(k, 0) for k in feature_order], dtype=float)\n",
        "\n",
        "# ---------- Labeled Dataset (20 REAL URLs, mixed quality) ----------\n",
        "# Labels are illustrative 0‚Äì100 targets. Adjust as you refine ground truth.\n",
        "LABELED_URLS = [\n",
        "    # Highly credible / institutional / quality editorial\n",
        "    (\"https://www.cdc.gov/flu/about/index.html\", 92),\n",
        "    (\"https://www.nih.gov/news-events/news-releases\", 88),\n",
        "    (\"https://www.mayoclinic.org/diseases-conditions/dehydration/symptoms-causes/syc-20354086\", 85),\n",
        "    (\"https://www.who.int/news-room/fact-sheets/detail/diabetes\", 90),\n",
        "    (\"https://www.britannica.com/science/photosynthesis\", 86),\n",
        "    (\"https://www.health.harvard.edu/staying-healthy/what-is-intermittent-fasting\", 78),\n",
        "    (\"https://www.hopkinsmedicine.org/health/conditions-and-diseases\", 84),\n",
        "    (\"https://www.bbc.com/news/science_and_environment\", 80),\n",
        "    (\"https://www.reuters.com/world/us/\", 80),\n",
        "    (\"https://apnews.com/hub/technology\", 78),\n",
        "    (\"https://en.wikipedia.org/wiki/Ice_cream\", 75),\n",
        "\n",
        "    # Mid credibility consumer health / informative\n",
        "    (\"https://www.healthline.com/nutrition/green-tea-and-weight-loss\", 70),\n",
        "    (\"https://www.webmd.com/diet/obesity/features/green-tea-and-weight-loss\", 68),\n",
        "    (\"https://www.nature.com/scitable/definition/photosynthesis-288/\", 82),\n",
        "    (\"https://med.stanford.edu/news/all-news.html\", 82),\n",
        "\n",
        "    # Lower credibility / UGC / lighter editorial\n",
        "    (\"https://medium.com/\", 45),\n",
        "    (\"https://www.reddit.com/r/icecreamery/comments/19elt19/looking_for_resources_to_learn_how_to_make_ice/\", 20),\n",
        "    (\"https://www.quora.com/Is-green-tea-good-for-weight-loss\", 25),\n",
        "    (\"https://www.livestrong.com/article/13715706-green-tea-benefits/\", 60),\n",
        "    (\"https://www.buzzfeed.com/\", 40),\n",
        "]\n",
        "\n",
        "# ---------- Build Dataset by Evaluating & Featurizing ----------\n",
        "rows_X, rows_y = [], []\n",
        "\n",
        "for url, label in LABELED_URLS:\n",
        "    try:\n",
        "        ev = evaluate_url(url)\n",
        "        if ev is None:  # STRICT EXCLUDE\n",
        "            continue\n",
        "        feats = features_from_eval(ev)\n",
        "        x = vectorize_features(feats)\n",
        "        rows_X.append(x)\n",
        "        rows_y.append(float(label))\n",
        "    except Exception:\n",
        "        # Strict policy: exclude on any error\n",
        "        continue\n",
        "\n",
        "X = np.vstack(rows_X) if rows_X else np.zeros((0, len(FEATURE_ORDER)))\n",
        "y = np.array(rows_y) if rows_y else np.zeros((0,))\n",
        "\n",
        "print(f\"Prepared dataset: {X.shape[0]} rows √ó {X.shape[1]} features.\")\n",
        "\n",
        "# ---------- 5-Fold Cross-Validation ----------\n",
        "# We use KFold regression CV with two metrics:\n",
        "#  - MAE (lower is better): absolute error in points of the 0‚Äì100 score\n",
        "#  - R^2 (higher is better): variance explained\n",
        "if len(X) >= 5:\n",
        "    k = min(5, len(X))  # guard in case many rows were excluded\n",
        "    if k < 2:\n",
        "        print(\"\\nNot enough rows for CV; skipping cross-validation.\")\n",
        "    else:\n",
        "        kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "        # MAE (scikit returns negative MAE for loss; invert sign to report positive MAE)\n",
        "        mae_scores = cross_val_score(LinearRegression(), X, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
        "        r2_scores  = cross_val_score(LinearRegression(), X, y, cv=kf, scoring=\"r2\")\n",
        "        mae_vals = -mae_scores  # turn to positive\n",
        "        print(f\"\\n{k}-fold CV results on {len(X)} rows:\")\n",
        "        print(f\"  MAE per fold: {np.round(mae_vals, 2)} | mean={mae_vals.mean():.2f}\")\n",
        "        print(f\"  R^2 per fold:  {np.round(r2_scores, 3)} | mean={r2_scores.mean():.3f}\")\n",
        "else:\n",
        "    print(\"\\nNot enough rows for CV; need ‚â•5 examples.\")\n",
        "\n",
        "# ---------- Fit Final Linear Regression on ALL available rows ----------\n",
        "linreg = LinearRegression()\n",
        "if len(X) >= 2:\n",
        "    linreg.fit(X, y)\n",
        "    print(\"\\nFinal model (trained on all available rows):\")\n",
        "    for kname, w in zip(FEATURE_ORDER, linreg.coef_):\n",
        "        print(f\"  {kname:>16s}: {w: .3f}\")\n",
        "    print(f\"  {'Intercept':>16s}: {linreg.intercept_: .3f}\")\n",
        "else:\n",
        "    print(\"\\nNot enough rows to train final model (need ‚â•2). Using neutral ML predictions.\")\n",
        "\n",
        "# ---------- Inference helpers ----------\n",
        "def predict_ml_score_from_eval(eval_obj: dict) -> float:\n",
        "    \"\"\"\n",
        "    Predict a 0‚Äì100 credibility score from features using the trained LinearRegression.\n",
        "    Falls back to 50.0 if not enough training data is available.\n",
        "    \"\"\"\n",
        "    if len(X) < 2:\n",
        "        return 50.0\n",
        "    feats = features_from_eval(eval_obj)\n",
        "    x = vectorize_features(feats).reshape(1, -1)\n",
        "    pred = linreg.predict(x)[0]\n",
        "    return float(np.clip(pred, 0.0, 100.0))\n",
        "\n",
        "def _stars(score_0_100: float) -> str:\n",
        "    stars = int(round(score_0_100 / 20))\n",
        "    stars = max(0, min(5, stars))\n",
        "    return \"‚òÖ\"*stars + \"‚òÜ\"*(5 - stars)\n",
        "\n",
        "def hybrid_score(url: str, alpha: float = 0.6) -> dict | None:\n",
        "    \"\"\"\n",
        "    Blend rule-based score with ML-predicted score:\n",
        "        final = alpha * rule_score + (1 - alpha) * ml_score\n",
        "\n",
        "    STRICT EXCLUDE:\n",
        "    - If fetch/evaluation fails, return None (do not score or include).\n",
        "    \"\"\"\n",
        "    ev = evaluate_url(url)\n",
        "    if ev is None:\n",
        "        return None  # STRICT EXCLUDE\n",
        "\n",
        "    details = ev.get(\"details\", {})\n",
        "    rule_score = float(ev.get(\"score\", 0.0))\n",
        "\n",
        "    # Normal path: combine rules + ML\n",
        "    ml_score = predict_ml_score_from_eval(ev)\n",
        "    final = float(np.clip(alpha * rule_score + (1 - alpha) * ml_score, 0.0, 100.0))\n",
        "\n",
        "    # Stars\n",
        "    stars = int(round(final / 20))\n",
        "    stars = max(0, min(5, stars))\n",
        "    star_str = \"‚òÖ\" * stars + \"‚òÜ\" * (5 - stars)\n",
        "\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"title\": details.get(\"title\"),\n",
        "        \"rule_score\": round(rule_score, 1),\n",
        "        \"ml_score\": round(ml_score, 1),\n",
        "        \"hybrid_score\": round(final, 1),\n",
        "        \"stars\": star_str,\n",
        "        \"explanation\": ev.get(\"explanation\", \"No detailed explanation available.\"),\n",
        "        \"details\": details,\n",
        "    }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BTtvEQ7VR8vn",
        "outputId": "ebd799e3-8adf-43b4-d234-e841233e13fe",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared dataset: 14 rows √ó 7 features.\n",
            "\n",
            "5-fold CV results on 14 rows:\n",
            "  MAE per fold: [10.16 63.73 20.13 71.33 20.27] | mean=37.12\n",
            "  R^2 per fold:  [ 4.180000e-01 -6.031900e+01 -2.000000e-02 -1.332505e+03 -1.582500e+01] | mean=-281.650\n",
            "\n",
            "Final model (trained on all available rows):\n",
            "             https:  0.000\n",
            "          inst_tld:  22.205\n",
            "        has_author:  3.819\n",
            "    num_paragraphs:  0.133\n",
            "       total_links:  0.013\n",
            "    external_links: -0.039\n",
            "    days_since_pub: -0.000\n",
            "         Intercept:  79.834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SERP API Functions:\n",
        "# --- Improved SerpAPI search (clean results, de-dupe, filter, robust errors) ---\n",
        "import os, re, requests\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "SERP_API_KEY = os.getenv(\"SERP_API_KEY\")\n",
        "\n",
        "# Common low-signal/social/video domains to exclude by default (tune as needed)\n",
        "_DEFAULT_EXCLUDE_DOMAINS = {\n",
        "    \"reddit.com\", \"www.reddit.com\", \"old.reddit.com\",\n",
        "    \"x.com\", \"twitter.com\", \"www.twitter.com\",\n",
        "    \"tiktok.com\", \"www.tiktok.com\",\n",
        "    \"pinterest.com\", \"www.pinterest.com\",\n",
        "    \"facebook.com\", \"www.facebook.com\",\n",
        "    \"instagram.com\", \"www.instagram.com\",\n",
        "    \"youtube.com\", \"www.youtube.com\", \"youtu.be\"\n",
        "}\n",
        "\n",
        "# Skip obvious non-article filetypes\n",
        "_SKIP_FILETYPES = re.compile(r\"\\.(pdf|pptx?|docx?|xlsx?|zip|rar)(?:$|\\?)\", re.I)\n",
        "\n",
        "def _host(url: str) -> str:\n",
        "    return urlparse(url).netloc.lower()\n",
        "\n",
        "def search_google(\n",
        "    query: str,\n",
        "    num_results: int = 30,\n",
        "    exclude_domains: set | None = None,\n",
        "    allow_news_results: bool = True,\n",
        "    timeout: int = 20,\n",
        ") -> list[dict]:\n",
        "    \"\"\"\n",
        "    Search Google via SerpAPI and return a clean list of results:\n",
        "        [{\"title\": str, \"link\": str, \"snippet\": str}, ...]\n",
        "    - De-duplicates by link and avoids flooding from the same host\n",
        "    - Skips social/UGC/video sites and non-HTML filetypes\n",
        "    - Optionally includes Google News results\n",
        "    \"\"\"\n",
        "    if not SERP_API_KEY:\n",
        "        raise RuntimeError(\"Missing SERP_API_KEY / SERPAPI_API_KEY. Set it in the environment first.\")\n",
        "\n",
        "    exclude = set(_DEFAULT_EXCLUDE_DOMAINS)\n",
        "    if exclude_domains:\n",
        "        exclude |= set(exclude_domains)\n",
        "\n",
        "    params = {\n",
        "        \"engine\": \"google\",\n",
        "        \"q\": query,\n",
        "        \"api_key\": SERP_API_KEY,\n",
        "        \"num\": 30,          # pull extra, then filter/trim to num_results\n",
        "        \"hl\": \"en\",\n",
        "        \"gl\": \"us\",\n",
        "        \"safe\": \"active\",   # optional: reduce NSFW\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        resp = requests.get(\"https://serpapi.com/search\", params=params, timeout=timeout)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "    except Exception as e:\n",
        "        # On failure, return empty list (caller can handle and message user)\n",
        "        print(f\"[SerpAPI] Error: {e}\")\n",
        "        return []\n",
        "\n",
        "    candidates: list[dict] = []\n",
        "\n",
        "    # 1) Organic results\n",
        "    for r in (data.get(\"organic_results\") or []):\n",
        "        link = r.get(\"link\")\n",
        "        title = r.get(\"title\")\n",
        "        snippet = r.get(\"snippet\") or \"\"\n",
        "        if not link or _SKIP_FILETYPES.search(link):\n",
        "            continue\n",
        "        host = _host(link)\n",
        "        if host in exclude:\n",
        "            continue\n",
        "        candidates.append({\"title\": title, \"link\": link, \"snippet\": snippet})\n",
        "\n",
        "    # 2) Optional: News results (helpful for timely topics)\n",
        "    if allow_news_results:\n",
        "        for r in (data.get(\"news_results\") or []):\n",
        "            link = r.get(\"link\")\n",
        "            title = r.get(\"title\")\n",
        "            snippet = r.get(\"snippet\") or \"\"\n",
        "            if not link or _SKIP_FILETYPES.search(link):\n",
        "                continue\n",
        "            host = _host(link)\n",
        "            if host in exclude:\n",
        "                continue\n",
        "            candidates.append({\"title\": title, \"link\": link, \"snippet\": snippet})\n",
        "\n",
        "    # De-duplicate by link; also avoid over-representing a single host\n",
        "    cleaned, seen_links, seen_hosts = [], set(), set()\n",
        "    for c in candidates:\n",
        "        link, host = c[\"link\"], _host(c[\"link\"])\n",
        "        if link in seen_links:\n",
        "            continue\n",
        "        # If we already have enough and this host is duplicate, skip\n",
        "        if host in seen_hosts and len(cleaned) >= num_results:\n",
        "            continue\n",
        "        seen_links.add(link)\n",
        "        seen_hosts.add(host)\n",
        "        cleaned.append(c)\n",
        "        if len(cleaned) >= num_results:\n",
        "            break\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "def credbot_summarize_text(text: str, query: str = \"\", max_chars: int = 500) -> str:\n",
        "    \"\"\"\n",
        "    Super-light extractive summary:\n",
        "      1) Split into sentences.\n",
        "      2) Score sentences by presence of query keywords + informative cues.\n",
        "      3) Pick top sentences until max_chars is reached.\n",
        "    Falls back to the first 2‚Äì3 sentences if scoring yields little.\n",
        "    \"\"\"\n",
        "\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Normalize whitespace\n",
        "    clean = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # Sentence split (simple, robust)\n",
        "    sentences = re.split(r\"(?<=[.!?])\\s+(?=[A-Z0-9\\\"'])\", clean)\n",
        "    sentences = [s.strip() for s in sentences if len(s.strip()) > 0]\n",
        "\n",
        "    if not sentences:\n",
        "        return clean[:max_chars]\n",
        "\n",
        "    # Prepare query keywords\n",
        "    q = (query or \"\").lower().strip()\n",
        "    q_terms = [t for t in re.split(r\"[^a-z0-9]+\", q) if len(t) >= 3]\n",
        "    q_terms = list(dict.fromkeys(q_terms))  # unique order\n",
        "\n",
        "    # Score sentences\n",
        "    scored = []\n",
        "    for i, s in enumerate(sentences):\n",
        "        sl = s.lower()\n",
        "\n",
        "        # keyword matches (weighted)\n",
        "        kw_hits = sum(1 for t in q_terms if t in sl)\n",
        "        kw_score = 3 * kw_hits\n",
        "\n",
        "        # informative cues\n",
        "        cues = 0\n",
        "        if len(s) > 80: cues += 1\n",
        "        if \":\" in s or \";\" in s or \"(\" in s or \")\" in s: cues += 1\n",
        "        if re.search(r\"\\b(according to|study|report|data|evidence|research|analysis|conclude|results)\\b\", sl):\n",
        "            cues += 2\n",
        "\n",
        "        # slight lead bias\n",
        "        lead_bonus = max(0, 3 - i)  # first few sentences get a small boost\n",
        "\n",
        "        score = kw_score + cues + 0.2 * lead_bonus\n",
        "        scored.append((score, i, s))\n",
        "\n",
        "    # Pick best sentences until we hit the budget\n",
        "    picked = []\n",
        "    total = 0\n",
        "    for _, _, s in sorted(scored, key=lambda x: (-x[0], x[1])):  # high score, then original order\n",
        "        if s in picked:\n",
        "            continue\n",
        "        if total + len(s) + (2 if picked else 0) > max_chars:\n",
        "            # if nothing picked yet, at least include a truncated sentence\n",
        "            if not picked:\n",
        "                picked.append(s[:max_chars].rstrip() + \"‚Ä¶\")\n",
        "                total = max_chars\n",
        "            break\n",
        "        picked.append(s)\n",
        "        total += len(s) + (2 if len(picked) > 1 else 0)\n",
        "\n",
        "        # Aim for ~3 sentences when possible\n",
        "        if len(picked) >= 3:\n",
        "            break\n",
        "\n",
        "    # Fallback to lead-2/3 if scoring produced very little\n",
        "    if len(picked) == 0:\n",
        "        lead = []\n",
        "        for s in sentences[:3]:\n",
        "            if sum(len(x) for x in lead) + len(s) + (2 if lead else 0) <= max_chars:\n",
        "                lead.append(s)\n",
        "        picked = lead or [sentences[0][:max_chars].rstrip() + \"‚Ä¶\"]\n",
        "\n",
        "    return \"  \".join(picked)"
      ],
      "metadata": {
        "id": "dMWhH62SWyQO",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Gemini Chat Integration: (pinned to gemini-2.5-flash only)\n",
        "import os, re, json, google.generativeai as genai\n",
        "\n",
        "DEBUG = False  # set True for light diagnostics\n",
        "\n",
        "# ---------- Gemini setup ----------\n",
        "for _k in [\"GOOGLE_API_BASE_URL\", \"GOOGLE_AI_API_BASE\", \"GOOGLE_API_ENDPOINT\"]:\n",
        "    os.environ.pop(_k, None)\n",
        "\n",
        "_GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "if not _GEMINI_API_KEY:\n",
        "    # raise RuntimeError(\"Missing GEMINI_API_KEY. Please set it in the environment first.\")\n",
        "    print(\"Missing GEMINI_API_KEY. Please set it in the environment first.\")\n",
        "\n",
        "genai.configure(api_key=_GEMINI_API_KEY, client_options={\"api_endpoint\": \"https://generativelanguage.googleapis.com\"})\n",
        "\n",
        "# Pin to a single model; no discovery/filters.\n",
        "MODEL_NAME = \"gemini-2.5-flash\"\n",
        "try:\n",
        "    _GEMINI = genai.GenerativeModel(model_name=MODEL_NAME)\n",
        "    if DEBUG:\n",
        "        try:\n",
        "            _ = _GEMINI.generate_content(\"ping\")\n",
        "            print(f\"[Gemini] {MODEL_NAME} ready.\")\n",
        "        except Exception as e:\n",
        "            print(f\"[Gemini] Probe warning on {MODEL_NAME}: {e}\")\n",
        "except Exception as e:\n",
        "    _GEMINI = None\n",
        "    if DEBUG:\n",
        "        print(f\"[Gemini] Init failed for {MODEL_NAME}: {e}\")\n",
        "\n",
        "def _gemini_text(prompt: str) -> str | None:\n",
        "    \"\"\"Minimal safe wrapper for a single pinned model.\"\"\"\n",
        "    if _GEMINI is None:\n",
        "        if DEBUG: print(\"[Gemini] Model unavailable.\")\n",
        "        return None\n",
        "    try:\n",
        "        resp = _GEMINI.generate_content(prompt)\n",
        "        out = getattr(resp, \"text\", None)\n",
        "        if not out and hasattr(resp, \"candidates\") and resp.candidates:\n",
        "            cand = resp.candidates[0]\n",
        "            if hasattr(cand, \"content\") and cand.content and getattr(cand.content, \"parts\", None):\n",
        "                out = cand.content.parts[0].text\n",
        "        if DEBUG: print(f\"[Gemini] Response present: {bool(out)}\")\n",
        "        return out.strip() if out else None\n",
        "    except Exception as e:\n",
        "        if DEBUG: print(f\"[Gemini] Call failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def _clip_summary(s):\n",
        "    if not s: return \"\"\n",
        "    parts = re.split(r'(?<=[.!?])\\s+', s.strip())\n",
        "    return \" \".join(parts[:3])[:350]\n",
        "\n",
        "def _parse_json_block(text):\n",
        "    if not text: return None\n",
        "    m = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, flags=re.S)\n",
        "    if m:\n",
        "        try: return json.loads(m.group(1))\n",
        "        except: pass\n",
        "    m = re.search(r\"(\\{.*\\})\", text, flags=re.S)\n",
        "    if m:\n",
        "        try: return json.loads(m.group(1))\n",
        "        except: pass\n",
        "    return None\n",
        "\n",
        "# ---------- Project-provided helpers expected earlier ----------\n",
        "# - search_google(query, num_results=10, exclude_domains=set())\n",
        "# - fetch_html(url) -> (html, err)\n",
        "# - extract_article_fields(html, url) -> dict(...)\n",
        "# - hybrid_score(url, alpha=0.6) -> dict(...)\n",
        "# - credbot_summarize_text(text, query) -> str\n",
        "\n",
        "def _score_and_summarize_url(link, query):\n",
        "    try:\n",
        "        hybrid = hybrid_score(link, alpha=0.6)\n",
        "        if hybrid is None:\n",
        "            return False\n",
        "        html, err = fetch_html(link)\n",
        "        if err or not html:\n",
        "            return False\n",
        "        fields = extract_article_fields(html, link)\n",
        "        text = (fields.get(\"text\") or \"\").strip()\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        summary = credbot_summarize_text(text, query)\n",
        "\n",
        "        title = fields.get(\"title\") or \"[No Title]\"\n",
        "        score = hybrid.get(\"hybrid_score\", 0.0)\n",
        "        stars = hybrid.get(\"stars\") or (\"‚òÖ\" * int(round(score / 20)))\n",
        "        why = (hybrid.get(\"explanation\") or \"\").strip()\n",
        "\n",
        "        print(f\"\\nüì∞ {title}\\nüîó {link}\\nüìÑ {summary}\\n‚≠ê {stars} ({score}/100)\")\n",
        "        if why:\n",
        "            print(f\"üìù {why}\\n\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[Score] Exception on {link}: {e}\")\n",
        "        return False\n",
        "\n",
        "def _run_search_flow(topic):\n",
        "    print(f\"üîç Searching for sources on ‚Äú{topic}‚Äù‚Ä¶\")\n",
        "    try:\n",
        "        results = search_google(\n",
        "            \"articles on \" + topic,\n",
        "            num_results=5,\n",
        "            exclude_domains={},\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Search error: {e}\")\n",
        "        return 0\n",
        "\n",
        "    if not results:\n",
        "        print(\"‚ö†Ô∏è No relevant articles found.\")\n",
        "        return 0\n",
        "\n",
        "    shown = 0\n",
        "    for r in results:\n",
        "        link = r.get(\"link\")\n",
        "        if link and _score_and_summarize_url(link, topic):\n",
        "            shown += 1\n",
        "\n",
        "    if shown == 0:\n",
        "        print(\"‚ö†Ô∏è None accessible.\")\n",
        "    return shown\n",
        "\n",
        "A_CHAT, A_RUN = \"chat_mode\", \"run_search\"\n",
        "\n",
        "_SYSTEM_PROMPT = (\n",
        "    \"You are CredBot, a concise and friendly assistant.\\n\"\n",
        "    \"Return ONLY a JSON object with keys: reply, summary, topic, action.\\n\"\n",
        "    f'action must be \\\"{A_CHAT}\\\" or \\\"{A_RUN}\\\".\\n'\n",
        "    \"- You (the model) decide greetings, topic extraction, and whether a web search is necessary.\\n\"\n",
        "    f\"- If a web search is needed, set action=\\\"{A_RUN}\\\" and provide a concise search query in \\\"topic\\\".\\n\"\n",
        "    \"- Keep reply ‚â§ 2 sentences. Keep summary to 2‚Äì3 sentences (‚â§350 chars).\\n\"\n",
        "    \"- Do not include backticks or extra text around the JSON.\"\n",
        ")\n",
        "\n",
        "def gemini_step(user_input, prev_summary, prev_topic):\n",
        "    prompt = (\n",
        "        f\"{_SYSTEM_PROMPT}\\n\\n\"\n",
        "        f\"Previous summary:\\n{prev_summary or ''}\\n\\n\"\n",
        "        f\"Previous topic:\\n{prev_topic or ''}\\n\\n\"\n",
        "        f\"User message:\\n{user_input}\\n\\n\"\n",
        "        'Example response format:\\n'\n",
        "        '{\"reply\":\"...\", \"summary\":\"...\", \"topic\":\"...\", \"action\":\"chat_mode\"}'\n",
        "    )\n",
        "\n",
        "    raw = _gemini_text(prompt)\n",
        "    data = _parse_json_block(raw) if raw else None\n",
        "    if DEBUG:\n",
        "        print(f\"[Gemini] Raw present: {bool(raw)} | JSON parsed: {isinstance(data, dict)}\")\n",
        "\n",
        "    if not isinstance(data, dict):\n",
        "        return (\"Please Double Check Your API Keys\",\n",
        "                prev_summary or \"\", prev_topic or \"\", A_CHAT)\n",
        "\n",
        "    reply    = (data.get(\"reply\") or \"\").strip() or \"Okay.\"\n",
        "    summary  = _clip_summary(data.get(\"summary\") or \"\")\n",
        "    topic    = (data.get(\"topic\") or \"\").strip()\n",
        "    action   = (data.get(\"action\") or A_CHAT).strip()\n",
        "    if action not in {A_CHAT, A_RUN}:\n",
        "        action = A_CHAT\n",
        "\n",
        "    return reply, summary, topic, action\n",
        "\n",
        "def run_chat():\n",
        "    print(\"ü§ñ Hi, I‚Äôm CredBot. We can chat normally. (Type 'exit' to quit.)\")\n",
        "    if DEBUG: print(f\"[Startup] Gemini ready: {bool(_GEMINI)}\")\n",
        "\n",
        "    summary, topic = \"\", \"\"\n",
        "\n",
        "    while True:\n",
        "        user = input(\"\\nYou: \").strip()\n",
        "        if not user:\n",
        "            continue\n",
        "        if user.lower() in {\"exit\",\"quit\",\"bye\"}:\n",
        "            print(\"üëã Goodbye!\")\n",
        "            break\n",
        "\n",
        "        reply, summary_out, topic_out, action = gemini_step(user, summary, topic)\n",
        "\n",
        "        if action == A_RUN:\n",
        "            run_topic = topic_out or topic or user\n",
        "            shown = _run_search_flow(run_topic)\n",
        "            if shown:\n",
        "                summary = _clip_summary((summary_out or \"\") + f\" Retrieved sources on {run_topic}.\")\n",
        "            else:\n",
        "                summary = summary_out or summary\n",
        "            topic = topic_out or topic\n",
        "            print(\"ü§ñ Anything else to explore?\")\n",
        "            print(f\"üß† [summary] {summary}\\nüéØ [topic] {topic}\\n‚öôÔ∏è [action] chat_mode\")\n",
        "            continue\n",
        "\n",
        "        print(f\"ü§ñ {reply}\")\n",
        "        summary = summary_out or summary\n",
        "        topic = topic_out or topic\n",
        "\n",
        "        if DEBUG:\n",
        "            print(f\"üß† [summary] {summary}\\nüéØ [topic] {topic}\\n‚öôÔ∏è [action] {A_CHAT}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "j2iKouHWLoHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e1d3a12-a31e-4404-e975-48bfeacc57ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing GEMINI_API_KEY. Please set it in the environment first.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ---- Gradio Setup + Chat (search results go into chat; links clickable; run LAST) ----\n",
        "from __future__ import annotations\n",
        "import os, io, re, contextlib, gradio as gr, google.generativeai as genai\n",
        "\n",
        "DEBUG = False\n",
        "MODEL_NAME = \"gemini-2.5-flash\"\n",
        "\n",
        "try:\n",
        "    _GEMINI\n",
        "except NameError:\n",
        "    _GEMINI = None\n",
        "\n",
        "# ---- Keys ----\n",
        "def _set_env_keys(serp_key: str | None, gemini_key: str | None) -> None:\n",
        "    if serp_key and serp_key.strip():\n",
        "        val = serp_key.strip()\n",
        "        os.environ[\"SERP_API_KEY\"] = val\n",
        "        os.environ[\"SERPAPI_API_KEY\"] = val\n",
        "        globals()[\"SERP_API_KEY\"] = val  # refresh cached var from SERP cell\n",
        "    if gemini_key and gemini_key.strip():\n",
        "        os.environ[\"GEMINI_API_KEY\"] = gemini_key.strip()\n",
        "\n",
        "def _get_serp_key() -> str | None:\n",
        "    return (os.getenv(\"SERP_API_KEY\") or os.getenv(\"SERPAPI_API_KEY\") or \"\").strip() or None\n",
        "\n",
        "# ---- Gemini ----\n",
        "def _init_gemini_from_env() -> tuple[bool, str]:\n",
        "    global _GEMINI\n",
        "    for _k in [\"GOOGLE_API_BASE_URL\", \"GOOGLE_AI_API_BASE\", \"GOOGLE_API_ENDPOINT\"]:\n",
        "        os.environ.pop(_k, None)\n",
        "    gem_key = os.getenv(\"GEMINI_API_KEY\", \"\").strip()\n",
        "    if not gem_key:\n",
        "        _GEMINI = None\n",
        "        return False, \"‚ùå Missing GEMINI_API_KEY.\"\n",
        "    try:\n",
        "        genai.configure(api_key=gem_key, client_options={\"api_endpoint\": \"https://generativelanguage.googleapis.com\"})\n",
        "        m = genai.GenerativeModel(model_name=MODEL_NAME)\n",
        "        try:\n",
        "            _ = m.generate_content(\"ping\")\n",
        "        except Exception as e:\n",
        "            if DEBUG: print(f\"[Gemini] probe warn: {e}\")\n",
        "        _GEMINI = m\n",
        "        return True, f\"‚úÖ Gemini ready ({MODEL_NAME}). Please Continue to Chat Tab or Return Back to Setup to Update API Keys\"\n",
        "    except Exception as e:\n",
        "        _GEMINI = None\n",
        "        return False, f\"‚ùå Gemini init failed: {e}\"\n",
        "\n",
        "# ---- Readiness ----\n",
        "def _training_ready() -> bool:\n",
        "    try:\n",
        "        return hasattr(linreg, \"coef_\") and len(getattr(linreg, \"coef_\", [])) > 0\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _overall_ready() -> bool:\n",
        "    return bool(_training_ready() and _GEMINI is not None)\n",
        "\n",
        "def _status_text() -> str:\n",
        "    rows = int(X.shape[0]) if \"X\" in globals() else 0\n",
        "    has_serp = bool(_get_serp_key())\n",
        "    has_gem  = bool(os.getenv(\"GEMINI_API_KEY\", \"\").strip())\n",
        "    return (\n",
        "        f\"Training: {'‚úÖ' if _training_ready() else '‚ùå'} (rows={rows}) ‚Ä¢ \"\n",
        "        f\"GEMINI_API_KEY: {'‚úÖ' if has_gem else '‚ùå'} ‚Ä¢ \"\n",
        "        f\"SERP_API_KEY: {'‚úÖ' if has_serp else '‚ö†Ô∏è optional'} ‚Ä¢ \"\n",
        "        f\"Gemini: {'‚úÖ' if _GEMINI is not None else '‚ùå'}\"\n",
        "    )\n",
        "\n",
        "# ---- Capture printed search for embedding into chat ----\n",
        "def _run_search_and_capture(topic: str) -> str:\n",
        "    buf = io.StringIO()\n",
        "    with contextlib.redirect_stdout(buf):\n",
        "        try:\n",
        "            _run_search_flow(topic)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Search error: {e}\")\n",
        "    return buf.getvalue().strip()\n",
        "\n",
        "# ---- Make URLs clickable in Markdown ----\n",
        "def _linkify_urls(text: str) -> str:\n",
        "    # Replace any plain URL with a markdown link\n",
        "    return re.sub(r'(https?://[^\\s<>\\]\\)]+)', r'[\\1](\\1)', text)\n",
        "\n",
        "# ---- UI callbacks ----\n",
        "def on_save_keys(serp_key: str, gemini_key: str):\n",
        "    _set_env_keys(serp_key, gemini_key)\n",
        "    ok, gem_status = _init_gemini_from_env()\n",
        "    ready = _overall_ready()\n",
        "    banner = _status_text()\n",
        "    interactive = gr.update(interactive=ready)\n",
        "    cleared = gr.update(value=\"\")\n",
        "    return banner, gem_status, cleared, cleared, interactive, interactive\n",
        "\n",
        "def chat_handler(user_msg: str, chat_hist, summary: str, topic: str):\n",
        "    if not _overall_ready():\n",
        "        return user_msg, chat_hist or [], summary, topic\n",
        "    if not user_msg.strip():\n",
        "        return \"\", chat_hist or [], summary, topic\n",
        "\n",
        "    reply, summary_out, topic_out, action = gemini_step(user_msg, summary, topic)\n",
        "\n",
        "    bot_msg = reply\n",
        "    if action == \"run_search\":\n",
        "        run_topic = topic_out or topic or user_msg\n",
        "        raw_log = _run_search_and_capture(run_topic)\n",
        "        if raw_log:\n",
        "            # Convert URLs to clickable links and show nicely\n",
        "            log_md = _linkify_urls(raw_log)\n",
        "            bot_msg = (\n",
        "                f\"{reply}\\n\\n\"\n",
        "                f\"<details><summary><b>[Click to Expand List] Search results for:</b> <i>{run_topic}</i></summary>\\n\\n\"\n",
        "                f\"{log_md}\\n\\n\"\n",
        "                f\"</details>\"\n",
        "            )\n",
        "        summary = _clip_summary((summary_out or summary) + f\" Retrieved sources on {run_topic}.\")\n",
        "        topic = topic_out or topic\n",
        "    else:\n",
        "        summary = summary_out or summary\n",
        "        topic = topic_out or topic\n",
        "\n",
        "    chat_hist = (chat_hist or []) + [[user_msg, bot_msg]]\n",
        "    return \"\", chat_hist, summary, topic\n",
        "\n",
        "# ---- UI ----\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ü§ñ CredBot ‚Äî Setup & Chat (Gemini 2.5 Flash)\")\n",
        "    status_md = gr.Markdown(_status_text())\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.Tab(\"üîê Setup\"):\n",
        "            gr.Markdown(\"Enter your API keys. **Values are hidden** and not printed.\")\n",
        "            with gr.Row():\n",
        "                serp_input = gr.Textbox(\n",
        "                    label=\"SERP_API_KEY (for web search)\",\n",
        "                    type=\"password\",\n",
        "                    placeholder=\"Paste SERP_API_KEY‚Ä¶\",\n",
        "                    scale=1,\n",
        "                )\n",
        "                gem_input = gr.Textbox(\n",
        "                    label=\"GEMINI_API_KEY (required)\",\n",
        "                    type=\"password\",\n",
        "                    placeholder=\"Paste GEMINI_API_KEY‚Ä¶\",\n",
        "                    scale=1,\n",
        "                )\n",
        "            with gr.Row():\n",
        "                save_btn = gr.Button(\"Save Keys & Initialize\", variant=\"primary\")\n",
        "            setup_status_md = gr.Markdown(\"\")\n",
        "\n",
        "        with gr.Tab(\"üí¨ Chat\"):\n",
        "            chatbot = gr.Chatbot(height=480, label=\"Dialogue\")  # search appears inside the chat now\n",
        "            with gr.Row():\n",
        "                msg = gr.Textbox(\n",
        "                    placeholder=\"Type your message‚Ä¶\",\n",
        "                    scale=4,\n",
        "                    autofocus=True,\n",
        "                    interactive=_overall_ready(),\n",
        "                )\n",
        "                send = gr.Button(\n",
        "                    \"Send\",\n",
        "                    variant=\"primary\",\n",
        "                    scale=1,\n",
        "                    interactive=_overall_ready(),\n",
        "                )\n",
        "\n",
        "    # memory\n",
        "    state_summary = gr.State(\"\")\n",
        "    state_topic = gr.State(\"\")\n",
        "\n",
        "    # wiring\n",
        "    save_btn.click(\n",
        "        on_save_keys,\n",
        "        inputs=[serp_input, gem_input],\n",
        "        outputs=[status_md, setup_status_md, serp_input, gem_input, msg, send],\n",
        "    )\n",
        "    send.click(\n",
        "        chat_handler,\n",
        "        inputs=[msg, chatbot, state_summary, state_topic],\n",
        "        outputs=[msg, chatbot, state_summary, state_topic],\n",
        "    )\n",
        "    msg.submit(\n",
        "        chat_handler,\n",
        "        inputs=[msg, chatbot, state_summary, state_topic],\n",
        "        outputs=[msg, chatbot, state_summary, state_topic],\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "BkKr7db0JDhD",
        "outputId": "7682b15a-38ac-48b1-c2d5-ca10b5e642cc",
        "cellView": "form"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3ad9ef159fc66410f4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3ad9ef159fc66410f4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tg-SOpaZiBIW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}