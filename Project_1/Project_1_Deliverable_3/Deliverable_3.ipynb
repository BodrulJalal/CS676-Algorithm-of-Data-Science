{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGQI7xzwoybPW/VxxkseFO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BodrulJalal/CS676-Algorithm-of-Data-Science/blob/main/Deliverable_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Installations:\n",
        "# ‚úÖ Install all required packages (including the latest Gemini SDK)\n",
        "# !pip install -U google-generativeai requests beautifulsoup4 tldextract python-dateutil lxml scikit-learn\n",
        "# ‚úÖ Install all required packages (silently)\n",
        "!pip install -q -U google-generativeai requests beautifulsoup4 tldextract python-dateutil lxml scikit-learn > /dev/null 2>&1\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OHwpiPTOZ7z9",
        "cellView": "form"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Environment API Keys:\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# os.environ[\"SERP_API_KEY\"] = \"test\"\n",
        "\n",
        "#real SERP_API_KEY\n",
        "os.environ[\"SERP_API_KEY\"] = userdata.get('SERP_API_KEY')\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "kzfWaqdIcaMG",
        "cellView": "form"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Credibility Scoring Functions:\n",
        "# ============================================================\n",
        "# ==========  C R E D I B I L I T Y   S C O R I N G  =========\n",
        "# ============================================================\n",
        "# This section is the *deliverable_1* rule-based scorer\n",
        "\n",
        "import re, time, json, math, tldextract, requests, random, hashlib\n",
        "from urllib.parse import urlparse\n",
        "from datetime import datetime\n",
        "from dateutil import parser as dateparser\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- Networking defaults ---\n",
        "USER_AGENT = 'Mozilla/5.0 (CredibilityPOC/0.1)'   # Browser-y UA to avoid blocks\n",
        "DEFAULT_TIMEOUT = 12                               # Seconds\n",
        "\n",
        "# --- Heuristic signals ---\n",
        "CLICKBAIT_TERMS = [\n",
        "    \"you won't believe\", 'shocking', 'jaw-dropping', 'what happened next',\n",
        "    'unbelievable', 'miracle', 'exposed', \"secret they don't want you to know\"\n",
        "]\n",
        "TRANSPARENCY_HINTS = [\n",
        "    'author','byline','by ','by:','written by','editor','editorial',\n",
        "    'fact-check','fact check','sources','references','citations',\n",
        "    'methodology','about us','about the author','corrections','disclosures'\n",
        "]\n",
        "INSTITUTIONAL_TLDS = {'edu','gov','ac','sch','mil'}\n",
        "\n",
        "def fetch_html(url: str):\n",
        "    \"\"\"\n",
        "    Fetch raw HTML for a URL with retries and basic anti-block heuristics.\n",
        "    - Rotates realistic headers\n",
        "    - Exponential backoff on 403/429/5xx\n",
        "    - Fallback for Reddit: try old.reddit.com if www.reddit.com blocks\n",
        "    Returns (html_text, None) on success, or (None, 'error message') on failure.\n",
        "    \"\"\"\n",
        "    import random\n",
        "    import time\n",
        "    import requests\n",
        "    from urllib.parse import urlparse\n",
        "\n",
        "    # Primary + backup user-agent/header sets\n",
        "    header_candidates = [\n",
        "        {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                          \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                          \"Chrome/126.0 Safari/537.36\",\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "            \"Connection\": \"keep-alive\",\n",
        "        },\n",
        "        {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
        "                          \"AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\",\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        },\n",
        "        {\n",
        "            \"User-Agent\": USER_AGENT,  # your original UA, as a last resort\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    def reddit_alt(u: str) -> list[str]:\n",
        "        \"\"\"If it's a Reddit URL, also try old.reddit.com.\"\"\"\n",
        "        try:\n",
        "            parsed = urlparse(u)\n",
        "            if parsed.netloc.endswith(\"reddit.com\") and not parsed.netloc.startswith(\"old.\"):\n",
        "                alt = u.replace(\"//www.reddit.com\", \"//old.reddit.com\")\n",
        "                if alt == u:  # if it wasn't www, still try old.\n",
        "                    alt = u.replace(\"//reddit.com\", \"//old.reddit.com\")\n",
        "                return [u, alt]\n",
        "        except Exception:\n",
        "            pass\n",
        "        return [u]\n",
        "\n",
        "    urls_to_try = reddit_alt(url)\n",
        "    last_err = None\n",
        "\n",
        "    for candidate_url in urls_to_try:\n",
        "        # up to 3 attempts per candidate url, with exponential backoff\n",
        "        backoff = 1.0\n",
        "        for attempt in range(3):\n",
        "            headers = header_candidates[min(attempt, len(header_candidates)-1)]\n",
        "            try:\n",
        "                resp = requests.get(candidate_url, headers=headers, timeout=DEFAULT_TIMEOUT, allow_redirects=True)\n",
        "                status = resp.status_code\n",
        "\n",
        "                # Retry on \"blocked\"/rate-limited or transient server errors\n",
        "                if status in (403, 429) or 500 <= status < 600:\n",
        "                    last_err = f\"{status} {resp.reason}\"\n",
        "                    time.sleep(backoff)\n",
        "                    backoff *= 2.0\n",
        "                    continue\n",
        "\n",
        "                resp.raise_for_status()\n",
        "\n",
        "                # Prefer HTML content; some sites return other types\n",
        "                ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
        "                if \"text/html\" not in ctype:\n",
        "                    last_err = f\"Non-HTML content-type: {ctype}\"\n",
        "                    # Don‚Äôt retry endlessly for non-HTML; move to next candidate/final\n",
        "                    break\n",
        "\n",
        "                return resp.text, None\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                last_err = str(e)\n",
        "                time.sleep(backoff)\n",
        "                backoff *= 2.0\n",
        "                continue\n",
        "\n",
        "    return None, f\"Fetch error: {last_err or 'unknown error'}\"\n",
        "\n",
        "\n",
        "def extract_article_fields(html: str, url: str):\n",
        "    \"\"\"\n",
        "    Parse HTML to extract: title, author, published date, body text (paragraphs),\n",
        "    and link counts (total + external), plus transparency hint flag.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    text_chunks, title, author, published = [], None, None, None\n",
        "\n",
        "    # --- Title from <title> or OG meta ---\n",
        "    if soup.title and soup.title.string:\n",
        "        title = soup.title.string.strip()\n",
        "    mt = soup.find('meta', attrs={'property':'og:title'}) or soup.find('meta', attrs={'name':'title'})\n",
        "    if not title and mt and mt.get('content'):\n",
        "        title = mt['content'].strip()\n",
        "\n",
        "    # --- Author / byline in common locations ---\n",
        "    for selector in [\n",
        "        {'name':'meta','attrs':{'name':'author'}},\n",
        "        {'name':'meta','attrs':{'property':'article:author'}},\n",
        "        {'name':'span','class_':re.compile('author|byline', re.I)},\n",
        "        {'name':'div','class_':re.compile('author|byline', re.I)},\n",
        "        {'name':'a','class_':re.compile('author', re.I)},\n",
        "    ]:\n",
        "        if selector['name']=='meta':\n",
        "            node = soup.find('meta', attrs=selector['attrs'])\n",
        "            if node and node.get('content'):\n",
        "                author = node['content'].strip(); break\n",
        "        else:\n",
        "            node = soup.find(selector['name'], class_=selector.get('class_'))\n",
        "            if node and node.get_text(strip=True):\n",
        "                candidate = node.get_text(' ', strip=True)\n",
        "                if len(candidate) >= 3:\n",
        "                    author = candidate; break\n",
        "\n",
        "    # --- Publish date in common meta/time/span patterns ---\n",
        "    for date_sel in [\n",
        "        {'name':'meta','attrs':{'property':'article:published_time'}},\n",
        "        {'name':'meta','attrs':{'name':'date'}},\n",
        "        {'name':'time','attrs':{}},\n",
        "        {'name':'span','class_':re.compile('date|time', re.I)},\n",
        "    ]:\n",
        "        if date_sel['name']=='meta':\n",
        "            node = soup.find('meta', attrs=date_sel['attrs'])\n",
        "            if node and node.get('content'):\n",
        "                try:\n",
        "                    published = dateparser.parse(node['content'], fuzzy=True); break\n",
        "                except Exception:\n",
        "                    pass\n",
        "        else:\n",
        "            node = soup.find(date_sel['name'], class_=date_sel.get('class_'))\n",
        "            if node and node.get_text(strip=True):\n",
        "                try:\n",
        "                    published = dateparser.parse(node.get_text(strip=True), fuzzy=True); break\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    # --- Body text: prefer a likely article container, else all <p> ---\n",
        "    main_container = None\n",
        "    for cls in ['article','post','story','content','entry-content','article-body']:\n",
        "        mc = soup.find(True, class_=re.compile(cls, re.I))\n",
        "        if mc: main_container = mc; break\n",
        "    paragraphs = (main_container.find_all('p') if main_container else soup.find_all('p'))\n",
        "    for p in paragraphs:\n",
        "        t = p.get_text(' ', strip=True)\n",
        "        if t and len(t) > 40: text_chunks.append(t)\n",
        "    article_text = '\\n\\n'.join(text_chunks)[:100000]  # cap to avoid huge pages\n",
        "\n",
        "    # --- Link counts: total & external ---\n",
        "    all_links, external_links = [], []\n",
        "    base_host = urlparse(url).netloc.lower()\n",
        "    for a in soup.find_all('a', href=True):\n",
        "        href = a['href']\n",
        "        if href.startswith('http://') or href.startswith('https://'):\n",
        "            all_links.append(href)\n",
        "            if urlparse(href).netloc.lower() != base_host:\n",
        "                external_links.append(href)\n",
        "\n",
        "    # --- Transparency hint flag (string match) ---\n",
        "    full_text_for_hints = (article_text + ' ' + ' '.join(TRANSPARENCY_HINTS)).lower()\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'author': author,\n",
        "        'published': published.isoformat() if published else None,\n",
        "        'text': article_text,\n",
        "        'num_paragraphs': len(text_chunks),\n",
        "        'all_links': all_links,\n",
        "        'external_links': external_links,\n",
        "        'has_transparency_hints': any(h in full_text_for_hints for h in TRANSPARENCY_HINTS),\n",
        "    }\n",
        "\n",
        "def score_url(url: str, fields: dict):\n",
        "    \"\"\"\n",
        "    Apply heuristic scoring rules ‚Üí (score 0‚Äì100, explanation string).\n",
        "    Starts at 50 and adds/subtracts per signal.\n",
        "    \"\"\"\n",
        "    explanation_bits = []\n",
        "    score = 50  # neutral baseline\n",
        "\n",
        "    # HTTPS\n",
        "    if url.lower().startswith('https://'):\n",
        "        score += 12; explanation_bits.append('+12: uses HTTPS')\n",
        "    else:\n",
        "        score -= 10; explanation_bits.append('-10: not using HTTPS')\n",
        "\n",
        "    # Institutional TLD\n",
        "    ext = tldextract.extract(url)\n",
        "    tld_last = (ext.suffix.split('.')[-1] if ext.suffix else '')\n",
        "    if tld_last in INSTITUTIONAL_TLDS:\n",
        "        score += 14; explanation_bits.append(f'+14: institutional TLD ({tld_last})')\n",
        "\n",
        "    # Author/byline\n",
        "    if fields.get('author'):\n",
        "        score += 10; explanation_bits.append('+10: author/byline found')\n",
        "    else:\n",
        "        score -= 6; explanation_bits.append('-6: no clear author/byline')\n",
        "\n",
        "    # Published recency (NOTE: uses datetime.utcnow(), may warn in 3.12+)\n",
        "    published = fields.get('published')\n",
        "    if published:\n",
        "        try:\n",
        "            dt = dateparser.parse(published)\n",
        "            if (datetime.utcnow() - dt).days <= 3650:\n",
        "                score += 6; explanation_bits.append('+6: reasonably recent publication date')\n",
        "            else:\n",
        "                score -= 4; explanation_bits.append('-4: appears quite old')\n",
        "        except Exception:\n",
        "            explanation_bits.append('0: could not parse publication date reliably')\n",
        "    else:\n",
        "        explanation_bits.append('0: no publication date found')\n",
        "\n",
        "    # References\n",
        "    total_links = len(fields.get('all_links', []))\n",
        "    external_links_count = len(fields.get('external_links', []))\n",
        "    if total_links >= 5 and external_links_count >= 3:\n",
        "        score += 10; explanation_bits.append(f'+10: provides references (links: {total_links}, external: {external_links_count})')\n",
        "    elif total_links >= 2:\n",
        "        score += 4; explanation_bits.append(f'+4: some references (links: {total_links})')\n",
        "    else:\n",
        "        score -= 6; explanation_bits.append(f'-6: minimal/no references (links: {total_links})')\n",
        "\n",
        "    # Length (by paragraph count)\n",
        "    num_paras = fields.get('num_paragraphs', 0)\n",
        "    if num_paras >= 8:\n",
        "        score += 6; explanation_bits.append('+6: substantive article length')\n",
        "    elif num_paras >= 3:\n",
        "        score += 2; explanation_bits.append('+2: moderate article length')\n",
        "    else:\n",
        "        score -= 6; explanation_bits.append('-6: very short article text')\n",
        "\n",
        "    # Clickbait language\n",
        "    text_lower = (fields.get('text') or '').lower()\n",
        "    clickbait_hits = sum(1 for term in CLICKBAIT_TERMS if term in text_lower)\n",
        "    if clickbait_hits >= 2:\n",
        "        score -= 10; explanation_bits.append('-10: strong clickbait indicators')\n",
        "    elif clickbait_hits == 1:\n",
        "        score -= 4; explanation_bits.append('-4: mild clickbait indicators')\n",
        "\n",
        "    # Advertising/sponsor cues\n",
        "    ad_signals = len(re.findall(r\"advertis(e|ement)|sponsor(ed|ship)\", text_lower))\n",
        "    iframes_penalty = min(8, math.floor(ad_signals / 5) * 2)\n",
        "    if iframes_penalty:\n",
        "        score -= iframes_penalty; explanation_bits.append(f'-{iframes_penalty}: advertising/sponsorship language')\n",
        "\n",
        "    # Clamp score and join explanation\n",
        "    score = max(0, min(100, int(round(score))))\n",
        "    explanation = '; '.join(explanation_bits)\n",
        "    return score, explanation\n",
        "\n",
        "def evaluate_url(url: str):\n",
        "    \"\"\"\n",
        "    Orchestrator: fetch ‚Üí parse ‚Üí score.\n",
        "    Returns a dict with 'score', 'explanation', and 'details' on success.\n",
        "    Returns None if the page cannot be accessed (STRICT EXCLUDE).\n",
        "    \"\"\"\n",
        "    if not isinstance(url, str) or not url.strip():\n",
        "        return None  # exclude invalid\n",
        "\n",
        "    html, err = fetch_html(url)\n",
        "    if err:\n",
        "        return None  # STRICT EXCLUDE: do not return a low score or an error object\n",
        "\n",
        "    fields = extract_article_fields(html, url)\n",
        "    score, explanation = score_url(url, fields)\n",
        "\n",
        "    return {\n",
        "        'score': score,\n",
        "        'explanation': explanation,\n",
        "        'details': {\n",
        "            'url': url,\n",
        "            'title': fields.get('title'),\n",
        "            'author': fields.get('author'),\n",
        "            'published': fields.get('published'),\n",
        "            'num_paragraphs': fields.get('num_paragraphs'),\n",
        "            'total_links': len(fields.get('all_links', [])),\n",
        "            'external_links': len(fields.get('external_links', [])),\n",
        "        },\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# ======  H Y B R I D   ( R u l e s  +  L i n e a r  R e g ) =\n",
        "# ============================================================\n",
        "# This section:\n",
        "# 1) builds features from evaluate_url() outputs\n",
        "# 2) creates a labeled dataset of 20 real URLs (0‚Äì100 labels)\n",
        "# 3) runs 5-fold cross-validation for LinearRegression\n",
        "# 4) fits a final model on ALL available rows\n",
        "# 5) exposes hybrid_score(url, alpha) for inference\n",
        "#\n",
        "# Notes:\n",
        "# - If some URLs fail to fetch, they're STRICTLY EXCLUDED (no score, not shown).\n",
        "# - CV uses MAE and R^2 to give you both error and fit quality.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from datetime import datetime, timezone\n",
        "from dateutil import parser as dateparser\n",
        "\n",
        "# ---------- Feature Engineering ----------\n",
        "def _safe(value, default=0):\n",
        "    return value if value is not None else default\n",
        "\n",
        "def _https_flag(url: str) -> int:\n",
        "    return 1 if isinstance(url, str) and url.lower().startswith('https://') else 0\n",
        "\n",
        "def _institutional_tld_flag(url: str) -> int:\n",
        "    try:\n",
        "        ext = tldextract.extract(url)\n",
        "        tld_last = (ext.suffix.split('.')[-1] if ext.suffix else '')\n",
        "        return 1 if tld_last in {'edu','gov','ac','sch','mil'} else 0\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "def _days_since(published_iso: str) -> int:\n",
        "    if not published_iso:\n",
        "        return 99999  # treat unknown/absent as very old\n",
        "    try:\n",
        "        dt = dateparser.parse(published_iso)\n",
        "        return max(0, (datetime.utcnow() - dt).days)  # OK to mirror your original utc usage\n",
        "    except Exception:\n",
        "        return 99999\n",
        "\n",
        "def features_from_eval(eval_obj: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Build a feature dict using ONLY fields the original scorer returns.\n",
        "    These features are model-agnostic and cheap to compute.\n",
        "    \"\"\"\n",
        "    d = eval_obj.get('details', {})\n",
        "    url = d.get('url') or ''\n",
        "    feats = {\n",
        "        \"https\": _https_flag(url),\n",
        "        \"inst_tld\": _institutional_tld_flag(url),\n",
        "        \"has_author\": 1 if d.get('author') else 0,\n",
        "        \"num_paragraphs\": _safe(d.get('num_paragraphs'), 0),\n",
        "        \"total_links\": _safe(d.get('total_links'), 0),\n",
        "        \"external_links\": _safe(d.get('external_links'), 0),\n",
        "        \"days_since_pub\": _days_since(d.get('published')),\n",
        "    }\n",
        "    return feats\n",
        "\n",
        "FEATURE_ORDER = [\n",
        "    \"https\",\n",
        "    \"inst_tld\",\n",
        "    \"has_author\",\n",
        "    \"num_paragraphs\",\n",
        "    \"total_links\",\n",
        "    \"external_links\",\n",
        "    \"days_since_pub\",\n",
        "]\n",
        "\n",
        "def vectorize_features(feat_dict: dict, feature_order=FEATURE_ORDER):\n",
        "    \"\"\"Convert a feature dict to a fixed-order numeric vector.\"\"\"\n",
        "    return np.array([feat_dict.get(k, 0) for k in feature_order], dtype=float)\n",
        "\n",
        "# ---------- Labeled Dataset (20 REAL URLs, mixed quality) ----------\n",
        "# Labels are illustrative 0‚Äì100 targets. Adjust as you refine ground truth.\n",
        "LABELED_URLS = [\n",
        "    # Highly credible / institutional / quality editorial\n",
        "    (\"https://www.cdc.gov/flu/about/index.html\", 92),\n",
        "    (\"https://www.nih.gov/news-events/news-releases\", 88),\n",
        "    (\"https://www.mayoclinic.org/diseases-conditions/dehydration/symptoms-causes/syc-20354086\", 85),\n",
        "    (\"https://www.who.int/news-room/fact-sheets/detail/diabetes\", 90),\n",
        "    (\"https://www.britannica.com/science/photosynthesis\", 86),\n",
        "    (\"https://www.health.harvard.edu/staying-healthy/what-is-intermittent-fasting\", 78),\n",
        "    (\"https://www.hopkinsmedicine.org/health/conditions-and-diseases\", 84),\n",
        "    (\"https://www.bbc.com/news/science_and_environment\", 80),\n",
        "    (\"https://www.reuters.com/world/us/\", 80),\n",
        "    (\"https://apnews.com/hub/technology\", 78),\n",
        "    (\"https://en.wikipedia.org/wiki/Ice_cream\", 75),\n",
        "\n",
        "    # Mid credibility consumer health / informative\n",
        "    (\"https://www.healthline.com/nutrition/green-tea-and-weight-loss\", 70),\n",
        "    (\"https://www.webmd.com/diet/obesity/features/green-tea-and-weight-loss\", 68),\n",
        "    (\"https://www.nature.com/scitable/definition/photosynthesis-288/\", 82),\n",
        "    (\"https://med.stanford.edu/news/all-news.html\", 82),\n",
        "\n",
        "    # Lower credibility / UGC / lighter editorial\n",
        "    (\"https://medium.com/\", 45),\n",
        "    (\"https://www.reddit.com/r/icecreamery/comments/19elt19/looking_for_resources_to_learn_how_to_make_ice/\", 20),\n",
        "    (\"https://www.quora.com/Is-green-tea-good-for-weight-loss\", 25),\n",
        "    (\"https://www.livestrong.com/article/13715706-green-tea-benefits/\", 60),\n",
        "    (\"https://www.buzzfeed.com/\", 40),\n",
        "]\n",
        "\n",
        "# ---------- Build Dataset by Evaluating & Featurizing ----------\n",
        "rows_X, rows_y = [], []\n",
        "\n",
        "for url, label in LABELED_URLS:\n",
        "    try:\n",
        "        ev = evaluate_url(url)\n",
        "        if ev is None:  # STRICT EXCLUDE\n",
        "            continue\n",
        "        feats = features_from_eval(ev)\n",
        "        x = vectorize_features(feats)\n",
        "        rows_X.append(x)\n",
        "        rows_y.append(float(label))\n",
        "    except Exception:\n",
        "        # Strict policy: exclude on any error\n",
        "        continue\n",
        "\n",
        "X = np.vstack(rows_X) if rows_X else np.zeros((0, len(FEATURE_ORDER)))\n",
        "y = np.array(rows_y) if rows_y else np.zeros((0,))\n",
        "\n",
        "print(f\"Prepared dataset: {X.shape[0]} rows √ó {X.shape[1]} features.\")\n",
        "\n",
        "# ---------- 5-Fold Cross-Validation ----------\n",
        "# We use KFold regression CV with two metrics:\n",
        "#  - MAE (lower is better): absolute error in points of the 0‚Äì100 score\n",
        "#  - R^2 (higher is better): variance explained\n",
        "if len(X) >= 5:\n",
        "    k = min(5, len(X))  # guard in case many rows were excluded\n",
        "    if k < 2:\n",
        "        print(\"\\nNot enough rows for CV; skipping cross-validation.\")\n",
        "    else:\n",
        "        kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "        # MAE (scikit returns negative MAE for loss; invert sign to report positive MAE)\n",
        "        mae_scores = cross_val_score(LinearRegression(), X, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
        "        r2_scores  = cross_val_score(LinearRegression(), X, y, cv=kf, scoring=\"r2\")\n",
        "        mae_vals = -mae_scores  # turn to positive\n",
        "        print(f\"\\n{k}-fold CV results on {len(X)} rows:\")\n",
        "        print(f\"  MAE per fold: {np.round(mae_vals, 2)} | mean={mae_vals.mean():.2f}\")\n",
        "        print(f\"  R^2 per fold:  {np.round(r2_scores, 3)} | mean={r2_scores.mean():.3f}\")\n",
        "else:\n",
        "    print(\"\\nNot enough rows for CV; need ‚â•5 examples.\")\n",
        "\n",
        "# ---------- Fit Final Linear Regression on ALL available rows ----------\n",
        "linreg = LinearRegression()\n",
        "if len(X) >= 2:\n",
        "    linreg.fit(X, y)\n",
        "    print(\"\\nFinal model (trained on all available rows):\")\n",
        "    for kname, w in zip(FEATURE_ORDER, linreg.coef_):\n",
        "        print(f\"  {kname:>16s}: {w: .3f}\")\n",
        "    print(f\"  {'Intercept':>16s}: {linreg.intercept_: .3f}\")\n",
        "else:\n",
        "    print(\"\\nNot enough rows to train final model (need ‚â•2). Using neutral ML predictions.\")\n",
        "\n",
        "# ---------- Inference helpers ----------\n",
        "def predict_ml_score_from_eval(eval_obj: dict) -> float:\n",
        "    \"\"\"\n",
        "    Predict a 0‚Äì100 credibility score from features using the trained LinearRegression.\n",
        "    Falls back to 50.0 if not enough training data is available.\n",
        "    \"\"\"\n",
        "    if len(X) < 2:\n",
        "        return 50.0\n",
        "    feats = features_from_eval(eval_obj)\n",
        "    x = vectorize_features(feats).reshape(1, -1)\n",
        "    pred = linreg.predict(x)[0]\n",
        "    return float(np.clip(pred, 0.0, 100.0))\n",
        "\n",
        "def _stars(score_0_100: float) -> str:\n",
        "    stars = int(round(score_0_100 / 20))\n",
        "    stars = max(0, min(5, stars))\n",
        "    return \"‚òÖ\"*stars + \"‚òÜ\"*(5 - stars)\n",
        "\n",
        "def hybrid_score(url: str, alpha: float = 0.6) -> dict | None:\n",
        "    \"\"\"\n",
        "    Blend rule-based score with ML-predicted score:\n",
        "        final = alpha * rule_score + (1 - alpha) * ml_score\n",
        "\n",
        "    STRICT EXCLUDE:\n",
        "    - If fetch/evaluation fails, return None (do not score or include).\n",
        "    \"\"\"\n",
        "    ev = evaluate_url(url)\n",
        "    if ev is None:\n",
        "        return None  # STRICT EXCLUDE\n",
        "\n",
        "    details = ev.get(\"details\", {})\n",
        "    rule_score = float(ev.get(\"score\", 0.0))\n",
        "\n",
        "    # Normal path: combine rules + ML\n",
        "    ml_score = predict_ml_score_from_eval(ev)\n",
        "    final = float(np.clip(alpha * rule_score + (1 - alpha) * ml_score, 0.0, 100.0))\n",
        "\n",
        "    # Stars\n",
        "    stars = int(round(final / 20))\n",
        "    stars = max(0, min(5, stars))\n",
        "    star_str = \"‚òÖ\" * stars + \"‚òÜ\" * (5 - stars)\n",
        "\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"title\": details.get(\"title\"),\n",
        "        \"rule_score\": round(rule_score, 1),\n",
        "        \"ml_score\": round(ml_score, 1),\n",
        "        \"hybrid_score\": round(final, 1),\n",
        "        \"stars\": star_str,\n",
        "        \"explanation\": ev.get(\"explanation\", \"No detailed explanation available.\"),\n",
        "        \"details\": details,\n",
        "    }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BTtvEQ7VR8vn",
        "outputId": "2d477558-30a8-401b-8029-a6af287cd64c",
        "cellView": "form"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared dataset: 13 rows √ó 7 features.\n",
            "\n",
            "5-fold CV results on 13 rows:\n",
            "  MAE per fold: [135.44 115.89  11.52  94.79  19.37] | mean=75.40\n",
            "  R^2 per fold:  [-1.161130e+02 -1.696133e+03  3.900000e-01 -4.411298e+03 -1.559900e+01] | mean=-1247.751\n",
            "\n",
            "Final model (trained on all available rows):\n",
            "             https:  0.000\n",
            "          inst_tld:  25.932\n",
            "        has_author: -5.102\n",
            "    num_paragraphs:  0.193\n",
            "       total_links:  0.013\n",
            "    external_links:  0.013\n",
            "    days_since_pub: -0.000\n",
            "         Intercept:  82.189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SERP API Functions:\n",
        "# --- Improved SerpAPI search (clean results, de-dupe, filter, robust errors) ---\n",
        "import os, re, requests\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "SERP_API_KEY = os.getenv(\"SERP_API_KEY\")\n",
        "\n",
        "# Common low-signal/social/video domains to exclude by default (tune as needed)\n",
        "_DEFAULT_EXCLUDE_DOMAINS = {\n",
        "    \"reddit.com\", \"www.reddit.com\", \"old.reddit.com\",\n",
        "    \"x.com\", \"twitter.com\", \"www.twitter.com\",\n",
        "    \"tiktok.com\", \"www.tiktok.com\",\n",
        "    \"pinterest.com\", \"www.pinterest.com\",\n",
        "    \"facebook.com\", \"www.facebook.com\",\n",
        "    \"instagram.com\", \"www.instagram.com\",\n",
        "    \"youtube.com\", \"www.youtube.com\", \"youtu.be\"\n",
        "}\n",
        "\n",
        "# Skip obvious non-article filetypes\n",
        "_SKIP_FILETYPES = re.compile(r\"\\.(pdf|pptx?|docx?|xlsx?|zip|rar)(?:$|\\?)\", re.I)\n",
        "\n",
        "def _host(url: str) -> str:\n",
        "    return urlparse(url).netloc.lower()\n",
        "\n",
        "def search_google(\n",
        "    query: str,\n",
        "    num_results: int = 30,\n",
        "    exclude_domains: set | None = None,\n",
        "    allow_news_results: bool = True,\n",
        "    timeout: int = 20,\n",
        ") -> list[dict]:\n",
        "    \"\"\"\n",
        "    Search Google via SerpAPI and return a clean list of results:\n",
        "        [{\"title\": str, \"link\": str, \"snippet\": str}, ...]\n",
        "    - De-duplicates by link and avoids flooding from the same host\n",
        "    - Skips social/UGC/video sites and non-HTML filetypes\n",
        "    - Optionally includes Google News results\n",
        "    \"\"\"\n",
        "    if not SERP_API_KEY:\n",
        "        raise RuntimeError(\"Missing SERP_API_KEY / SERPAPI_API_KEY. Set it in the environment first.\")\n",
        "\n",
        "    exclude = set(_DEFAULT_EXCLUDE_DOMAINS)\n",
        "    if exclude_domains:\n",
        "        exclude |= set(exclude_domains)\n",
        "\n",
        "    params = {\n",
        "        \"engine\": \"google\",\n",
        "        \"q\": query,\n",
        "        \"api_key\": SERP_API_KEY,\n",
        "        \"num\": 30,          # pull extra, then filter/trim to num_results\n",
        "        \"hl\": \"en\",\n",
        "        \"gl\": \"us\",\n",
        "        \"safe\": \"active\",   # optional: reduce NSFW\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        resp = requests.get(\"https://serpapi.com/search\", params=params, timeout=timeout)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "    except Exception as e:\n",
        "        # On failure, return empty list (caller can handle and message user)\n",
        "        print(f\"[SerpAPI] Error: {e}\")\n",
        "        return []\n",
        "\n",
        "    candidates: list[dict] = []\n",
        "\n",
        "    # 1) Organic results\n",
        "    for r in (data.get(\"organic_results\") or []):\n",
        "        link = r.get(\"link\")\n",
        "        title = r.get(\"title\")\n",
        "        snippet = r.get(\"snippet\") or \"\"\n",
        "        if not link or _SKIP_FILETYPES.search(link):\n",
        "            continue\n",
        "        host = _host(link)\n",
        "        if host in exclude:\n",
        "            continue\n",
        "        candidates.append({\"title\": title, \"link\": link, \"snippet\": snippet})\n",
        "\n",
        "    # 2) Optional: News results (helpful for timely topics)\n",
        "    if allow_news_results:\n",
        "        for r in (data.get(\"news_results\") or []):\n",
        "            link = r.get(\"link\")\n",
        "            title = r.get(\"title\")\n",
        "            snippet = r.get(\"snippet\") or \"\"\n",
        "            if not link or _SKIP_FILETYPES.search(link):\n",
        "                continue\n",
        "            host = _host(link)\n",
        "            if host in exclude:\n",
        "                continue\n",
        "            candidates.append({\"title\": title, \"link\": link, \"snippet\": snippet})\n",
        "\n",
        "    # De-duplicate by link; also avoid over-representing a single host\n",
        "    cleaned, seen_links, seen_hosts = [], set(), set()\n",
        "    for c in candidates:\n",
        "        link, host = c[\"link\"], _host(c[\"link\"])\n",
        "        if link in seen_links:\n",
        "            continue\n",
        "        # If we already have enough and this host is duplicate, skip\n",
        "        if host in seen_hosts and len(cleaned) >= num_results:\n",
        "            continue\n",
        "        seen_links.add(link)\n",
        "        seen_hosts.add(host)\n",
        "        cleaned.append(c)\n",
        "        if len(cleaned) >= num_results:\n",
        "            break\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "def credbot_summarize_text(text: str, query: str = \"\", max_chars: int = 500) -> str:\n",
        "    \"\"\"\n",
        "    Super-light extractive summary:\n",
        "      1) Split into sentences.\n",
        "      2) Score sentences by presence of query keywords + informative cues.\n",
        "      3) Pick top sentences until max_chars is reached.\n",
        "    Falls back to the first 2‚Äì3 sentences if scoring yields little.\n",
        "    \"\"\"\n",
        "\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Normalize whitespace\n",
        "    clean = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # Sentence split (simple, robust)\n",
        "    sentences = re.split(r\"(?<=[.!?])\\s+(?=[A-Z0-9\\\"'])\", clean)\n",
        "    sentences = [s.strip() for s in sentences if len(s.strip()) > 0]\n",
        "\n",
        "    if not sentences:\n",
        "        return clean[:max_chars]\n",
        "\n",
        "    # Prepare query keywords\n",
        "    q = (query or \"\").lower().strip()\n",
        "    q_terms = [t for t in re.split(r\"[^a-z0-9]+\", q) if len(t) >= 3]\n",
        "    q_terms = list(dict.fromkeys(q_terms))  # unique order\n",
        "\n",
        "    # Score sentences\n",
        "    scored = []\n",
        "    for i, s in enumerate(sentences):\n",
        "        sl = s.lower()\n",
        "\n",
        "        # keyword matches (weighted)\n",
        "        kw_hits = sum(1 for t in q_terms if t in sl)\n",
        "        kw_score = 3 * kw_hits\n",
        "\n",
        "        # informative cues\n",
        "        cues = 0\n",
        "        if len(s) > 80: cues += 1\n",
        "        if \":\" in s or \";\" in s or \"(\" in s or \")\" in s: cues += 1\n",
        "        if re.search(r\"\\b(according to|study|report|data|evidence|research|analysis|conclude|results)\\b\", sl):\n",
        "            cues += 2\n",
        "\n",
        "        # slight lead bias\n",
        "        lead_bonus = max(0, 3 - i)  # first few sentences get a small boost\n",
        "\n",
        "        score = kw_score + cues + 0.2 * lead_bonus\n",
        "        scored.append((score, i, s))\n",
        "\n",
        "    # Pick best sentences until we hit the budget\n",
        "    picked = []\n",
        "    total = 0\n",
        "    for _, _, s in sorted(scored, key=lambda x: (-x[0], x[1])):  # high score, then original order\n",
        "        if s in picked:\n",
        "            continue\n",
        "        if total + len(s) + (2 if picked else 0) > max_chars:\n",
        "            # if nothing picked yet, at least include a truncated sentence\n",
        "            if not picked:\n",
        "                picked.append(s[:max_chars].rstrip() + \"‚Ä¶\")\n",
        "                total = max_chars\n",
        "            break\n",
        "        picked.append(s)\n",
        "        total += len(s) + (2 if len(picked) > 1 else 0)\n",
        "\n",
        "        # Aim for ~3 sentences when possible\n",
        "        if len(picked) >= 3:\n",
        "            break\n",
        "\n",
        "    # Fallback to lead-2/3 if scoring produced very little\n",
        "    if len(picked) == 0:\n",
        "        lead = []\n",
        "        for s in sentences[:3]:\n",
        "            if sum(len(x) for x in lead) + len(s) + (2 if lead else 0) <= max_chars:\n",
        "                lead.append(s)\n",
        "        picked = lead or [sentences[0][:max_chars].rstrip() + \"‚Ä¶\"]\n",
        "\n",
        "    return \"  \".join(picked)"
      ],
      "metadata": {
        "id": "dMWhH62SWyQO",
        "cellView": "form"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Gemini Chat Integration: (model-driven actions; code only runs search on search mode)\n",
        "import os, re, json, google.generativeai as genai\n",
        "\n",
        "DEBUG = False  # set to False to silence diagnostics\n",
        "\n",
        "# ---------- Gemini setup ----------\n",
        "# Avoid bad env endpoints\n",
        "for _k in [\"GOOGLE_API_BASE_URL\", \"GOOGLE_AI_API_BASE\", \"GOOGLE_API_ENDPOINT\"]:\n",
        "    os.environ.pop(_k, None)\n",
        "\n",
        "_GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "if not _GEMINI_API_KEY:\n",
        "    raise RuntimeError(\"Missing GEMINI_API_KEY. Please set it in the environment first.\")\n",
        "\n",
        "genai.configure(api_key=_GEMINI_API_KEY, client_options={\"api_endpoint\": \"https://generativelanguage.googleapis.com\"})\n",
        "\n",
        "# Preferred, stable chat models first\n",
        "_PREFERRED = [\n",
        "    \"gemini-2.5-flash\", \"gemini-2.5-pro\",\n",
        "    \"gemini-1.5-flash\", \"gemini-1.5-pro\",\n",
        "    \"gemini-pro\",\n",
        "]\n",
        "\n",
        "# Exclude non-general or unstable families from normal chat\n",
        "_EXCLUDE_SUBSTRINGS = {\"robotics\", \"er\", \"exp\", \"lite\", \"vision\", \"audio\", \"multimodalllm\", \"preview\"}\n",
        "\n",
        "def _supports_generate(m) -> bool:\n",
        "    methods = set(getattr(m, \"supported_generation_methods\", []) or [])\n",
        "    return (\"generateContent\" in methods) or (\"streamGenerateContent\" in methods) or (\"generate_text\" in methods)\n",
        "\n",
        "def _clean(name: str) -> str:\n",
        "    return name.replace(\"models/\", \"\")\n",
        "\n",
        "def _choose_from_list(models) -> str | None:\n",
        "    usable = [\n",
        "        m for m in models\n",
        "        if _supports_generate(m) and not any(bad in m.name.lower() for bad in _EXCLUDE_SUBSTRINGS)\n",
        "    ]\n",
        "    names = [_clean(m.name) for m in usable]\n",
        "    for prefer in _PREFERRED:\n",
        "        if prefer in names:\n",
        "            return prefer\n",
        "    stable_25 = [n for n in names if n.startswith(\"gemini-2.5-\")]\n",
        "    for tag in (\"flash\", \"pro\"):\n",
        "        for n in stable_25:\n",
        "            if f\"-{tag}\" in n:\n",
        "                return n\n",
        "    return names[0] if names else None\n",
        "\n",
        "def _pick_model():\n",
        "    try:\n",
        "        models = list(genai.list_models())\n",
        "        if DEBUG:\n",
        "            visible = [m.name for m in models]\n",
        "            print(\"[Gemini] Models visible to key:\", visible[:10], \"...\" if len(visible) > 10 else \"\")\n",
        "        chosen = _choose_from_list(models)\n",
        "        if not chosen:\n",
        "            if DEBUG: print(\"[Gemini] No usable chat model found for this key/project.\")\n",
        "            return None\n",
        "        if DEBUG: print(f\"[Gemini] Selected model: {chosen}\")\n",
        "        m = genai.GenerativeModel(model_name=chosen)\n",
        "        try:\n",
        "            _ = m.generate_content(\"ping\")\n",
        "        except Exception as e:\n",
        "            if DEBUG: print(f\"[Gemini] Probe on {chosen} returned error (continuing anyway): {e}\")\n",
        "        return m\n",
        "    except Exception as e:\n",
        "        if DEBUG: print(f\"[Gemini] list_models failed: {e}\")\n",
        "        return None\n",
        "\n",
        "_GEMINI = _pick_model()\n",
        "\n",
        "def _gemini_text(prompt: str) -> str | None:\n",
        "    \"\"\"Safe Gemini call; never surface internal errors to the user.\"\"\"\n",
        "    if _GEMINI is None:\n",
        "        if DEBUG: print(\"[Gemini] No model selected; skipping call.\")\n",
        "        return None\n",
        "    try:\n",
        "        resp = _GEMINI.generate_content(prompt)\n",
        "        out = getattr(resp, \"text\", None)\n",
        "        if not out and hasattr(resp, \"candidates\") and resp.candidates:\n",
        "            cand = resp.candidates[0]\n",
        "            if hasattr(cand, \"content\") and cand.content and getattr(cand.content, \"parts\", None):\n",
        "                out = cand.content.parts[0].text\n",
        "        if DEBUG: print(f\"[Gemini] Response present: {bool(out)}\")\n",
        "        return out.strip() if out else None\n",
        "    except Exception as e:\n",
        "        if DEBUG: print(f\"[Gemini] Call failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def _clip_summary(s):\n",
        "    if not s: return \"\"\n",
        "    parts = re.split(r'(?<=[.!?])\\s+', s.strip())\n",
        "    return \" \".join(parts[:3])[:350]\n",
        "\n",
        "def _parse_json_block(text):\n",
        "    if not text: return None\n",
        "    m = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, flags=re.S)\n",
        "    if m:\n",
        "        try: return json.loads(m.group(1))\n",
        "        except: pass\n",
        "    m = re.search(r\"(\\{.*\\})\", text, flags=re.S)\n",
        "    if m:\n",
        "        try: return json.loads(m.group(1))\n",
        "        except: pass\n",
        "    return None\n",
        "\n",
        "# ---------- YOUR project-provided helpers (assumed available) ----------\n",
        "# Required functions expected from your earlier notebook cells:\n",
        "# - search_google(query, num_results=10, exclude_domains=set())\n",
        "# - fetch_html(url) -> (html, err)\n",
        "# - extract_article_fields(html, url) -> dict(title=..., text=..., ...)\n",
        "# - hybrid_score(url, alpha=0.6) -> dict(hybrid_score=..., stars=..., explanation=...)\n",
        "# - credbot_summarize_text(text, query) -> str\n",
        "\n",
        "# 1) Force the scoring path to use your built-in summarizer\n",
        "def _score_and_summarize_url(link, query):\n",
        "    try:\n",
        "        hybrid = hybrid_score(link, alpha=0.6)\n",
        "        if hybrid is None:\n",
        "            return False\n",
        "        html, err = fetch_html(link)\n",
        "        if err or not html:\n",
        "            return False\n",
        "        fields = extract_article_fields(html, link)\n",
        "        text = (fields.get(\"text\") or \"\").strip()\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        summary = credbot_summarize_text(text, query)\n",
        "\n",
        "        title = fields.get(\"title\") or \"[No Title]\"\n",
        "        score = hybrid.get(\"hybrid_score\", 0.0)\n",
        "        stars = hybrid.get(\"stars\") or (\"‚òÖ\" * int(round(score / 20)))\n",
        "        why = (hybrid.get(\"explanation\") or \"\").strip()\n",
        "\n",
        "        print(f\"\\nüì∞ {title}\\nüîó {link}\\nüìÑ {summary}\\n‚≠ê {stars} ({score}/100)\")\n",
        "        if why:\n",
        "            print(f\"üìù {why}\\n\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[Score] Exception on {link}: {e}\")\n",
        "        return False\n",
        "\n",
        "# 2) Keep your tuned search flow; *only called when the MODEL says run_search*\n",
        "def _run_search_flow(topic):\n",
        "    print(f\"üîç Searching for sources on ‚Äú{topic}‚Äù‚Ä¶\")\n",
        "    try:\n",
        "        results = search_google(\n",
        "            topic,\n",
        "            num_results=5,\n",
        "            exclude_domains={\n",
        "                \"walmart.com\",\"www.walmart.com\",\n",
        "                \"amazon.com\",\"www.amazon.com\",\n",
        "                \"brusters.com\",\"www.brusters.com\",\n",
        "                \"baskinrobbins.com\",\"www.baskinrobbins.com\",\n",
        "                \"coldstonecreamery.com\",\"www.coldstonecreamery.com\",\n",
        "            },\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Search error: {e}\")\n",
        "        return 0\n",
        "\n",
        "    if not results:\n",
        "        print(\"‚ö†Ô∏è No relevant articles found.\")\n",
        "        return 0\n",
        "\n",
        "    shown = 0\n",
        "    for r in results:\n",
        "        link = r.get(\"link\")\n",
        "        if link and _score_and_summarize_url(link, topic):\n",
        "            shown += 1\n",
        "\n",
        "    if shown == 0:\n",
        "        print(\"‚ö†Ô∏è None accessible.\")\n",
        "    return shown\n",
        "\n",
        "# ---------- Single-step MODEL-DRIVEN engine ----------\n",
        "# No local greeting/topic detection. No offer-search. No client-side policy.\n",
        "# The *model* decides everything via JSON. We obey it literally.\n",
        "A_CHAT, A_RUN = \"chat_mode\", \"run_search\"\n",
        "\n",
        "_SYSTEM_PROMPT = (\n",
        "    \"You are CredBot, a concise and friendly assistant.\\n\"\n",
        "    \"Return ONLY a JSON object with keys: reply, summary, topic, action.\\n\"\n",
        "    f'action must be \"{A_CHAT}\" or \"{A_RUN}\".\\n'\n",
        "    \"- You (the model) decide greetings, topic extraction, and whether a web search is necessary.\\n\"\n",
        "    f'- If a web search is needed, set action=\"{A_RUN}\" and provide a concise search query in \"topic\".\\n'\n",
        "    \"- Keep reply ‚â§ 2 sentences. Keep summary to 2‚Äì3 sentences (‚â§350 chars).\\n\"\n",
        "    \"- Do not include backticks or extra text around the JSON.\"\n",
        ")\n",
        "\n",
        "def gemini_step(user_input, prev_summary, prev_topic):\n",
        "    prompt = (\n",
        "        f\"{_SYSTEM_PROMPT}\\n\\n\"\n",
        "        f\"Previous summary:\\n{prev_summary or ''}\\n\\n\"\n",
        "        f\"Previous topic:\\n{prev_topic or ''}\\n\\n\"\n",
        "        f\"User message:\\n{user_input}\\n\\n\"\n",
        "        'Example response format:\\n'\n",
        "        '{\"reply\":\"...\", \"summary\":\"...\", \"topic\":\"...\", \"action\":\"chat_mode\"}'\n",
        "    )\n",
        "\n",
        "    raw = _gemini_text(prompt)\n",
        "    data = _parse_json_block(raw) if raw else None\n",
        "    if DEBUG:\n",
        "        print(f\"[Gemini] Raw present: {bool(raw)} | JSON parsed: {isinstance(data, dict)}\")\n",
        "\n",
        "    if not isinstance(data, dict):\n",
        "        # Minimal fallback; no local decisions beyond graceful error handling.\n",
        "        return (\"Sorry‚ÄîI'm having trouble reading that. Could you try again?\",\n",
        "                prev_summary or \"\", prev_topic or \"\", A_CHAT)\n",
        "\n",
        "    reply    = (data.get(\"reply\") or \"\").strip() or \"Okay.\"\n",
        "    summary  = _clip_summary(data.get(\"summary\") or \"\")\n",
        "    topic    = (data.get(\"topic\") or \"\").strip()\n",
        "    action   = (data.get(\"action\") or A_CHAT).strip()\n",
        "    if action not in {A_CHAT, A_RUN}:\n",
        "        action = A_CHAT\n",
        "\n",
        "    return reply, summary, topic, action\n",
        "\n",
        "# ---------- Main loop ----------\n",
        "def run_chat():\n",
        "    print(\"ü§ñ Hi, I‚Äôm CredBot. We can chat normally. (Type 'exit' to quit.)\")\n",
        "    if DEBUG: print(f\"[Startup] Gemini ready: {bool(_GEMINI)}\")\n",
        "\n",
        "    summary, topic = \"\", \"\"\n",
        "\n",
        "    while True:\n",
        "        user = input(\"\\nYou: \").strip()\n",
        "        if not user:\n",
        "            continue\n",
        "        if user.lower() in {\"exit\",\"quit\",\"bye\"}:\n",
        "            print(\"üëã Goodbye!\")\n",
        "            break\n",
        "\n",
        "        reply, summary_out, topic_out, action = gemini_step(user, summary, topic)\n",
        "\n",
        "        # Only special behavior: if (and only if) the model says run_search, we run search.\n",
        "        if action == A_RUN:\n",
        "            run_topic = topic_out or topic or user\n",
        "            shown = _run_search_flow(run_topic)\n",
        "            if shown:\n",
        "                # Update summary with a light note; do not override model content otherwise.\n",
        "                summary = _clip_summary((summary_out or \"\") + f\" Retrieved sources on {run_topic}.\")\n",
        "            else:\n",
        "                summary = summary_out or summary\n",
        "            topic = topic_out or topic\n",
        "            print(\"ü§ñ Anything else to explore?\")\n",
        "            # Always return to chat mode after a search\n",
        "            print(f\"üß† [summary] {summary}\\nüéØ [topic] {topic}\\n‚öôÔ∏è [action] chat_mode\")\n",
        "            continue\n",
        "\n",
        "        # Normal chat mode: print what the model said, adopt its memory updates\n",
        "        print(f\"ü§ñ {reply}\")\n",
        "        summary = summary_out or summary\n",
        "        topic = topic_out or topic\n",
        "\n",
        "        if DEBUG:\n",
        "            print(f\"üß† [summary] {summary}\\nüéØ [topic] {topic}\\n‚öôÔ∏è [action] {A_CHAT}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "j2iKouHWLoHC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0q5nlL3j8w3",
        "outputId": "b1fe2fcd-f4dc-4ef4-97c0-34e75d766b4f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Hi, I‚Äôm CredBot. We can chat normally. (Type 'exit' to quit.)\n",
            "\n",
            "You: hey how is it going\n",
            "ü§ñ Hi there! I'm doing great, thank you for asking.\n",
            "\n",
            "You: Thats great, id like to learn about ice cream\n",
            "üîç Searching for sources on ‚Äúice cream‚Äù‚Ä¶\n",
            "\n",
            "üì∞ Location | Van Leeuwen Ice Cream\n",
            "üîó https://vanleeuwenicecream.com/scoop-shops/\n",
            "üìÑ Store Hours: Sunday-Thursday 11am-12am Friday & Saturday 11am-1am Serving a full menu of our ice cream and vegan ice cream, chocolate chip cookies, cookie sandwiches, sundaes and milkshakes.  Store Hours: Open Daily from 11am-1am Serving a full menu of our ice cream and vegan ice cream, chocolate chip cookies, cookie sandwiches, sundaes and milkshakes.\n",
            "‚≠ê ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (80.1/100)\n",
            "üìù +12: uses HTTPS; -6: no clear author/byline; 0: no publication date found; +10: provides references (links: 172, external: 170); +6: substantive article length\n",
            "\n",
            "\n",
            "üì∞ Morgenstern's Finest Ice Cream\n",
            "üîó https://www.morgensternsnyc.com/?srsltid=AfmBOopzxIJjhcZ3sXpzvcBKb-qrfO9H_V1xWLIxkxv0WtqE-r4NoqgG\n",
            "üìÑ Shop our packs of 4 or 6 ice cream pints for National Shipping Shop Morgenstern's Finest ice cream cakes for local NYC pick-up or for once-weekly National Shipping Choose from our ice cream pies or freshly baked pies for local NYC pick-up ROWING BLAZERS COTTON PEACH SWEATER - YELLOW ROWING BLAZERS COTTON STRIPED SWEATER - BLUE ROWING BLAZERS TERRY BUTTON-UP UNIFORM - WHITE MORGENSTERN'S FINEST ICE CREAM: A COOKBOOK This site is protected by hCaptcha and the hCaptcha Privacy Policy and Terms of S‚Ä¶\n",
            "‚≠ê ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (68.2/100)\n",
            "üìù +12: uses HTTPS; -6: no clear author/byline; 0: no publication date found; +10: provides references (links: 26, external: 10); +6: substantive article length\n",
            "\n",
            "ü§ñ Anything else to explore?\n",
            "üß† [summary] The user expressed interest in learning about ice cream. The AI will now conduct a web search to gather information on this topic, shifting from the previous greeting exchange. Retrieved sources on ice cream.\n",
            "üéØ [topic] ice cream\n",
            "‚öôÔ∏è [action] chat_mode\n",
            "\n",
            "You: what is your favorite ice cream\n",
            "ü§ñ As an AI, I don't have the ability to taste or enjoy ice cream, so I don't have a favorite! What about you?\n",
            "\n",
            "You: Oh ok, exit then\n",
            "ü§ñ Understood! It sounds like you're ready to wrap up. Is there anything else I can help with or another topic you'd like to explore?\n",
            "\n",
            "You: exit\n",
            "üëã Goodbye!\n"
          ]
        }
      ]
    }
  ]
}