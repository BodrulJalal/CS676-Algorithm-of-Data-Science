{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwD3jKkF6G1LxN/vr1aOm1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Install all required packages (including the latest Gemini SDK)\n",
        "!pip install -U google-generativeai requests beautifulsoup4 tldextract python-dateutil lxml scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OHwpiPTOZ7z9",
        "outputId": "b6b7ea99-ec60-4957-b04a-80a242f41ff7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.14.2)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.12/dist-packages (5.3.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (2.9.0.post0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (6.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.7.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.183.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.12/dist-packages (from tldextract) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.19.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "#temporary serp api key so that I don't run out of api calls\n",
        "# os.environ[\"SERP_API_KEY\"] = \"test\"\n",
        "\n",
        "#real SERP_API_KEY\n",
        "os.environ[\"SERP_API_KEY\"] = userdata.get('SERP_API_KEY')\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "kzfWaqdIcaMG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ==========  C R E D I B I L I T Y   S C O R I N G  =========\n",
        "# ============================================================\n",
        "# This section is the *deliverable_1* rule-based scorer\n",
        "\n",
        "import re, time, json, math, tldextract, requests\n",
        "from urllib.parse import urlparse\n",
        "from datetime import datetime\n",
        "from dateutil import parser as dateparser\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- Networking defaults ---\n",
        "USER_AGENT = 'Mozilla/5.0 (CredibilityPOC/0.1)'   # Browser-y UA to avoid blocks\n",
        "DEFAULT_TIMEOUT = 12                               # Seconds\n",
        "\n",
        "# --- Heuristic signals ---\n",
        "CLICKBAIT_TERMS = [\n",
        "    \"you won't believe\", 'shocking', 'jaw-dropping', 'what happened next',\n",
        "    'unbelievable', 'miracle', 'exposed', \"secret they don't want you to know\"\n",
        "]\n",
        "TRANSPARENCY_HINTS = [\n",
        "    'author','byline','by ','by:','written by','editor','editorial',\n",
        "    'fact-check','fact check','sources','references','citations',\n",
        "    'methodology','about us','about the author','corrections','disclosures'\n",
        "]\n",
        "INSTITUTIONAL_TLDS = {'edu','gov','ac','sch','mil'}\n",
        "\n",
        "# def fetch_html(url: str):\n",
        "#     \"\"\"\n",
        "#     Fetch raw HTML for a URL.\n",
        "#     Returns (html_text, None) on success, or (None, 'error message') on failure.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         headers = {'User-Agent': USER_AGENT}\n",
        "#         resp = requests.get(url, headers=headers, timeout=DEFAULT_TIMEOUT)\n",
        "#         resp.raise_for_status()\n",
        "#         return resp.text, None\n",
        "#     except Exception as e:\n",
        "#         return None, f'Fetch error: {e}'\n",
        "#REPLACED TO ADD RETRIES FOR EDGE CASE:\n",
        "# adds:\n",
        "# Header rotation (Chrome/Safari/user agent)\n",
        "# Backoff retry on 403/429/5xx\n",
        "# Reddit fallback to old.reddit.com (often less strict)\n",
        "# Ensures we actually got HTML\n",
        "\n",
        "def fetch_html(url: str):\n",
        "    \"\"\"\n",
        "    Fetch raw HTML for a URL with retries and basic anti-block heuristics.\n",
        "    - Rotates realistic headers\n",
        "    - Exponential backoff on 403/429/5xx\n",
        "    - Fallback for Reddit: try old.reddit.com if www.reddit.com blocks\n",
        "    Returns (html_text, None) on success, or (None, 'error message') on failure.\n",
        "    \"\"\"\n",
        "    import random\n",
        "    import time\n",
        "    import requests\n",
        "    from urllib.parse import urlparse\n",
        "\n",
        "    # Primary + backup user-agent/header sets\n",
        "    header_candidates = [\n",
        "        {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                          \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                          \"Chrome/126.0 Safari/537.36\",\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "            \"Connection\": \"keep-alive\",\n",
        "        },\n",
        "        {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
        "                          \"AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\",\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        },\n",
        "        {\n",
        "            \"User-Agent\": USER_AGENT,  # your original UA, as a last resort\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    def reddit_alt(u: str) -> list[str]:\n",
        "        \"\"\"If it's a Reddit URL, also try old.reddit.com.\"\"\"\n",
        "        try:\n",
        "            parsed = urlparse(u)\n",
        "            if parsed.netloc.endswith(\"reddit.com\") and not parsed.netloc.startswith(\"old.\"):\n",
        "                alt = u.replace(\"//www.reddit.com\", \"//old.reddit.com\")\n",
        "                if alt == u:  # if it wasn't www, still try old.\n",
        "                    alt = u.replace(\"//reddit.com\", \"//old.reddit.com\")\n",
        "                return [u, alt]\n",
        "        except Exception:\n",
        "            pass\n",
        "        return [u]\n",
        "\n",
        "    urls_to_try = reddit_alt(url)\n",
        "    last_err = None\n",
        "\n",
        "    for candidate_url in urls_to_try:\n",
        "        # up to 3 attempts per candidate url, with exponential backoff\n",
        "        backoff = 1.0\n",
        "        for attempt in range(3):\n",
        "            headers = header_candidates[min(attempt, len(header_candidates)-1)]\n",
        "            try:\n",
        "                resp = requests.get(candidate_url, headers=headers, timeout=DEFAULT_TIMEOUT, allow_redirects=True)\n",
        "                status = resp.status_code\n",
        "\n",
        "                # Retry on \"blocked\"/rate-limited or transient server errors\n",
        "                if status in (403, 429) or 500 <= status < 600:\n",
        "                    last_err = f\"{status} {resp.reason}\"\n",
        "                    time.sleep(backoff)\n",
        "                    backoff *= 2.0\n",
        "                    continue\n",
        "\n",
        "                resp.raise_for_status()\n",
        "\n",
        "                # Prefer HTML content; some sites return other types\n",
        "                ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
        "                if \"text/html\" not in ctype:\n",
        "                    last_err = f\"Non-HTML content-type: {ctype}\"\n",
        "                    # Don‚Äôt retry endlessly for non-HTML; move to next candidate/final\n",
        "                    break\n",
        "\n",
        "                return resp.text, None\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                last_err = str(e)\n",
        "                time.sleep(backoff)\n",
        "                backoff *= 2.0\n",
        "                continue\n",
        "\n",
        "    return None, f\"Fetch error: {last_err or 'unknown error'}\"\n",
        "\n",
        "\n",
        "def extract_article_fields(html: str, url: str):\n",
        "    \"\"\"\n",
        "    Parse HTML to extract: title, author, published date, body text (paragraphs),\n",
        "    and link counts (total + external), plus transparency hint flag.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    text_chunks, title, author, published = [], None, None, None\n",
        "\n",
        "    # --- Title from <title> or OG meta ---\n",
        "    if soup.title and soup.title.string:\n",
        "        title = soup.title.string.strip()\n",
        "    mt = soup.find('meta', attrs={'property':'og:title'}) or soup.find('meta', attrs={'name':'title'})\n",
        "    if not title and mt and mt.get('content'):\n",
        "        title = mt['content'].strip()\n",
        "\n",
        "    # --- Author / byline in common locations ---\n",
        "    for selector in [\n",
        "        {'name':'meta','attrs':{'name':'author'}},\n",
        "        {'name':'meta','attrs':{'property':'article:author'}},\n",
        "        {'name':'span','class_':re.compile('author|byline', re.I)},\n",
        "        {'name':'div','class_':re.compile('author|byline', re.I)},\n",
        "        {'name':'a','class_':re.compile('author', re.I)},\n",
        "    ]:\n",
        "        if selector['name']=='meta':\n",
        "            node = soup.find('meta', attrs=selector['attrs'])\n",
        "            if node and node.get('content'):\n",
        "                author = node['content'].strip(); break\n",
        "        else:\n",
        "            node = soup.find(selector['name'], class_=selector.get('class_'))\n",
        "            if node and node.get_text(strip=True):\n",
        "                candidate = node.get_text(' ', strip=True)\n",
        "                if len(candidate) >= 3:\n",
        "                    author = candidate; break\n",
        "\n",
        "    # --- Publish date in common meta/time/span patterns ---\n",
        "    for date_sel in [\n",
        "        {'name':'meta','attrs':{'property':'article:published_time'}},\n",
        "        {'name':'meta','attrs':{'name':'date'}},\n",
        "        {'name':'time','attrs':{}},\n",
        "        {'name':'span','class_':re.compile('date|time', re.I)},\n",
        "    ]:\n",
        "        if date_sel['name']=='meta':\n",
        "            node = soup.find('meta', attrs=date_sel['attrs'])\n",
        "            if node and node.get('content'):\n",
        "                try:\n",
        "                    published = dateparser.parse(node['content'], fuzzy=True); break\n",
        "                except Exception:\n",
        "                    pass\n",
        "        else:\n",
        "            node = soup.find(date_sel['name'], class_=date_sel.get('class_'))\n",
        "            if node and node.get_text(strip=True):\n",
        "                try:\n",
        "                    published = dateparser.parse(node.get_text(strip=True), fuzzy=True); break\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    # --- Body text: prefer a likely article container, else all <p> ---\n",
        "    main_container = None\n",
        "    for cls in ['article','post','story','content','entry-content','article-body']:\n",
        "        mc = soup.find(True, class_=re.compile(cls, re.I))\n",
        "        if mc: main_container = mc; break\n",
        "    paragraphs = (main_container.find_all('p') if main_container else soup.find_all('p'))\n",
        "    for p in paragraphs:\n",
        "        t = p.get_text(' ', strip=True)\n",
        "        if t and len(t) > 40: text_chunks.append(t)\n",
        "    article_text = '\\n\\n'.join(text_chunks)[:100000]  # cap to avoid huge pages\n",
        "\n",
        "    # --- Link counts: total & external ---\n",
        "    all_links, external_links = [], []\n",
        "    base_host = urlparse(url).netloc.lower()\n",
        "    for a in soup.find_all('a', href=True):\n",
        "        href = a['href']\n",
        "        if href.startswith('http://') or href.startswith('https://'):\n",
        "            all_links.append(href)\n",
        "            if urlparse(href).netloc.lower() != base_host:\n",
        "                external_links.append(href)\n",
        "\n",
        "    # --- Transparency hint flag (string match) ---\n",
        "    full_text_for_hints = (article_text + ' ' + ' '.join(TRANSPARENCY_HINTS)).lower()\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'author': author,\n",
        "        'published': published.isoformat() if published else None,\n",
        "        'text': article_text,\n",
        "        'num_paragraphs': len(text_chunks),\n",
        "        'all_links': all_links,\n",
        "        'external_links': external_links,\n",
        "        'has_transparency_hints': any(h in full_text_for_hints for h in TRANSPARENCY_HINTS),\n",
        "    }\n",
        "\n",
        "def score_url(url: str, fields: dict):\n",
        "    \"\"\"\n",
        "    Apply heuristic scoring rules ‚Üí (score 0‚Äì100, explanation string).\n",
        "    Starts at 50 and adds/subtracts per signal.\n",
        "    \"\"\"\n",
        "    explanation_bits = []\n",
        "    score = 50  # neutral baseline\n",
        "\n",
        "    # HTTPS\n",
        "    if url.lower().startswith('https://'):\n",
        "        score += 12; explanation_bits.append('+12: uses HTTPS')\n",
        "    else:\n",
        "        score -= 10; explanation_bits.append('-10: not using HTTPS')\n",
        "\n",
        "    # Institutional TLD\n",
        "    ext = tldextract.extract(url)\n",
        "    tld_last = (ext.suffix.split('.')[-1] if ext.suffix else '')\n",
        "    if tld_last in INSTITUTIONAL_TLDS:\n",
        "        score += 14; explanation_bits.append(f'+14: institutional TLD ({tld_last})')\n",
        "\n",
        "    # Author/byline\n",
        "    if fields.get('author'):\n",
        "        score += 10; explanation_bits.append('+10: author/byline found')\n",
        "    else:\n",
        "        score -= 6; explanation_bits.append('-6: no clear author/byline')\n",
        "\n",
        "    # Published recency (NOTE: uses datetime.utcnow(), may warn in 3.12+)\n",
        "    published = fields.get('published')\n",
        "    if published:\n",
        "        try:\n",
        "            dt = dateparser.parse(published)\n",
        "            if (datetime.utcnow() - dt).days <= 3650:\n",
        "                score += 6; explanation_bits.append('+6: reasonably recent publication date')\n",
        "            else:\n",
        "                score -= 4; explanation_bits.append('-4: appears quite old')\n",
        "        except Exception:\n",
        "            explanation_bits.append('0: could not parse publication date reliably')\n",
        "    else:\n",
        "        explanation_bits.append('0: no publication date found')\n",
        "\n",
        "    # References\n",
        "    total_links = len(fields.get('all_links', []))\n",
        "    external_links_count = len(fields.get('external_links', []))\n",
        "    if total_links >= 5 and external_links_count >= 3:\n",
        "        score += 10; explanation_bits.append(f'+10: provides references (links: {total_links}, external: {external_links_count})')\n",
        "    elif total_links >= 2:\n",
        "        score += 4; explanation_bits.append(f'+4: some references (links: {total_links})')\n",
        "    else:\n",
        "        score -= 6; explanation_bits.append(f'-6: minimal/no references (links: {total_links})')\n",
        "\n",
        "    # Length (by paragraph count)\n",
        "    num_paras = fields.get('num_paragraphs', 0)\n",
        "    if num_paras >= 8:\n",
        "        score += 6; explanation_bits.append('+6: substantive article length')\n",
        "    elif num_paras >= 3:\n",
        "        score += 2; explanation_bits.append('+2: moderate article length')\n",
        "    else:\n",
        "        score -= 6; explanation_bits.append('-6: very short article text')\n",
        "\n",
        "    # Clickbait language\n",
        "    text_lower = (fields.get('text') or '').lower()\n",
        "    clickbait_hits = sum(1 for term in CLICKBAIT_TERMS if term in text_lower)\n",
        "    if clickbait_hits >= 2:\n",
        "        score -= 10; explanation_bits.append('-10: strong clickbait indicators')\n",
        "    elif clickbait_hits == 1:\n",
        "        score -= 4; explanation_bits.append('-4: mild clickbait indicators')\n",
        "\n",
        "    # Advertising/sponsor cues\n",
        "    ad_signals = len(re.findall(r\"advertis(e|ement)|sponsor(ed|ship)\", text_lower))\n",
        "    iframes_penalty = min(8, math.floor(ad_signals / 5) * 2)\n",
        "    if iframes_penalty:\n",
        "        score -= iframes_penalty; explanation_bits.append(f'-{iframes_penalty}: advertising/sponsorship language')\n",
        "\n",
        "    # Clamp score and join explanation\n",
        "    score = max(0, min(100, int(round(score))))\n",
        "    explanation = '; '.join(explanation_bits)\n",
        "    return score, explanation\n",
        "\n",
        "def evaluate_url(url: str):\n",
        "    \"\"\"\n",
        "    Orchestrator: fetch ‚Üí parse ‚Üí score.\n",
        "    Returns a dict with top-level 'score', 'explanation', and 'details' for display/debug.\n",
        "    \"\"\"\n",
        "    if not isinstance(url, str) or not url.strip():\n",
        "        return {'score': 0, 'explanation': 'Invalid URL input.', 'details': {'error': 'empty_or_non_string'}}\n",
        "\n",
        "    html, err = fetch_html(url)\n",
        "    if err:\n",
        "        return {'score': 0, 'explanation': f'Failed to fetch: {err}', 'details': {'error': 'fetch_failed', 'url': url}}\n",
        "\n",
        "    fields = extract_article_fields(html, url)\n",
        "    score, explanation = score_url(url, fields)\n",
        "\n",
        "    return {\n",
        "        'score': score,\n",
        "        'explanation': explanation,\n",
        "        'details': {\n",
        "            'url': url,\n",
        "            'title': fields.get('title'),\n",
        "            'author': fields.get('author'),\n",
        "            'published': fields.get('published'),\n",
        "            'num_paragraphs': fields.get('num_paragraphs'),\n",
        "            'total_links': len(fields.get('all_links', [])),\n",
        "            'external_links': len(fields.get('external_links', [])),\n",
        "        },\n",
        "    }\n",
        "\n",
        "# Optional quick smoke test:\n",
        "# print(json.dumps(evaluate_url('https://www.mayoclinic.org/diseases-conditions/dehydration/symptoms-causes/syc-20354086'), indent=2))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ======  H Y B R I D   ( R u l e s  +  L i n e a r  R e g ) =\n",
        "# ============================================================\n",
        "# This section:\n",
        "# 1) builds features from evaluate_url() outputs\n",
        "# 2) creates a labeled dataset of 20 real URLs (0‚Äì100 labels)\n",
        "# 3) runs 5-fold cross-validation for LinearRegression\n",
        "# 4) fits a final model on ALL available rows\n",
        "# 5) exposes hybrid_score(url, alpha) for inference\n",
        "#\n",
        "# Notes:\n",
        "# - If some URLs fail to fetch, they're skipped gracefully.\n",
        "# - CV uses MAE and R^2 to give you both error and fit quality.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from datetime import datetime, timezone\n",
        "from dateutil import parser as dateparser\n",
        "\n",
        "# ---------- Feature Engineering ----------\n",
        "def _safe(value, default=0):\n",
        "    return value if value is not None else default\n",
        "\n",
        "def _https_flag(url: str) -> int:\n",
        "    return 1 if isinstance(url, str) and url.lower().startswith('https://') else 0\n",
        "\n",
        "def _institutional_tld_flag(url: str) -> int:\n",
        "    try:\n",
        "        ext = tldextract.extract(url)\n",
        "        tld_last = (ext.suffix.split('.')[-1] if ext.suffix else '')\n",
        "        return 1 if tld_last in {'edu','gov','ac','sch','mil'} else 0\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "def _days_since(published_iso: str) -> int:\n",
        "    if not published_iso:\n",
        "        return 99999  # treat unknown/absent as very old\n",
        "    try:\n",
        "        dt = dateparser.parse(published_iso)\n",
        "        return max(0, (datetime.utcnow() - dt).days)  # OK to mirror your original utc usage\n",
        "    except Exception:\n",
        "        return 99999\n",
        "\n",
        "def features_from_eval(eval_obj: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Build a feature dict using ONLY fields the original scorer returns.\n",
        "    These features are model-agnostic and cheap to compute.\n",
        "    \"\"\"\n",
        "    d = eval_obj.get('details', {})\n",
        "    url = d.get('url') or ''\n",
        "    feats = {\n",
        "        \"https\": _https_flag(url),\n",
        "        \"inst_tld\": _institutional_tld_flag(url),\n",
        "        \"has_author\": 1 if d.get('author') else 0,\n",
        "        \"num_paragraphs\": _safe(d.get('num_paragraphs'), 0),\n",
        "        \"total_links\": _safe(d.get('total_links'), 0),\n",
        "        \"external_links\": _safe(d.get('external_links'), 0),\n",
        "        \"days_since_pub\": _days_since(d.get('published')),\n",
        "    }\n",
        "    return feats\n",
        "\n",
        "FEATURE_ORDER = [\n",
        "    \"https\",\n",
        "    \"inst_tld\",\n",
        "    \"has_author\",\n",
        "    \"num_paragraphs\",\n",
        "    \"total_links\",\n",
        "    \"external_links\",\n",
        "    \"days_since_pub\",\n",
        "]\n",
        "\n",
        "def vectorize_features(feat_dict: dict, feature_order=FEATURE_ORDER):\n",
        "    \"\"\"Convert a feature dict to a fixed-order numeric vector.\"\"\"\n",
        "    return np.array([feat_dict.get(k, 0) for k in feature_order], dtype=float)\n",
        "\n",
        "# ---------- Labeled Dataset (20 REAL URLs, mixed quality) ----------\n",
        "# Labels are illustrative 0‚Äì100 targets. Adjust as you refine ground truth.\n",
        "LABELED_URLS = [\n",
        "    # Highly credible / institutional / quality editorial\n",
        "    (\"https://www.cdc.gov/flu/about/index.html\", 92),\n",
        "    (\"https://www.nih.gov/news-events/news-releases\", 88),\n",
        "    (\"https://www.mayoclinic.org/diseases-conditions/dehydration/symptoms-causes/syc-20354086\", 85),\n",
        "    (\"https://www.who.int/news-room/fact-sheets/detail/diabetes\", 90),\n",
        "    (\"https://www.britannica.com/science/photosynthesis\", 86),\n",
        "    (\"https://www.health.harvard.edu/staying-healthy/what-is-intermittent-fasting\", 78),\n",
        "    (\"https://www.hopkinsmedicine.org/health/conditions-and-diseases\", 84),\n",
        "    (\"https://www.bbc.com/news/science_and_environment\", 80),\n",
        "    (\"https://www.reuters.com/world/us/\", 80),\n",
        "    (\"https://apnews.com/hub/technology\", 78),\n",
        "    (\"https://en.wikipedia.org/wiki/Ice_cream\", 75),\n",
        "\n",
        "    # Mid credibility consumer health / informative\n",
        "    (\"https://www.healthline.com/nutrition/green-tea-and-weight-loss\", 70),\n",
        "    (\"https://www.webmd.com/diet/obesity/features/green-tea-and-weight-loss\", 68),\n",
        "    (\"https://www.nature.com/scitable/definition/photosynthesis-288/\", 82),\n",
        "    (\"https://med.stanford.edu/news/all-news.html\", 82),\n",
        "\n",
        "    # Lower credibility / UGC / lighter editorial\n",
        "    (\"https://medium.com/\", 45),\n",
        "    (\"https://www.reddit.com/r/icecreamery/comments/19elt19/looking_for_resources_to_learn_how_to_make_ice/\", 20),\n",
        "    (\"https://www.quora.com/Is-green-tea-good-for-weight-loss\", 25),\n",
        "    (\"https://www.livestrong.com/article/13715706-green-tea-benefits/\", 60),\n",
        "    (\"https://www.buzzfeed.com/\", 40),\n",
        "]\n",
        "\n",
        "# ---------- Build Dataset by Evaluating & Featurizing ----------\n",
        "rows_X, rows_y, skipped = [], [], []\n",
        "\n",
        "for url, label in LABELED_URLS:\n",
        "    try:\n",
        "        ev = evaluate_url(url)\n",
        "        if ev.get(\"details\", {}).get(\"error\") == \"fetch_failed\":\n",
        "            skipped.append((url, \"fetch_failed\"))\n",
        "            continue\n",
        "        feats = features_from_eval(ev)\n",
        "        x = vectorize_features(feats)\n",
        "        rows_X.append(x)\n",
        "        rows_y.append(float(label))\n",
        "    except Exception as e:\n",
        "        skipped.append((url, str(e)))\n",
        "\n",
        "X = np.vstack(rows_X) if rows_X else np.zeros((0, len(FEATURE_ORDER)))\n",
        "y = np.array(rows_y) if rows_y else np.zeros((0,))\n",
        "\n",
        "print(f\"Prepared dataset: {X.shape[0]} rows √ó {X.shape[1]} features. Skipped: {len(skipped)}\")\n",
        "if skipped:\n",
        "    print(\"Skipped examples (first few):\", skipped[:3])\n",
        "\n",
        "# ---------- 5-Fold Cross-Validation ----------\n",
        "# We use KFold regression CV with two metrics:\n",
        "#  - MAE (lower is better): absolute error in points of the 0‚Äì100 score\n",
        "#  - R^2 (higher is better): variance explained\n",
        "if len(X) >= 5:\n",
        "    k = min(5, len(X))  # guard in case many rows were skipped\n",
        "    if k < 2:\n",
        "        print(\"\\nNot enough rows for CV; skipping cross-validation.\")\n",
        "    else:\n",
        "        kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "        # MAE (scikit returns negative MAE for loss; we invert sign to report positive MAE)\n",
        "        mae_scores = cross_val_score(LinearRegression(), X, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
        "        r2_scores  = cross_val_score(LinearRegression(), X, y, cv=kf, scoring=\"r2\")\n",
        "        mae_vals = -mae_scores  # turn to positive\n",
        "        print(f\"\\n5-fold CV (k={k}) results on {len(X)} rows:\")\n",
        "        print(f\"  MAE per fold: {np.round(mae_vals, 2)} | mean={mae_vals.mean():.2f}\")\n",
        "        print(f\"  R^2 per fold:  {np.round(r2_scores, 3)} | mean={r2_scores.mean():.3f}\")\n",
        "else:\n",
        "    print(\"\\nNot enough rows for CV; need ‚â•5 examples.\")\n",
        "\n",
        "# ---------- Fit Final Linear Regression on ALL available rows ----------\n",
        "linreg = LinearRegression()\n",
        "if len(X) >= 2:\n",
        "    linreg.fit(X, y)\n",
        "    print(\"\\nFinal model (trained on all available rows):\")\n",
        "    for kname, w in zip(FEATURE_ORDER, linreg.coef_):\n",
        "        print(f\"  {kname:>16s}: {w: .3f}\")\n",
        "    print(f\"  {'Intercept':>16s}: {linreg.intercept_: .3f}\")\n",
        "else:\n",
        "    print(\"\\nNot enough rows to train final model (need ‚â•2). Using neutral ML predictions.\")\n",
        "\n",
        "# ---------- Inference helpers ----------\n",
        "def predict_ml_score_from_eval(eval_obj: dict) -> float:\n",
        "    \"\"\"\n",
        "    Predict a 0‚Äì100 credibility score from features using the trained LinearRegression.\n",
        "    Falls back to 50.0 if not enough training data is available.\n",
        "    \"\"\"\n",
        "    if len(X) < 2:\n",
        "        return 50.0\n",
        "    feats = features_from_eval(eval_obj)\n",
        "    x = vectorize_features(feats).reshape(1, -1)\n",
        "    pred = linreg.predict(x)[0]\n",
        "    return float(np.clip(pred, 0.0, 100.0))\n",
        "\n",
        "def _stars(score_0_100: float) -> str:\n",
        "    stars = int(round(score_0_100 / 20))\n",
        "    stars = max(0, min(5, stars))\n",
        "    return \"‚òÖ\"*stars + \"‚òÜ\"*(5 - stars)\n",
        "\n",
        "# def hybrid_score(url: str, alpha: float = 0.6) -> dict:\n",
        "#     \"\"\"\n",
        "#     Blend rule-based score with ML-predicted score:\n",
        "#         final = alpha * rule_score + (1 - alpha) * ml_score\n",
        "#     Returns a dict with scores, stars, and the rule explanation for transparency.\n",
        "#     \"\"\"\n",
        "#     ev = evaluate_url(url)\n",
        "#     rule_score = float(ev.get(\"score\", 0.0))\n",
        "#     ml_score = predict_ml_score_from_eval(ev)\n",
        "#     final = float(np.clip(alpha * rule_score + (1 - alpha) * ml_score, 0.0, 100.0))\n",
        "#     return {\n",
        "#         \"url\": url,\n",
        "#         \"title\": ev.get(\"details\", {}).get(\"title\"),\n",
        "#         \"rule_score\": round(rule_score, 1),\n",
        "#         \"ml_score\": round(ml_score, 1),\n",
        "#         \"hybrid_score\": round(final, 1),\n",
        "#         \"stars\": _stars(final),\n",
        "#         \"explanation\": ev.get(\"explanation\"),\n",
        "#         \"details\": ev.get(\"details\"),\n",
        "#     }\n",
        "# REPLACED TO CONSIDER EDGE CASE OF FAILED FETCHING:\n",
        "def hybrid_score(url: str, alpha: float = 0.6, fetch_fail_floor: float = 20.0) -> dict:\n",
        "    \"\"\"\n",
        "    Blend rule-based score with ML-predicted score:\n",
        "        final = alpha * rule_score + (1 - alpha) * ml_score\n",
        "\n",
        "    Edge case handling:\n",
        "    - If fetch failed, we KEEP a low‚Äîbut non-zero‚Äîscore (fetch_fail_floor),\n",
        "      skip ML, and return a clear explanation about limited accessibility.\n",
        "    - Otherwise, proceed normally.\n",
        "\n",
        "    Args:\n",
        "        url: page to score\n",
        "        alpha: weight for RULE score (0..1). (1-alpha) goes to ML score.\n",
        "        fetch_fail_floor: low fallback score (0..100) used when fetch fails.\n",
        "\n",
        "    Returns:\n",
        "        dict with rule_score, ml_score, hybrid_score, stars, explanation, details.\n",
        "    \"\"\"\n",
        "    ev = evaluate_url(url)\n",
        "    details = ev.get(\"details\", {})\n",
        "    rule_score = float(ev.get(\"score\", 0.0))\n",
        "\n",
        "    # üß© Edge case: fetch failed\n",
        "    if details.get(\"error\") == \"fetch_failed\":\n",
        "        final = float(np.clip(fetch_fail_floor, 0.0, 100.0))  # keep a LOW score, not zero\n",
        "        # Friendly explanation (prepend ours, keep the original failure note too if you like)\n",
        "        friendly = (\n",
        "            \"Credibility is low due to limited accessible information ‚Äî \"\n",
        "            \"the page could not be fetched or analyzed (e.g., the site blocked automated requests).\"\n",
        "        )\n",
        "        base_expl = ev.get(\"explanation\") or \"\"\n",
        "        full_expl = f\"{friendly} {(' | ' + base_expl) if base_expl else ''}\".strip()\n",
        "\n",
        "        # Stars from final score\n",
        "        stars = int(round(final / 20))\n",
        "        stars = max(0, min(5, stars))\n",
        "        star_str = \"‚òÖ\" * stars + \"‚òÜ\" * (5 - stars)\n",
        "\n",
        "        return {\n",
        "            \"url\": url,\n",
        "            \"title\": details.get(\"title\"),\n",
        "            \"rule_score\": round(rule_score, 1),  # usually 0 on fetch fail, but shown for transparency\n",
        "            \"ml_score\": 0.0,                      # we skip ML when fetch failed\n",
        "            \"hybrid_score\": round(final, 1),\n",
        "            \"stars\": star_str,\n",
        "            \"explanation\": full_expl,\n",
        "            \"details\": details,\n",
        "        }\n",
        "\n",
        "    # ‚úÖ Normal path: combine rules + ML\n",
        "    ml_score = predict_ml_score_from_eval(ev)\n",
        "    final = float(np.clip(alpha * rule_score + (1 - alpha) * ml_score, 0.0, 100.0))\n",
        "\n",
        "    # Stars\n",
        "    stars = int(round(final / 20))\n",
        "    stars = max(0, min(5, stars))\n",
        "    star_str = \"‚òÖ\" * stars + \"‚òÜ\" * (5 - stars)\n",
        "\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"title\": details.get(\"title\"),\n",
        "        \"rule_score\": round(rule_score, 1),\n",
        "        \"ml_score\": round(ml_score, 1),\n",
        "        \"hybrid_score\": round(final, 1),\n",
        "        \"stars\": star_str,\n",
        "        \"explanation\": ev.get(\"explanation\", \"No detailed explanation available.\"),\n",
        "        \"details\": details,\n",
        "    }\n",
        "\n",
        "# ---------- Demo on a few URLs ----------\n",
        "demo_urls = [\n",
        "    \"https://www.cdc.gov/flu/about/index.html\",\n",
        "    \"https://www.mayoclinic.org/diseases-conditions/dehydration/symptoms-causes/syc-20354086\",\n",
        "    \"https://www.healthline.com/nutrition/green-tea-and-weight-loss\",\n",
        "    \"https://www.reddit.com/r/icecreamery/comments/19elt19/looking_for_resources_to_learn_how_to_make_ice/\",\n",
        "    \"https://www.buzzfeed.com/\"\n",
        "]\n",
        "\n",
        "for u in demo_urls:\n",
        "    try:\n",
        "        res = hybrid_score(u, alpha=0.6)  # 60% rules / 40% ML\n",
        "        print(\"\\n\" + \"‚Äî\"*70)\n",
        "        print(f\"üîó {res['url']}\")\n",
        "        print(f\"üì∞ {res['title'] or '[No Title]'}\")\n",
        "        print(f\"‚≠ê {res['stars']}  ({res['hybrid_score']}/100)  |  Rule: {res['rule_score']}  ML: {res['ml_score']}\")\n",
        "        print(f\"üìù Why: {res['explanation']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error on {u}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BTtvEQ7VR8vn",
        "outputId": "17cee1d2-d324-4900-9749-f55c18a4d443"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2069810422.py:256: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  if (datetime.utcnow() - dt).days <= 3650:\n",
            "/tmp/ipython-input-2069810422.py:376: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return max(0, (datetime.utcnow() - dt).days)  # OK to mirror your original utc usage\n",
            "/usr/local/lib/python3.12/dist-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname BST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
            "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared dataset: 14 rows √ó 7 features. Skipped: 6\n",
            "Skipped examples (first few): [('https://www.health.harvard.edu/staying-healthy/what-is-intermittent-fasting', 'fetch_failed'), ('https://www.webmd.com/diet/obesity/features/green-tea-and-weight-loss', 'fetch_failed'), ('https://www.nature.com/scitable/definition/photosynthesis-288/', 'fetch_failed')]\n",
            "\n",
            "5-fold CV (k=5) results on 14 rows:\n",
            "  MAE per fold: [ 40.78 135.69  17.67 113.8    5.25] | mean=62.64\n",
            "  R^2 per fold:  [-7.93610e+01 -1.51254e+02  1.54000e-01 -6.42219e+02 -8.30000e-01] | mean=-174.702\n",
            "\n",
            "Final model (trained on all available rows):\n",
            "             https:  0.000\n",
            "          inst_tld:  25.909\n",
            "        has_author: -5.094\n",
            "    num_paragraphs:  0.191\n",
            "       total_links:  0.013\n",
            "    external_links:  0.011\n",
            "    days_since_pub: -0.000\n",
            "         Intercept:  82.622\n",
            "\n",
            "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
            "üîó https://www.cdc.gov/flu/about/index.html\n",
            "üì∞ About Influenza | Influenza (Flu) | CDC\n",
            "‚≠ê ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ  (93.6/100)  |  Rule: 98.0  ML: 87.1\n",
            "üìù Why: +12: uses HTTPS; +14: institutional TLD (gov); +10: author/byline found; 0: no publication date found; +10: provides references (links: 96, external: 31); +6: substantive article length; -4: mild clickbait indicators\n",
            "\n",
            "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
            "üîó https://www.mayoclinic.org/diseases-conditions/dehydration/symptoms-causes/syc-20354086\n",
            "üì∞ Dehydration - Symptoms & causes - Mayo Clinic\n",
            "‚≠ê ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ  (61.4/100)  |  Rule: 60.0  ML: 63.5\n",
            "üìù Why: +12: uses HTTPS; -6: no clear author/byline; 0: no publication date found; +10: provides references (links: 138, external: 110); -6: very short article text\n",
            "\n",
            "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
            "üîó https://www.healthline.com/nutrition/green-tea-and-weight-loss\n",
            "üì∞ Green Tea for Weight Loss: How it Works\n",
            "‚≠ê ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ  (72.7/100)  |  Rule: 72.0  ML: 73.7\n",
            "üìù Why: +12: uses HTTPS; -6: no clear author/byline; 0: no publication date found; +10: provides references (links: 248, external: 72); +6: substantive article length\n",
            "\n",
            "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
            "üîó https://www.reddit.com/r/icecreamery/comments/19elt19/looking_for_resources_to_learn_how_to_make_ice/\n",
            "üì∞ [No Title]\n",
            "‚≠ê ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ  (20.0/100)  |  Rule: 0.0  ML: 0.0\n",
            "üìù Why: Credibility is low due to limited accessible information ‚Äî the page could not be fetched or analyzed (e.g., the site blocked automated requests).  | Failed to fetch: Fetch error: 403 Blocked\n",
            "\n",
            "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
            "üîó https://www.buzzfeed.com/\n",
            "üì∞ BuzzFeed\n",
            "‚≠ê ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ  (60.8/100)  |  Rule: 60.0  ML: 62.1\n",
            "üìù Why: +12: uses HTTPS; -6: no clear author/byline; 0: no publication date found; +10: provides references (links: 116, external: 6); -6: very short article text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRMep1K6ZtUS",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# # DELIVERABLE_1\n",
        "# # ============================================\n",
        "# # FILE: credibility_scoring.py\n",
        "# # ============================================\n",
        "# # This module implements the credibility scoring logic for articles\n",
        "\n",
        "# import re, time, json, math, tldextract, requests\n",
        "# from urllib.parse import urlparse\n",
        "# from datetime import datetime\n",
        "# from dateutil import parser as dateparser\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "# # Define a custom user agent to mimic a browser\n",
        "# USER_AGENT = 'Mozilla/5.0 (CredibilityPOC/0.1)'\n",
        "# DEFAULT_TIMEOUT = 12\n",
        "\n",
        "# # List of common clickbait terms to detect low-quality articles\n",
        "# CLICKBAIT_TERMS = [\n",
        "#     \"you won't believe\", 'shocking', 'jaw-dropping', 'what happened next',\n",
        "#     'unbelievable', 'miracle', 'exposed', \"secret they don't want you to know\"\n",
        "# ]\n",
        "\n",
        "# # Signals of editorial transparency that indicate credibility\n",
        "# TRANSPARENCY_HINTS = [\n",
        "#     'author','byline','by ','by:','written by','editor','editorial',\n",
        "#     'fact-check','fact check','sources','references','citations',\n",
        "#     'methodology','about us','about the author','corrections','disclosures'\n",
        "# ]\n",
        "\n",
        "# # Institutional top-level domains are considered more credible\n",
        "# INSTITUTIONAL_TLDS = {'edu','gov','ac','sch','mil'}\n",
        "\n",
        "# # Function to fetch the HTML content of a URL\n",
        "# def fetch_html(url: str):\n",
        "#     try:\n",
        "#         headers = {'User-Agent': USER_AGENT}\n",
        "#         resp = requests.get(url, headers=headers, timeout=DEFAULT_TIMEOUT)\n",
        "#         resp.raise_for_status()\n",
        "#         return resp.text, None\n",
        "#     except Exception as e:\n",
        "#         return None, f'Fetch error: {e}'\n",
        "\n",
        "# # Function to extract useful metadata and content from an article\n",
        "# def extract_article_fields(html: str, url: str):\n",
        "#     soup = BeautifulSoup(html, 'lxml')\n",
        "#     text_chunks, title, author, published = [], None, None, None\n",
        "\n",
        "#     # Attempt to extract the title from multiple locations\n",
        "#     if soup.title and soup.title.string:\n",
        "#         title = soup.title.string.strip()\n",
        "#     mt = soup.find('meta', attrs={'property':'og:title'}) or soup.find('meta', attrs={'name':'title'})\n",
        "#     if not title and mt and mt.get('content'):\n",
        "#         title = mt['content'].strip()\n",
        "\n",
        "#     # Try to find author info\n",
        "#     for selector in [\n",
        "#         {'name':'meta','attrs':{'name':'author'}},\n",
        "#         {'name':'meta','attrs':{'property':'article:author'}},\n",
        "#         {'name':'span','class_':re.compile('author|byline', re.I)},\n",
        "#         {'name':'div','class_':re.compile('author|byline', re.I)},\n",
        "#         {'name':'a','class_':re.compile('author', re.I)},\n",
        "#     ]:\n",
        "#         if selector['name']=='meta':\n",
        "#             node = soup.find('meta', attrs=selector['attrs'])\n",
        "#             if node and node.get('content'):\n",
        "#                 author = node['content'].strip(); break\n",
        "#         else:\n",
        "#             node = soup.find(selector['name'], class_=selector.get('class_'))\n",
        "#             if node and node.get_text(strip=True):\n",
        "#                 candidate = node.get_text(' ', strip=True)\n",
        "#                 if len(candidate) >= 3:\n",
        "#                     author = candidate; break\n",
        "\n",
        "#     # Try to find a publish date\n",
        "#     for date_sel in [\n",
        "#         {'name':'meta','attrs':{'property':'article:published_time'}},\n",
        "#         {'name':'meta','attrs':{'name':'date'}},\n",
        "#         {'name':'time','attrs':{}},\n",
        "#         {'name':'span','class_':re.compile('date|time', re.I)},\n",
        "#     ]:\n",
        "#         if date_sel['name']=='meta':\n",
        "#             node = soup.find('meta', attrs=date_sel['attrs'])\n",
        "#             if node and node.get('content'):\n",
        "#                 try:\n",
        "#                     published = dateparser.parse(node['content'], fuzzy=True); break\n",
        "#                 except Exception:\n",
        "#                     pass\n",
        "#         else:\n",
        "#             node = soup.find(date_sel['name'], class_=date_sel.get('class_'))\n",
        "#             if node and node.get_text(strip=True):\n",
        "#                 try:\n",
        "#                     published = dateparser.parse(node.get_text(strip=True), fuzzy=True); break\n",
        "#                 except Exception:\n",
        "#                     pass\n",
        "\n",
        "#     # Extract paragraph text from the article\n",
        "#     main_container = None\n",
        "#     for cls in ['article','post','story','content','entry-content','article-body']:\n",
        "#         mc = soup.find(True, class_=re.compile(cls, re.I))\n",
        "#         if mc: main_container = mc; break\n",
        "\n",
        "#     paragraphs = (main_container.find_all('p') if main_container else soup.find_all('p'))\n",
        "#     for p in paragraphs:\n",
        "#         t = p.get_text(' ', strip=True)\n",
        "#         if t and len(t) > 40: text_chunks.append(t)\n",
        "\n",
        "#     article_text = '\\n\\n'.join(text_chunks)[:100000]\n",
        "\n",
        "#     # Extract links and count external references\n",
        "#     all_links, external_links = [], []\n",
        "#     base_host = urlparse(url).netloc.lower()\n",
        "#     for a in soup.find_all('a', href=True):\n",
        "#         href = a['href']\n",
        "#         if href.startswith('http://') or href.startswith('https://'):\n",
        "#             all_links.append(href)\n",
        "#             if urlparse(href).netloc.lower() != base_host:\n",
        "#                 external_links.append(href)\n",
        "\n",
        "#     # Check for transparency language\n",
        "#     full_text_for_hints = (article_text + ' ' + ' '.join(TRANSPARENCY_HINTS)).lower()\n",
        "\n",
        "#     return {\n",
        "#         'title': title,\n",
        "#         'author': author,\n",
        "#         'published': published.isoformat() if published else None,\n",
        "#         'text': article_text,\n",
        "#         'num_paragraphs': len(text_chunks),\n",
        "#         'all_links': all_links,\n",
        "#         'external_links': external_links,\n",
        "#         'has_transparency_hints': any(h in full_text_for_hints for h in TRANSPARENCY_HINTS),\n",
        "#     }\n",
        "\n",
        "# # Function to calculate a credibility score based on article metadata and content\n",
        "# def score_url(url: str, fields: dict):\n",
        "#     explanation_bits = []\n",
        "#     score = 50  # Start from neutral score\n",
        "\n",
        "#     # + HTTPS is a sign of trust\n",
        "#     if url.lower().startswith('https://'):\n",
        "#         score += 12; explanation_bits.append('+12: uses HTTPS')\n",
        "#     else:\n",
        "#         score -= 10; explanation_bits.append('-10: not using HTTPS')\n",
        "\n",
        "#     # + TLD check (institutional domains boost score)\n",
        "#     ext = tldextract.extract(url)\n",
        "#     tld_last = (ext.suffix.split('.')[-1] if ext.suffix else '')\n",
        "#     if tld_last in INSTITUTIONAL_TLDS:\n",
        "#         score += 14; explanation_bits.append(f'+14: institutional TLD ({tld_last})')\n",
        "\n",
        "#     # + Author attribution\n",
        "#     if fields.get('author'):\n",
        "#         score += 10; explanation_bits.append('+10: author/byline found')\n",
        "#     else:\n",
        "#         score -= 6; explanation_bits.append('-6: no clear author/byline')\n",
        "\n",
        "#     # + Publication recency\n",
        "#     published = fields.get('published')\n",
        "#     if published:\n",
        "#         try:\n",
        "#             dt = dateparser.parse(published)\n",
        "#             if (datetime.utcnow() - dt).days <= 3650:\n",
        "#                 score += 6; explanation_bits.append('+6: reasonably recent publication date')\n",
        "#             else:\n",
        "#                 score -= 4; explanation_bits.append('-4: appears quite old')\n",
        "#         except Exception:\n",
        "#             explanation_bits.append('0: could not parse publication date reliably')\n",
        "#     else:\n",
        "#         explanation_bits.append('0: no publication date found')\n",
        "\n",
        "#     # + References/links\n",
        "#     total_links = len(fields.get('all_links', []))\n",
        "#     external_links_count = len(fields.get('external_links', []))\n",
        "#     if total_links >= 5 and external_links_count >= 3:\n",
        "#         score += 10; explanation_bits.append(f'+10: provides references (links: {total_links}, external: {external_links_count})')\n",
        "#     elif total_links >= 2:\n",
        "#         score += 4; explanation_bits.append(f'+4: some references (links: {total_links})')\n",
        "#     else:\n",
        "#         score -= 6; explanation_bits.append(f'-6: minimal/no references (links: {total_links})')\n",
        "\n",
        "#     # + Article length\n",
        "#     num_paras = fields.get('num_paragraphs', 0)\n",
        "#     if num_paras >= 8:\n",
        "#         score += 6; explanation_bits.append('+6: substantive article length')\n",
        "#     elif num_paras >= 3:\n",
        "#         score += 2; explanation_bits.append('+2: moderate article length')\n",
        "#     else:\n",
        "#         score -= 6; explanation_bits.append('-6: very short article text')\n",
        "\n",
        "#     # - Clickbait detection\n",
        "#     text_lower = (fields.get('text') or '').lower()\n",
        "#     clickbait_hits = sum(1 for term in CLICKBAIT_TERMS if term in text_lower)\n",
        "#     if clickbait_hits >= 2:\n",
        "#         score -= 10; explanation_bits.append('-10: strong clickbait indicators')\n",
        "#     elif clickbait_hits == 1:\n",
        "#         score -= 4; explanation_bits.append('-4: mild clickbait indicators')\n",
        "\n",
        "#     # - Advertising/sponsored content signal\n",
        "#     ad_signals = len(re.findall(r\"advertis(e|ement)|sponsor(ed|ship)\", text_lower))\n",
        "#     iframes_penalty = min(8, math.floor(ad_signals / 5) * 2)\n",
        "#     if iframes_penalty:\n",
        "#         score -= iframes_penalty; explanation_bits.append(f'-{iframes_penalty}: advertising/sponsorship language')\n",
        "\n",
        "#     score = max(0, min(100, int(round(score))))\n",
        "#     explanation = '; '.join(explanation_bits)\n",
        "#     return score, explanation\n",
        "\n",
        "# # Main function to evaluate the URL and return all metadata, score, and explanation\n",
        "# def evaluate_url(url: str):\n",
        "#     if not isinstance(url, str) or not url.strip():\n",
        "#         return {'score': 0, 'explanation': 'Invalid URL input.', 'details': {'error': 'empty_or_non_string'}}\n",
        "\n",
        "#     html, err = fetch_html(url)\n",
        "#     if err:\n",
        "#         return {'score': 0, 'explanation': f'Failed to fetch: {err}', 'details': {'error': 'fetch_failed'}}\n",
        "\n",
        "#     fields = extract_article_fields(html, url)\n",
        "#     score, explanation = score_url(url, fields)\n",
        "\n",
        "#     return {\n",
        "#         'score': score,\n",
        "#         'explanation': explanation,\n",
        "#         'details': {\n",
        "#             'url': url,\n",
        "#             'title': fields.get('title'),\n",
        "#             'author': fields.get('author'),\n",
        "#             'published': fields.get('published'),\n",
        "#             'num_paragraphs': fields.get('num_paragraphs'),\n",
        "#             'total_links': len(fields.get('all_links', [])),\n",
        "#             'external_links': len(fields.get('external_links', [])),\n",
        "#         },\n",
        "#     }\n",
        "\n",
        "# # Sample test\n",
        "# if __name__ == \"__main__\":\n",
        "#     res = evaluate_url('https://www.mayoclinic.org/diseases-conditions/dehydration/symptoms-causes/syc-20354086')\n",
        "#     print(json.dumps(res, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# VERSION_1\n",
        "# import requests, os\n",
        "\n",
        "# SERP_API_KEY = os.getenv(\"SERP_API_KEY\")\n",
        "\n",
        "# def search_google(query, num_results=3):\n",
        "#     url = \"https://serpapi.com/search\"\n",
        "#     params = {\n",
        "#         \"q\": query,\n",
        "#         \"api_key\": SERP_API_KEY,\n",
        "#         \"num\": num_results,\n",
        "#         \"engine\": \"google\",\n",
        "#         \"hl\": \"en\",\n",
        "#     }\n",
        "#     response = requests.get(url, params=params)\n",
        "#     data = response.json()\n",
        "#     links = [r.get(\"link\") for r in data.get(\"organic_results\", []) if r.get(\"link\")]\n",
        "#     return links"
      ],
      "metadata": {
        "id": "mre8vh4EfdOC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Improved SerpAPI search (clean results, de-dupe, filter, robust errors) ---\n",
        "import os, re, requests\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "SERP_API_KEY = os.getenv(\"SERP_API_KEY\")  # set this earlier in your notebook\n",
        "\n",
        "# Common low-signal/social/video domains to exclude by default (tune as needed)\n",
        "_DEFAULT_EXCLUDE_DOMAINS = {\n",
        "    \"reddit.com\", \"www.reddit.com\", \"old.reddit.com\",\n",
        "    \"x.com\", \"twitter.com\", \"www.twitter.com\",\n",
        "    \"tiktok.com\", \"www.tiktok.com\",\n",
        "    \"pinterest.com\", \"www.pinterest.com\",\n",
        "    \"facebook.com\", \"www.facebook.com\",\n",
        "    \"instagram.com\", \"www.instagram.com\",\n",
        "    \"youtube.com\", \"www.youtube.com\", \"youtu.be\"\n",
        "}\n",
        "\n",
        "# Skip obvious non-article filetypes\n",
        "_SKIP_FILETYPES = re.compile(r\"\\.(pdf|pptx?|docx?|xlsx?|zip|rar)(?:$|\\?)\", re.I)\n",
        "\n",
        "def _host(url: str) -> str:\n",
        "    return urlparse(url).netloc.lower()\n",
        "\n",
        "def search_google(\n",
        "    query: str,\n",
        "    num_results: int = 5,\n",
        "    exclude_domains: set | None = None,\n",
        "    allow_news_results: bool = True,\n",
        "    timeout: int = 20,\n",
        ") -> list[dict]:\n",
        "    \"\"\"\n",
        "    Search Google via SerpAPI and return a clean list of results:\n",
        "        [{\"title\": str, \"link\": str, \"snippet\": str}, ...]\n",
        "    - De-duplicates by link and avoids flooding from the same host\n",
        "    - Skips social/UGC/video sites and non-HTML filetypes\n",
        "    - Optionally includes Google News results\n",
        "    \"\"\"\n",
        "    if not SERP_API_KEY:\n",
        "        raise RuntimeError(\"Missing SERP_API_KEY. Set os.environ['SERP_API_KEY'] first.\")\n",
        "\n",
        "    exclude = set(_DEFAULT_EXCLUDE_DOMAINS)\n",
        "    if exclude_domains:\n",
        "        exclude |= set(exclude_domains)\n",
        "\n",
        "    params = {\n",
        "        \"engine\": \"google\",\n",
        "        \"q\": query,\n",
        "        \"api_key\": SERP_API_KEY,\n",
        "        \"num\": 10,          # pull extra, then filter/trim to num_results\n",
        "        \"hl\": \"en\",\n",
        "        \"gl\": \"us\",\n",
        "        \"safe\": \"active\",   # optional: reduce NSFW\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        resp = requests.get(\"https://serpapi.com/search\", params=params, timeout=timeout)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "    except Exception as e:\n",
        "        # On failure, return empty list (caller can handle and message user)\n",
        "        print(f\"[SerpAPI] Error: {e}\")\n",
        "        return []\n",
        "\n",
        "    candidates: list[dict] = []\n",
        "\n",
        "    # 1) Organic results\n",
        "    for r in (data.get(\"organic_results\") or []):\n",
        "        link = r.get(\"link\")\n",
        "        title = r.get(\"title\")\n",
        "        snippet = r.get(\"snippet\") or \"\"\n",
        "        if not link or _SKIP_FILETYPES.search(link):\n",
        "            continue\n",
        "        host = _host(link)\n",
        "        if host in exclude:\n",
        "            continue\n",
        "        candidates.append({\"title\": title, \"link\": link, \"snippet\": snippet})\n",
        "\n",
        "    # 2) Optional: News results (helpful for timely topics)\n",
        "    if allow_news_results:\n",
        "        for r in (data.get(\"news_results\") or []):\n",
        "            link = r.get(\"link\")\n",
        "            title = r.get(\"title\")\n",
        "            snippet = r.get(\"snippet\") or \"\"\n",
        "            if not link or _SKIP_FILETYPES.search(link):\n",
        "                continue\n",
        "            host = _host(link)\n",
        "            if host in exclude:\n",
        "                continue\n",
        "            candidates.append({\"title\": title, \"link\": link, \"snippet\": snippet})\n",
        "\n",
        "    # De-duplicate by link; also avoid over-representing a single host\n",
        "    cleaned, seen_links, seen_hosts = [], set(), set()\n",
        "    for c in candidates:\n",
        "        link, host = c[\"link\"], _host(c[\"link\"])\n",
        "        if link in seen_links:\n",
        "            continue\n",
        "        # If we already have enough and this host is duplicate, skip\n",
        "        if host in seen_hosts and len(cleaned) >= num_results:\n",
        "            continue\n",
        "        seen_links.add(link)\n",
        "        seen_hosts.add(host)\n",
        "        cleaned.append(c)\n",
        "        if len(cleaned) >= num_results:\n",
        "            break\n",
        "\n",
        "    return cleaned"
      ],
      "metadata": {
        "id": "dMWhH62SWyQO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, google.generativeai as genai\n",
        "\n",
        "# 1) Configure the SDK with your key (make sure you set os.environ[\"GEMINI_API_KEY\"] earlier)\n",
        "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
        "\n",
        "# 2) Use a current model name (the old \"gemini-pro\" on v1beta throws 404)\n",
        "#    Options: \"gemini-1.5-flash\" (faster/cheaper) or \"gemini-1.5-pro\" (stronger).\n",
        "MODEL_NAME = \"gemini-1.5-flash\"\n",
        "model = genai.GenerativeModel(MODEL_NAME)\n",
        "\n",
        "def summarize_text(text: str, query: str) -> str:\n",
        "    \"\"\"\n",
        "    Summarize `text` for the user's `query` using Gemini.\n",
        "    Falls back to a simple extractive summary if the API call fails.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"[No article text extracted to summarize.]\"\n",
        "\n",
        "    # keep prompt compact; long inputs can hit limits\n",
        "    clipped = text[:4000]\n",
        "\n",
        "    prompt = (\n",
        "        \"You are a helpful assistant. The user asked:\\n\"\n",
        "        f\"{query}\\n\\n\"\n",
        "        \"Based on the following article content, write a concise 3‚Äì5 sentence summary \"\n",
        "        \"that directly helps answer the user's question. Avoid fluff; be factual.\\n\\n\"\n",
        "        f\"Article:\\n{clipped}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        resp = model.generate_content(prompt)\n",
        "        # Some SDK versions use resp.text; keep a safe fallback\n",
        "        out = getattr(resp, \"text\", None)\n",
        "        return out.strip() if out else \"[Empty response from model.]\"\n",
        "    except Exception as e:\n",
        "        # Fallback: simple extractive summary (first few sentences)\n",
        "        import re\n",
        "        sents = re.split(r'(?<=[.!?])\\s+', clipped)\n",
        "        return \" \".join(sents[:3]) + f\"\\n\\n[Gemini fallback due to error: {e}]\""
      ],
      "metadata": {
        "id": "IsrPre2Qfhte"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# VERSION_1\n",
        "# chat_history = []\n",
        "\n",
        "# def display_response(title, url, summary, score):\n",
        "#     stars = \"‚òÖ\" * round(score / 20)\n",
        "#     print(f\"\\nüì∞ Title: {title or '[No Title]'}\")\n",
        "#     print(f\"üîó Link: {url}\")\n",
        "#     print(f\"üìÑ Summary: {summary}\")\n",
        "#     print(f\"‚≠ê Credibility: {stars} ({score}/100)\\n\")\n",
        "\n",
        "# def run_chatbot():\n",
        "#     print(\"ü§ñ Hello! I'm CredBot. What would you like to learn about today? FYI: To end session please enter 'exit' at any time!\")\n",
        "#     while True:\n",
        "#         user_input = input(\"\\nYou: \").strip()\n",
        "#         if user_input.lower() in {\"exit\", \"quit\", \"bye\"}:\n",
        "#             print(\"üëã Goodbye!\")\n",
        "#             break\n",
        "\n",
        "#         chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "#         print(\"üîç Searching Google...\")\n",
        "#         links = search_google(user_input)\n",
        "\n",
        "#         if not links:\n",
        "#             print(\"‚ö†Ô∏è No relevant articles found.\")\n",
        "#             continue\n",
        "\n",
        "#         for link in links:\n",
        "#             print(f\"\\nEvaluating: {link}\")\n",
        "#             result = evaluate_url(link)\n",
        "#             score = result[\"score\"]\n",
        "#             details = result[\"details\"]\n",
        "#             title = details.get(\"title\", \"Untitled\")\n",
        "#             text = details.get(\"text\") or \"\"\n",
        "#             summary = summarize_text(text, user_input)\n",
        "#             display_response(title, link, summary, score)\n",
        "\n",
        "#         chat_history.append({\"role\": \"bot\", \"content\": f\"Returned {len(links)} articles.\"})\n",
        "\n",
        "# run_chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTrRMsIQfjoL",
        "outputId": "95d3e7f8-1d99-49e2-aae7-de8498854e07",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Hello! I'm CredBot. What would you like to learn about today? FYI: To end session please enter 'exit' at any time!\n",
            "\n",
            "You: I want to learn about ice cream\n",
            "üîç Searching Google...\n",
            "\n",
            "Evaluating: {'title': 'Ice cream', 'link': 'https://en.wikipedia.org/wiki/Ice_cream', 'snippet': 'Ice cream is a frozen dessert typically made from milk or cream that has been flavoured with a sweetener, either sugar or an alternative, and a spice'}\n",
            "\n",
            "üì∞ Title: Untitled\n",
            "üîó Link: {'title': 'Ice cream', 'link': 'https://en.wikipedia.org/wiki/Ice_cream', 'snippet': 'Ice cream is a frozen dessert typically made from milk or cream that has been flavoured with a sweetener, either sugar or an alternative, and a spice'}\n",
            "üìÑ Summary: [No article text extracted to summarize.]\n",
            "‚≠ê Credibility:  (0/100)\n",
            "\n",
            "\n",
            "Evaluating: {'title': 'Ice Cream Science', 'link': 'https://www.dreamscoops.com/ice-cream-science/', 'snippet': \"The fats also give ice cream it's creamy texture and richness. Higher fat ice creams are rich and creamy with a long lingering after-taste.\"}\n",
            "\n",
            "üì∞ Title: Untitled\n",
            "üîó Link: {'title': 'Ice Cream Science', 'link': 'https://www.dreamscoops.com/ice-cream-science/', 'snippet': \"The fats also give ice cream it's creamy texture and richness. Higher fat ice creams are rich and creamy with a long lingering after-taste.\"}\n",
            "üìÑ Summary: [No article text extracted to summarize.]\n",
            "‚≠ê Credibility:  (0/100)\n",
            "\n",
            "\n",
            "Evaluating: {'title': \"15 Ice Cream Fun Facts That You Didn't Know\", 'link': 'https://www.mymochi.com/blog/fun-facts-about-ice-cream-that-you-didnt-know/', 'snippet': 'It takes 12 pounds of milk to produce just 1 gallon of ice cream ¬∑ The average number of licks to finish a scoop of ice cream is 50 ¬∑ The country that consumes ...'}\n",
            "\n",
            "üì∞ Title: Untitled\n",
            "üîó Link: {'title': \"15 Ice Cream Fun Facts That You Didn't Know\", 'link': 'https://www.mymochi.com/blog/fun-facts-about-ice-cream-that-you-didnt-know/', 'snippet': 'It takes 12 pounds of milk to produce just 1 gallon of ice cream ¬∑ The average number of licks to finish a scoop of ice cream is 50 ¬∑ The country that consumes ...'}\n",
            "üìÑ Summary: [No article text extracted to summarize.]\n",
            "‚≠ê Credibility:  (0/100)\n",
            "\n",
            "\n",
            "Evaluating: {'title': 'The History of Ice Cream - IDFA', 'link': 'https://www.idfa.org/the-history-of-ice-cream', 'snippet': \"Ice cream's origins are known to reach back as far as the second century BC, although no specific date of origin nor inventor has been undisputably credited ...\"}\n",
            "\n",
            "üì∞ Title: Untitled\n",
            "üîó Link: {'title': 'The History of Ice Cream - IDFA', 'link': 'https://www.idfa.org/the-history-of-ice-cream', 'snippet': \"Ice cream's origins are known to reach back as far as the second century BC, although no specific date of origin nor inventor has been undisputably credited ...\"}\n",
            "üìÑ Summary: [No article text extracted to summarize.]\n",
            "‚≠ê Credibility:  (0/100)\n",
            "\n",
            "\n",
            "Evaluating: {'title': 'The Scoop: Getting to Know Ice Cream', 'link': 'https://www.ice.edu/blog/scoop-getting-know-ice-cream', 'snippet': 'Chef Jenny McCoy dives into the history and science of ice cream, from soft serve to Philly-style to gelato.'}\n",
            "\n",
            "üì∞ Title: Untitled\n",
            "üîó Link: {'title': 'The Scoop: Getting to Know Ice Cream', 'link': 'https://www.ice.edu/blog/scoop-getting-know-ice-cream', 'snippet': 'Chef Jenny McCoy dives into the history and science of ice cream, from soft serve to Philly-style to gelato.'}\n",
            "üìÑ Summary: [No article text extracted to summarize.]\n",
            "‚≠ê Credibility:  (0/100)\n",
            "\n",
            "\n",
            "You: exit\n",
            "üëã Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# VERSION_1 OUTPUT\n",
        "# ü§ñ Hello! I'm CredBot. What would you like to learn about today? FYI: To end session please enter 'exit' at any time!\n",
        "\n",
        "# You: I want to learn about ice cream\n",
        "# üîç Searching Google...\n",
        "\n",
        "# Evaluating: {'title': 'Ice cream', 'link': 'https://en.wikipedia.org/wiki/Ice_cream', 'snippet': 'Ice cream is a frozen dessert typically made from milk or cream that has been flavoured with a sweetener, either sugar or an alternative, and a spice'}\n",
        "\n",
        "# üì∞ Title: Untitled\n",
        "# üîó Link: {'title': 'Ice cream', 'link': 'https://en.wikipedia.org/wiki/Ice_cream', 'snippet': 'Ice cream is a frozen dessert typically made from milk or cream that has been flavoured with a sweetener, either sugar or an alternative, and a spice'}\n",
        "# üìÑ Summary: [No article text extracted to summarize.]\n",
        "# ‚≠ê Credibility:  (0/100)\n",
        "\n",
        "\n",
        "# Evaluating: {'title': 'Ice Cream Science', 'link': 'https://www.dreamscoops.com/ice-cream-science/', 'snippet': \"The fats also give ice cream it's creamy texture and richness. Higher fat ice creams are rich and creamy with a long lingering after-taste.\"}\n",
        "\n",
        "# üì∞ Title: Untitled\n",
        "# üîó Link: {'title': 'Ice Cream Science', 'link': 'https://www.dreamscoops.com/ice-cream-science/', 'snippet': \"The fats also give ice cream it's creamy texture and richness. Higher fat ice creams are rich and creamy with a long lingering after-taste.\"}\n",
        "# üìÑ Summary: [No article text extracted to summarize.]\n",
        "# ‚≠ê Credibility:  (0/100)\n",
        "\n",
        "\n",
        "# Evaluating: {'title': \"15 Ice Cream Fun Facts That You Didn't Know\", 'link': 'https://www.mymochi.com/blog/fun-facts-about-ice-cream-that-you-didnt-know/', 'snippet': 'It takes 12 pounds of milk to produce just 1 gallon of ice cream ¬∑ The average number of licks to finish a scoop of ice cream is 50 ¬∑ The country that consumes ...'}\n",
        "\n",
        "# üì∞ Title: Untitled\n",
        "# üîó Link: {'title': \"15 Ice Cream Fun Facts That You Didn't Know\", 'link': 'https://www.mymochi.com/blog/fun-facts-about-ice-cream-that-you-didnt-know/', 'snippet': 'It takes 12 pounds of milk to produce just 1 gallon of ice cream ¬∑ The average number of licks to finish a scoop of ice cream is 50 ¬∑ The country that consumes ...'}\n",
        "# üìÑ Summary: [No article text extracted to summarize.]\n",
        "# ‚≠ê Credibility:  (0/100)\n",
        "\n",
        "\n",
        "# Evaluating: {'title': 'The History of Ice Cream - IDFA', 'link': 'https://www.idfa.org/the-history-of-ice-cream', 'snippet': \"Ice cream's origins are known to reach back as far as the second century BC, although no specific date of origin nor inventor has been undisputably credited ...\"}\n",
        "\n",
        "# üì∞ Title: Untitled\n",
        "# üîó Link: {'title': 'The History of Ice Cream - IDFA', 'link': 'https://www.idfa.org/the-history-of-ice-cream', 'snippet': \"Ice cream's origins are known to reach back as far as the second century BC, although no specific date of origin nor inventor has been undisputably credited ...\"}\n",
        "# üìÑ Summary: [No article text extracted to summarize.]\n",
        "# ‚≠ê Credibility:  (0/100)\n",
        "\n",
        "\n",
        "# Evaluating: {'title': 'The Scoop: Getting to Know Ice Cream', 'link': 'https://www.ice.edu/blog/scoop-getting-know-ice-cream', 'snippet': 'Chef Jenny McCoy dives into the history and science of ice cream, from soft serve to Philly-style to gelato.'}\n",
        "\n",
        "# üì∞ Title: Untitled\n",
        "# üîó Link: {'title': 'The Scoop: Getting to Know Ice Cream', 'link': 'https://www.ice.edu/blog/scoop-getting-know-ice-cream', 'snippet': 'Chef Jenny McCoy dives into the history and science of ice cream, from soft serve to Philly-style to gelato.'}\n",
        "# üìÑ Summary: [No article text extracted to summarize.]\n",
        "# ‚≠ê Credibility:  (0/100)\n",
        "\n",
        "\n",
        "# You: exit\n",
        "# üëã Goodbye!\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UF0fXx0nYC6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Simple console chatbot (CredBot) using HYBRID scoring\n",
        "# ============================\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "def display_response(title, url, summary, hybrid_dict):\n",
        "    \"\"\"\n",
        "    Pretty-prints one search result using the hybrid scorer output.\n",
        "    `hybrid_dict` is the object returned by hybrid_score(url).\n",
        "    \"\"\"\n",
        "    score = hybrid_dict.get(\"hybrid_score\", 0.0)\n",
        "    stars = hybrid_dict.get(\"stars\") or (\"‚òÖ\" * int(round(score / 20)))\n",
        "    why = hybrid_dict.get(\"explanation\", \"\").strip()\n",
        "\n",
        "    print(f\"\\nüì∞ Title: {title or '[No Title]'}\")\n",
        "    print(f\"üîó Link: {url}\")\n",
        "    print(f\"üìÑ Summary: {summary}\")\n",
        "    print(f\"‚≠ê Credibility: {stars} ({score}/100)\")\n",
        "    if why:\n",
        "        print(f\"üìù Why: {why}\\n\")\n",
        "\n",
        "def run_chatbot():\n",
        "    print(\"ü§ñ Hello! I'm CredBot. What would you like to learn about today? FYI: To end session please enter 'exit' at any time!\")\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \").strip()\n",
        "        if user_input.lower() in {\"exit\", \"quit\", \"bye\"}:\n",
        "            print(\"üëã Goodbye!\")\n",
        "            break\n",
        "\n",
        "        chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "        print(\"üîç Searching Google...\")\n",
        "\n",
        "        # NOTE: our improved search_google returns a list of dicts:\n",
        "        # [{\"title\": str, \"link\": str, \"snippet\": str}, ...]\n",
        "        results = search_google(user_input, num_results=5)\n",
        "\n",
        "        if not results:\n",
        "            print(\"‚ö†Ô∏è No relevant articles found.\")\n",
        "            continue\n",
        "\n",
        "        for item in results:\n",
        "            title = item.get(\"title\") or \"[No Title]\"\n",
        "            link = item.get(\"link\")\n",
        "            snippet = item.get(\"snippet\") or \"\"\n",
        "\n",
        "            if not link:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nEvaluating: {link}\")\n",
        "\n",
        "            # 1) Get hybrid credibility score (uses your rule+ML logic)\n",
        "            hybrid = hybrid_score(link, alpha=0.6)\n",
        "\n",
        "            # 2) Build summary text\n",
        "            #    Prefer full article text; fall back to the search snippet if needed.\n",
        "            #    evaluate_url() does not return article text in 'details', so we fetch & parse here.\n",
        "            article_text = \"\"\n",
        "            html, err = fetch_html(link)\n",
        "            if not err and html:\n",
        "                try:\n",
        "                    fields = extract_article_fields(html, link)\n",
        "                    article_text = fields.get(\"text\") or \"\"\n",
        "                except Exception:\n",
        "                    article_text = \"\"\n",
        "\n",
        "            # If we still don't have content, summarize the snippet (so user sees something useful).\n",
        "            summary_input = article_text if article_text else snippet\n",
        "            summary = summarize_text(summary_input, user_input)\n",
        "\n",
        "            # 3) Display nicely with hybrid score + ‚Äúwhy‚Äù\n",
        "            display_response(title, link, summary, hybrid)\n",
        "\n",
        "        chat_history.append({\"role\": \"bot\", \"content\": f\"Returned {len(results)} articles.\"})\n",
        "\n",
        "# Run it\n",
        "run_chatbot()"
      ],
      "metadata": {
        "id": "Kxb0TjgGfm_K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b50d1dd-5db2-494c-9980-e96d4c5cd7c0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Hello! I'm CredBot. What would you like to learn about today? FYI: To end session please enter 'exit' at any time!\n",
            "\n",
            "You: i want to learn about ice cream\n",
            "üîç Searching Google...\n",
            "\n",
            "Evaluating: https://en.wikipedia.org/wiki/Ice_cream\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2069810422.py:256: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  if (datetime.utcnow() - dt).days <= 3650:\n",
            "/tmp/ipython-input-2069810422.py:376: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return max(0, (datetime.utcnow() - dt).days)  # OK to mirror your original utc usage\n",
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 685.99ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì∞ Title: Ice cream\n",
            "üîó Link: https://en.wikipedia.org/wiki/Ice_cream\n",
            "üìÑ Summary: Ice cream is a frozen dessert typically made from milk or cream that has been flavoured with a sweetener, either sugar or an alternative, and a spice\n",
            "\n",
            "[Gemini fallback due to error: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.]\n",
            "‚≠ê Credibility: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (76.6/100)\n",
            "üìù Why: +12: uses HTTPS; +10: author/byline found; -4: appears quite old; +10: provides references (links: 308, external: 307); -6: very short article text\n",
            "\n",
            "\n",
            "Evaluating: https://www.dreamscoops.com/ice-cream-science/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 203.99ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì∞ Title: Ice Cream Science\n",
            "üîó Link: https://www.dreamscoops.com/ice-cream-science/\n",
            "üìÑ Summary: Ice cream contains solid ice and fat, liquid sugar solution, and gas air bubbles. Ice crystals give firmness, air gives softness, and fat ...\n",
            "\n",
            "[Gemini fallback due to error: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.]\n",
            "‚≠ê Credibility: ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (50.6/100)\n",
            "üìù Why: +12: uses HTTPS; -6: no clear author/byline; 0: no publication date found; -6: minimal/no references (links: 0); -6: very short article text\n",
            "\n",
            "\n",
            "Evaluating: https://www.mymochi.com/blog/fun-facts-about-ice-cream-that-you-didnt-know/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2069810422.py:256: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  if (datetime.utcnow() - dt).days <= 3650:\n",
            "/tmp/ipython-input-2069810422.py:376: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return max(0, (datetime.utcnow() - dt).days)  # OK to mirror your original utc usage\n",
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 153.25ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì∞ Title: 15 Ice Cream Fun Facts That You Didn't Know\n",
            "üîó Link: https://www.mymochi.com/blog/fun-facts-about-ice-cream-that-you-didnt-know/\n",
            "üìÑ Summary: Do you love ice cream as much as we do? If you can‚Äôt go a day, a week, or even a month without the delicious flavors of ice cream, then we know how you feel. While you may be a pro at eating ice cream, did you know that there are some really interesting facts about ice cream?\n",
            "\n",
            "[Gemini fallback due to error: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.]\n",
            "‚≠ê Credibility: ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (69.4/100)\n",
            "üìù Why: +12: uses HTTPS; +10: author/byline found; 0: could not parse publication date reliably; +4: some references (links: 31); +2: moderate article length\n",
            "\n",
            "\n",
            "Evaluating: https://www.ice.edu/blog/scoop-getting-know-ice-cream\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 229.98ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì∞ Title: The Scoop: Getting to Know Ice Cream\n",
            "üîó Link: https://www.ice.edu/blog/scoop-getting-know-ice-cream\n",
            "üìÑ Summary: Chef Jenny McCoy dives into the history and science of ice cream, from soft serve to Philly-style to gelato.\n",
            "\n",
            "[Gemini fallback due to error: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.]\n",
            "‚≠ê Credibility: ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ (20.0/100)\n",
            "üìù Why: Credibility is low due to limited accessible information ‚Äî the page could not be fetched or analyzed (e.g., the site blocked automated requests).  | Failed to fetch: Fetch error: 403 Forbidden\n",
            "\n",
            "\n",
            "Evaluating: https://www.idfa.org/the-history-of-ice-cream\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 207.62ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì∞ Title: The History of Ice Cream - IDFA\n",
            "üîó Link: https://www.idfa.org/the-history-of-ice-cream\n",
            "üìÑ Summary: Ice cream's origins are known to reach back as far as the second century B.C., although no specific date of origin nor inventor has been undisputably credited with its discovery. We know that Alexander the Great enjoyed snow and ice flavored with honey and nectar. Biblical references also show that King Solomon was fond of iced drinks during harvesting.\n",
            "\n",
            "[Gemini fallback due to error: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.]\n",
            "‚≠ê Credibility: ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (65.8/100)\n",
            "üìù Why: +12: uses HTTPS; -6: no clear author/byline; 0: no publication date found; +10: provides references (links: 39, external: 10); +2: moderate article length\n",
            "\n",
            "\n",
            "You: exit\n",
            "üëã Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# DELIVERABLE_2: Simple manual test on 5 ice cream URLs\n",
        "# ============================================================\n",
        "\n",
        "test_links = [\n",
        "    \"https://en.wikipedia.org/wiki/Ice_cream\",\n",
        "    \"https://www.dreamscoops.com/ice-cream-science/\",\n",
        "    \"https://www.mymochi.com/blog/fun-facts-about-ice-cream-that-you-didnt-know/\",\n",
        "    \"https://www.idfa.org/the-history-of-ice-cream\",\n",
        "    \"https://www.ice.edu/blog/scoop-getting-know-ice-cream\",\n",
        "]\n",
        "\n",
        "print(\"üß™ Running hybrid credibility scoring test on 5 URLs...\\n\")\n",
        "\n",
        "for url in test_links:\n",
        "    try:\n",
        "        result = hybrid_score(url, alpha=0.6)  # uses rule + ML\n",
        "        print(\"‚Äî\" * 70)\n",
        "        print(f\"üîó {url}\")\n",
        "        print(f\"‚≠ê {result['stars']}  ({result['hybrid_score']}/100)\")\n",
        "        print(f\"   ‚Ä¢ Rule score: {result['rule_score']}\")\n",
        "        print(f\"   ‚Ä¢ ML score:   {result['ml_score']}\")\n",
        "        print(f\"üìù Why: {result['explanation']}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error scoring {url}: {e}\")\n",
        "\n",
        "print(\"‚úÖ Test complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ5o8JtYYLAa",
        "outputId": "a5163650-adcb-49bd-e022-95c1c78556cf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Running hybrid credibility scoring test on 5 URLs...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2069810422.py:256: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  if (datetime.utcnow() - dt).days <= 3650:\n",
            "/tmp/ipython-input-2069810422.py:376: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return max(0, (datetime.utcnow() - dt).days)  # OK to mirror your original utc usage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
            "üîó https://en.wikipedia.org/wiki/Ice_cream\n",
            "‚≠ê ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ  (76.6/100)\n",
            "   ‚Ä¢ Rule score: 72.0\n",
            "   ‚Ä¢ ML score:   83.5\n",
            "üìù Why: +12: uses HTTPS; +10: author/byline found; -4: appears quite old; +10: provides references (links: 308, external: 307); -6: very short article text\n",
            "\n",
            "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
            "üîó https://www.dreamscoops.com/ice-cream-science/\n",
            "‚≠ê ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ  (50.6/100)\n",
            "   ‚Ä¢ Rule score: 44.0\n",
            "   ‚Ä¢ ML score:   60.5\n",
            "üìù Why: +12: uses HTTPS; -6: no clear author/byline; 0: no publication date found; -6: minimal/no references (links: 0); -6: very short article text\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2069810422.py:256: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  if (datetime.utcnow() - dt).days <= 3650:\n",
            "/tmp/ipython-input-2069810422.py:376: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return max(0, (datetime.utcnow() - dt).days)  # OK to mirror your original utc usage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
            "üîó https://www.mymochi.com/blog/fun-facts-about-ice-cream-that-you-didnt-know/\n",
            "‚≠ê ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ  (69.4/100)\n",
            "   ‚Ä¢ Rule score: 78.0\n",
            "   ‚Ä¢ ML score:   56.6\n",
            "üìù Why: +12: uses HTTPS; +10: author/byline found; 0: could not parse publication date reliably; +4: some references (links: 31); +2: moderate article length\n",
            "\n",
            "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
            "üîó https://www.idfa.org/the-history-of-ice-cream\n",
            "‚≠ê ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ  (65.8/100)\n",
            "   ‚Ä¢ Rule score: 68.0\n",
            "   ‚Ä¢ ML score:   62.5\n",
            "üìù Why: +12: uses HTTPS; -6: no clear author/byline; 0: no publication date found; +10: provides references (links: 39, external: 10); +2: moderate article length\n",
            "\n",
            "‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
            "üîó https://www.ice.edu/blog/scoop-getting-know-ice-cream\n",
            "‚≠ê ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ  (20.0/100)\n",
            "   ‚Ä¢ Rule score: 0.0\n",
            "   ‚Ä¢ ML score:   0.0\n",
            "üìù Why: Credibility is low due to limited accessible information ‚Äî the page could not be fetched or analyzed (e.g., the site blocked automated requests).  | Failed to fetch: Fetch error: 403 Forbidden\n",
            "\n",
            "‚úÖ Test complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url = \"https://www.ice.edu/blog/scoop-getting-know-ice-cream\"\n",
        "r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=12)\n",
        "print(r.status_code, r.headers.get(\"Content-Type\"))\n",
        "print(r.text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ydpfF3XZm-z",
        "outputId": "fffea328-84e1-4ae6-e273-e13449f2abe4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "403 text/html; charset=UTF-8\n",
            "<!DOCTYPE html><html lang=\"en-US\"><head><title>Just a moment...</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"><meta name=\"robots\" content=\"noindex,nofollow\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"><style>*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,\"Helve\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deliverable 2 Summary:\n",
        "This phase integrates Google Search (SERP API), the rule-based credibility engine, and a lightweight Linear Regression model into one hybrid scoring pipeline. It features smarter HTML fetching with header rotation, back-off retries, and explicit edge-case handling for blocked sites. Twenty real URLs are labeled and used to train and 5-fold-validate the ML model, producing blended scores (rules + ML) with clear explanations and star ratings. A manual test cell confirms that each link runs end-to-end‚Äîfrom fetching and feature extraction to final hybrid credibility output‚Äîdemonstrating a robust, interpretable scoring system ready for chatbot integration in Deliverable 3.\n",
        "\n",
        "\n",
        "DELIVERABLE 3 needs to better intergrate the gemini chat bot so that it doesn't just search for links right away and actually provides a back and forth conversation. I also need to figure out a better solution to the credibility function not being able to access some websites, maybe i won't have the chat bot include those links.\n",
        "\n",
        "But for now, deliverable 2 has the function working properly and tackling edge cases such as providing low score and a reasoning behind the low score when not being able to access contents of a link."
      ],
      "metadata": {
        "id": "6YM42dAraTfR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9d4zjB_avZh"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}