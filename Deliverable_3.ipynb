{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOS7vgPKUQEEMRH+Nqctsvg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ‚úÖ Install all required packages (including the latest Gemini SDK)\n",
        "# !pip install -U google-generativeai requests beautifulsoup4 tldextract python-dateutil lxml scikit-learn\n",
        "# ‚úÖ Install all required packages (silently)\n",
        "!pip install -q -U google-generativeai requests beautifulsoup4 tldextract python-dateutil lxml scikit-learn > /dev/null 2>&1\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OHwpiPTOZ7z9",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# os.environ[\"SERP_API_KEY\"] = \"test\"\n",
        "\n",
        "#real SERP_API_KEY\n",
        "os.environ[\"SERP_API_KEY\"] = userdata.get('SERP_API_KEY')\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "kzfWaqdIcaMG",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# ==========  C R E D I B I L I T Y   S C O R I N G  =========\n",
        "# ============================================================\n",
        "# This section is the *deliverable_1* rule-based scorer\n",
        "\n",
        "import re, time, json, math, tldextract, requests, random, hashlib\n",
        "from urllib.parse import urlparse\n",
        "from datetime import datetime\n",
        "from dateutil import parser as dateparser\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- Networking defaults ---\n",
        "USER_AGENT = 'Mozilla/5.0 (CredibilityPOC/0.1)'   # Browser-y UA to avoid blocks\n",
        "DEFAULT_TIMEOUT = 12                               # Seconds\n",
        "\n",
        "# --- Heuristic signals ---\n",
        "CLICKBAIT_TERMS = [\n",
        "    \"you won't believe\", 'shocking', 'jaw-dropping', 'what happened next',\n",
        "    'unbelievable', 'miracle', 'exposed', \"secret they don't want you to know\"\n",
        "]\n",
        "TRANSPARENCY_HINTS = [\n",
        "    'author','byline','by ','by:','written by','editor','editorial',\n",
        "    'fact-check','fact check','sources','references','citations',\n",
        "    'methodology','about us','about the author','corrections','disclosures'\n",
        "]\n",
        "INSTITUTIONAL_TLDS = {'edu','gov','ac','sch','mil'}\n",
        "\n",
        "def fetch_html(url: str):\n",
        "    \"\"\"\n",
        "    Fetch raw HTML for a URL with retries and basic anti-block heuristics.\n",
        "    - Rotates realistic headers\n",
        "    - Exponential backoff on 403/429/5xx\n",
        "    - Fallback for Reddit: try old.reddit.com if www.reddit.com blocks\n",
        "    Returns (html_text, None) on success, or (None, 'error message') on failure.\n",
        "    \"\"\"\n",
        "    import random\n",
        "    import time\n",
        "    import requests\n",
        "    from urllib.parse import urlparse\n",
        "\n",
        "    # Primary + backup user-agent/header sets\n",
        "    header_candidates = [\n",
        "        {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                          \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                          \"Chrome/126.0 Safari/537.36\",\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "            \"Connection\": \"keep-alive\",\n",
        "        },\n",
        "        {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
        "                          \"AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\",\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        },\n",
        "        {\n",
        "            \"User-Agent\": USER_AGENT,  # your original UA, as a last resort\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    def reddit_alt(u: str) -> list[str]:\n",
        "        \"\"\"If it's a Reddit URL, also try old.reddit.com.\"\"\"\n",
        "        try:\n",
        "            parsed = urlparse(u)\n",
        "            if parsed.netloc.endswith(\"reddit.com\") and not parsed.netloc.startswith(\"old.\"):\n",
        "                alt = u.replace(\"//www.reddit.com\", \"//old.reddit.com\")\n",
        "                if alt == u:  # if it wasn't www, still try old.\n",
        "                    alt = u.replace(\"//reddit.com\", \"//old.reddit.com\")\n",
        "                return [u, alt]\n",
        "        except Exception:\n",
        "            pass\n",
        "        return [u]\n",
        "\n",
        "    urls_to_try = reddit_alt(url)\n",
        "    last_err = None\n",
        "\n",
        "    for candidate_url in urls_to_try:\n",
        "        # up to 3 attempts per candidate url, with exponential backoff\n",
        "        backoff = 1.0\n",
        "        for attempt in range(3):\n",
        "            headers = header_candidates[min(attempt, len(header_candidates)-1)]\n",
        "            try:\n",
        "                resp = requests.get(candidate_url, headers=headers, timeout=DEFAULT_TIMEOUT, allow_redirects=True)\n",
        "                status = resp.status_code\n",
        "\n",
        "                # Retry on \"blocked\"/rate-limited or transient server errors\n",
        "                if status in (403, 429) or 500 <= status < 600:\n",
        "                    last_err = f\"{status} {resp.reason}\"\n",
        "                    time.sleep(backoff)\n",
        "                    backoff *= 2.0\n",
        "                    continue\n",
        "\n",
        "                resp.raise_for_status()\n",
        "\n",
        "                # Prefer HTML content; some sites return other types\n",
        "                ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
        "                if \"text/html\" not in ctype:\n",
        "                    last_err = f\"Non-HTML content-type: {ctype}\"\n",
        "                    # Don‚Äôt retry endlessly for non-HTML; move to next candidate/final\n",
        "                    break\n",
        "\n",
        "                return resp.text, None\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                last_err = str(e)\n",
        "                time.sleep(backoff)\n",
        "                backoff *= 2.0\n",
        "                continue\n",
        "\n",
        "    return None, f\"Fetch error: {last_err or 'unknown error'}\"\n",
        "\n",
        "\n",
        "def extract_article_fields(html: str, url: str):\n",
        "    \"\"\"\n",
        "    Parse HTML to extract: title, author, published date, body text (paragraphs),\n",
        "    and link counts (total + external), plus transparency hint flag.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    text_chunks, title, author, published = [], None, None, None\n",
        "\n",
        "    # --- Title from <title> or OG meta ---\n",
        "    if soup.title and soup.title.string:\n",
        "        title = soup.title.string.strip()\n",
        "    mt = soup.find('meta', attrs={'property':'og:title'}) or soup.find('meta', attrs={'name':'title'})\n",
        "    if not title and mt and mt.get('content'):\n",
        "        title = mt['content'].strip()\n",
        "\n",
        "    # --- Author / byline in common locations ---\n",
        "    for selector in [\n",
        "        {'name':'meta','attrs':{'name':'author'}},\n",
        "        {'name':'meta','attrs':{'property':'article:author'}},\n",
        "        {'name':'span','class_':re.compile('author|byline', re.I)},\n",
        "        {'name':'div','class_':re.compile('author|byline', re.I)},\n",
        "        {'name':'a','class_':re.compile('author', re.I)},\n",
        "    ]:\n",
        "        if selector['name']=='meta':\n",
        "            node = soup.find('meta', attrs=selector['attrs'])\n",
        "            if node and node.get('content'):\n",
        "                author = node['content'].strip(); break\n",
        "        else:\n",
        "            node = soup.find(selector['name'], class_=selector.get('class_'))\n",
        "            if node and node.get_text(strip=True):\n",
        "                candidate = node.get_text(' ', strip=True)\n",
        "                if len(candidate) >= 3:\n",
        "                    author = candidate; break\n",
        "\n",
        "    # --- Publish date in common meta/time/span patterns ---\n",
        "    for date_sel in [\n",
        "        {'name':'meta','attrs':{'property':'article:published_time'}},\n",
        "        {'name':'meta','attrs':{'name':'date'}},\n",
        "        {'name':'time','attrs':{}},\n",
        "        {'name':'span','class_':re.compile('date|time', re.I)},\n",
        "    ]:\n",
        "        if date_sel['name']=='meta':\n",
        "            node = soup.find('meta', attrs=date_sel['attrs'])\n",
        "            if node and node.get('content'):\n",
        "                try:\n",
        "                    published = dateparser.parse(node['content'], fuzzy=True); break\n",
        "                except Exception:\n",
        "                    pass\n",
        "        else:\n",
        "            node = soup.find(date_sel['name'], class_=date_sel.get('class_'))\n",
        "            if node and node.get_text(strip=True):\n",
        "                try:\n",
        "                    published = dateparser.parse(node.get_text(strip=True), fuzzy=True); break\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    # --- Body text: prefer a likely article container, else all <p> ---\n",
        "    main_container = None\n",
        "    for cls in ['article','post','story','content','entry-content','article-body']:\n",
        "        mc = soup.find(True, class_=re.compile(cls, re.I))\n",
        "        if mc: main_container = mc; break\n",
        "    paragraphs = (main_container.find_all('p') if main_container else soup.find_all('p'))\n",
        "    for p in paragraphs:\n",
        "        t = p.get_text(' ', strip=True)\n",
        "        if t and len(t) > 40: text_chunks.append(t)\n",
        "    article_text = '\\n\\n'.join(text_chunks)[:100000]  # cap to avoid huge pages\n",
        "\n",
        "    # --- Link counts: total & external ---\n",
        "    all_links, external_links = [], []\n",
        "    base_host = urlparse(url).netloc.lower()\n",
        "    for a in soup.find_all('a', href=True):\n",
        "        href = a['href']\n",
        "        if href.startswith('http://') or href.startswith('https://'):\n",
        "            all_links.append(href)\n",
        "            if urlparse(href).netloc.lower() != base_host:\n",
        "                external_links.append(href)\n",
        "\n",
        "    # --- Transparency hint flag (string match) ---\n",
        "    full_text_for_hints = (article_text + ' ' + ' '.join(TRANSPARENCY_HINTS)).lower()\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'author': author,\n",
        "        'published': published.isoformat() if published else None,\n",
        "        'text': article_text,\n",
        "        'num_paragraphs': len(text_chunks),\n",
        "        'all_links': all_links,\n",
        "        'external_links': external_links,\n",
        "        'has_transparency_hints': any(h in full_text_for_hints for h in TRANSPARENCY_HINTS),\n",
        "    }\n",
        "\n",
        "def score_url(url: str, fields: dict):\n",
        "    \"\"\"\n",
        "    Apply heuristic scoring rules ‚Üí (score 0‚Äì100, explanation string).\n",
        "    Starts at 50 and adds/subtracts per signal.\n",
        "    \"\"\"\n",
        "    explanation_bits = []\n",
        "    score = 50  # neutral baseline\n",
        "\n",
        "    # HTTPS\n",
        "    if url.lower().startswith('https://'):\n",
        "        score += 12; explanation_bits.append('+12: uses HTTPS')\n",
        "    else:\n",
        "        score -= 10; explanation_bits.append('-10: not using HTTPS')\n",
        "\n",
        "    # Institutional TLD\n",
        "    ext = tldextract.extract(url)\n",
        "    tld_last = (ext.suffix.split('.')[-1] if ext.suffix else '')\n",
        "    if tld_last in INSTITUTIONAL_TLDS:\n",
        "        score += 14; explanation_bits.append(f'+14: institutional TLD ({tld_last})')\n",
        "\n",
        "    # Author/byline\n",
        "    if fields.get('author'):\n",
        "        score += 10; explanation_bits.append('+10: author/byline found')\n",
        "    else:\n",
        "        score -= 6; explanation_bits.append('-6: no clear author/byline')\n",
        "\n",
        "    # Published recency (NOTE: uses datetime.utcnow(), may warn in 3.12+)\n",
        "    published = fields.get('published')\n",
        "    if published:\n",
        "        try:\n",
        "            dt = dateparser.parse(published)\n",
        "            if (datetime.utcnow() - dt).days <= 3650:\n",
        "                score += 6; explanation_bits.append('+6: reasonably recent publication date')\n",
        "            else:\n",
        "                score -= 4; explanation_bits.append('-4: appears quite old')\n",
        "        except Exception:\n",
        "            explanation_bits.append('0: could not parse publication date reliably')\n",
        "    else:\n",
        "        explanation_bits.append('0: no publication date found')\n",
        "\n",
        "    # References\n",
        "    total_links = len(fields.get('all_links', []))\n",
        "    external_links_count = len(fields.get('external_links', []))\n",
        "    if total_links >= 5 and external_links_count >= 3:\n",
        "        score += 10; explanation_bits.append(f'+10: provides references (links: {total_links}, external: {external_links_count})')\n",
        "    elif total_links >= 2:\n",
        "        score += 4; explanation_bits.append(f'+4: some references (links: {total_links})')\n",
        "    else:\n",
        "        score -= 6; explanation_bits.append(f'-6: minimal/no references (links: {total_links})')\n",
        "\n",
        "    # Length (by paragraph count)\n",
        "    num_paras = fields.get('num_paragraphs', 0)\n",
        "    if num_paras >= 8:\n",
        "        score += 6; explanation_bits.append('+6: substantive article length')\n",
        "    elif num_paras >= 3:\n",
        "        score += 2; explanation_bits.append('+2: moderate article length')\n",
        "    else:\n",
        "        score -= 6; explanation_bits.append('-6: very short article text')\n",
        "\n",
        "    # Clickbait language\n",
        "    text_lower = (fields.get('text') or '').lower()\n",
        "    clickbait_hits = sum(1 for term in CLICKBAIT_TERMS if term in text_lower)\n",
        "    if clickbait_hits >= 2:\n",
        "        score -= 10; explanation_bits.append('-10: strong clickbait indicators')\n",
        "    elif clickbait_hits == 1:\n",
        "        score -= 4; explanation_bits.append('-4: mild clickbait indicators')\n",
        "\n",
        "    # Advertising/sponsor cues\n",
        "    ad_signals = len(re.findall(r\"advertis(e|ement)|sponsor(ed|ship)\", text_lower))\n",
        "    iframes_penalty = min(8, math.floor(ad_signals / 5) * 2)\n",
        "    if iframes_penalty:\n",
        "        score -= iframes_penalty; explanation_bits.append(f'-{iframes_penalty}: advertising/sponsorship language')\n",
        "\n",
        "    # Clamp score and join explanation\n",
        "    score = max(0, min(100, int(round(score))))\n",
        "    explanation = '; '.join(explanation_bits)\n",
        "    return score, explanation\n",
        "\n",
        "def evaluate_url(url: str):\n",
        "    \"\"\"\n",
        "    Orchestrator: fetch ‚Üí parse ‚Üí score.\n",
        "    Returns a dict with 'score', 'explanation', and 'details' on success.\n",
        "    Returns None if the page cannot be accessed (STRICT EXCLUDE).\n",
        "    \"\"\"\n",
        "    if not isinstance(url, str) or not url.strip():\n",
        "        return None  # exclude invalid\n",
        "\n",
        "    html, err = fetch_html(url)\n",
        "    if err:\n",
        "        return None  # STRICT EXCLUDE: do not return a low score or an error object\n",
        "\n",
        "    fields = extract_article_fields(html, url)\n",
        "    score, explanation = score_url(url, fields)\n",
        "\n",
        "    return {\n",
        "        'score': score,\n",
        "        'explanation': explanation,\n",
        "        'details': {\n",
        "            'url': url,\n",
        "            'title': fields.get('title'),\n",
        "            'author': fields.get('author'),\n",
        "            'published': fields.get('published'),\n",
        "            'num_paragraphs': fields.get('num_paragraphs'),\n",
        "            'total_links': len(fields.get('all_links', [])),\n",
        "            'external_links': len(fields.get('external_links', [])),\n",
        "        },\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# ======  H Y B R I D   ( R u l e s  +  L i n e a r  R e g ) =\n",
        "# ============================================================\n",
        "# This section:\n",
        "# 1) builds features from evaluate_url() outputs\n",
        "# 2) creates a labeled dataset of 20 real URLs (0‚Äì100 labels)\n",
        "# 3) runs 5-fold cross-validation for LinearRegression\n",
        "# 4) fits a final model on ALL available rows\n",
        "# 5) exposes hybrid_score(url, alpha) for inference\n",
        "#\n",
        "# Notes:\n",
        "# - If some URLs fail to fetch, they're STRICTLY EXCLUDED (no score, not shown).\n",
        "# - CV uses MAE and R^2 to give you both error and fit quality.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from datetime import datetime, timezone\n",
        "from dateutil import parser as dateparser\n",
        "\n",
        "# ---------- Feature Engineering ----------\n",
        "def _safe(value, default=0):\n",
        "    return value if value is not None else default\n",
        "\n",
        "def _https_flag(url: str) -> int:\n",
        "    return 1 if isinstance(url, str) and url.lower().startswith('https://') else 0\n",
        "\n",
        "def _institutional_tld_flag(url: str) -> int:\n",
        "    try:\n",
        "        ext = tldextract.extract(url)\n",
        "        tld_last = (ext.suffix.split('.')[-1] if ext.suffix else '')\n",
        "        return 1 if tld_last in {'edu','gov','ac','sch','mil'} else 0\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "def _days_since(published_iso: str) -> int:\n",
        "    if not published_iso:\n",
        "        return 99999  # treat unknown/absent as very old\n",
        "    try:\n",
        "        dt = dateparser.parse(published_iso)\n",
        "        return max(0, (datetime.utcnow() - dt).days)  # OK to mirror your original utc usage\n",
        "    except Exception:\n",
        "        return 99999\n",
        "\n",
        "def features_from_eval(eval_obj: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Build a feature dict using ONLY fields the original scorer returns.\n",
        "    These features are model-agnostic and cheap to compute.\n",
        "    \"\"\"\n",
        "    d = eval_obj.get('details', {})\n",
        "    url = d.get('url') or ''\n",
        "    feats = {\n",
        "        \"https\": _https_flag(url),\n",
        "        \"inst_tld\": _institutional_tld_flag(url),\n",
        "        \"has_author\": 1 if d.get('author') else 0,\n",
        "        \"num_paragraphs\": _safe(d.get('num_paragraphs'), 0),\n",
        "        \"total_links\": _safe(d.get('total_links'), 0),\n",
        "        \"external_links\": _safe(d.get('external_links'), 0),\n",
        "        \"days_since_pub\": _days_since(d.get('published')),\n",
        "    }\n",
        "    return feats\n",
        "\n",
        "FEATURE_ORDER = [\n",
        "    \"https\",\n",
        "    \"inst_tld\",\n",
        "    \"has_author\",\n",
        "    \"num_paragraphs\",\n",
        "    \"total_links\",\n",
        "    \"external_links\",\n",
        "    \"days_since_pub\",\n",
        "]\n",
        "\n",
        "def vectorize_features(feat_dict: dict, feature_order=FEATURE_ORDER):\n",
        "    \"\"\"Convert a feature dict to a fixed-order numeric vector.\"\"\"\n",
        "    return np.array([feat_dict.get(k, 0) for k in feature_order], dtype=float)\n",
        "\n",
        "# ---------- Labeled Dataset (20 REAL URLs, mixed quality) ----------\n",
        "# Labels are illustrative 0‚Äì100 targets. Adjust as you refine ground truth.\n",
        "LABELED_URLS = [\n",
        "    # Highly credible / institutional / quality editorial\n",
        "    (\"https://www.cdc.gov/flu/about/index.html\", 92),\n",
        "    (\"https://www.nih.gov/news-events/news-releases\", 88),\n",
        "    (\"https://www.mayoclinic.org/diseases-conditions/dehydration/symptoms-causes/syc-20354086\", 85),\n",
        "    (\"https://www.who.int/news-room/fact-sheets/detail/diabetes\", 90),\n",
        "    (\"https://www.britannica.com/science/photosynthesis\", 86),\n",
        "    (\"https://www.health.harvard.edu/staying-healthy/what-is-intermittent-fasting\", 78),\n",
        "    (\"https://www.hopkinsmedicine.org/health/conditions-and-diseases\", 84),\n",
        "    (\"https://www.bbc.com/news/science_and_environment\", 80),\n",
        "    (\"https://www.reuters.com/world/us/\", 80),\n",
        "    (\"https://apnews.com/hub/technology\", 78),\n",
        "    (\"https://en.wikipedia.org/wiki/Ice_cream\", 75),\n",
        "\n",
        "    # Mid credibility consumer health / informative\n",
        "    (\"https://www.healthline.com/nutrition/green-tea-and-weight-loss\", 70),\n",
        "    (\"https://www.webmd.com/diet/obesity/features/green-tea-and-weight-loss\", 68),\n",
        "    (\"https://www.nature.com/scitable/definition/photosynthesis-288/\", 82),\n",
        "    (\"https://med.stanford.edu/news/all-news.html\", 82),\n",
        "\n",
        "    # Lower credibility / UGC / lighter editorial\n",
        "    (\"https://medium.com/\", 45),\n",
        "    (\"https://www.reddit.com/r/icecreamery/comments/19elt19/looking_for_resources_to_learn_how_to_make_ice/\", 20),\n",
        "    (\"https://www.quora.com/Is-green-tea-good-for-weight-loss\", 25),\n",
        "    (\"https://www.livestrong.com/article/13715706-green-tea-benefits/\", 60),\n",
        "    (\"https://www.buzzfeed.com/\", 40),\n",
        "]\n",
        "\n",
        "# ---------- Build Dataset by Evaluating & Featurizing ----------\n",
        "rows_X, rows_y = [], []\n",
        "\n",
        "for url, label in LABELED_URLS:\n",
        "    try:\n",
        "        ev = evaluate_url(url)\n",
        "        if ev is None:  # STRICT EXCLUDE\n",
        "            continue\n",
        "        feats = features_from_eval(ev)\n",
        "        x = vectorize_features(feats)\n",
        "        rows_X.append(x)\n",
        "        rows_y.append(float(label))\n",
        "    except Exception:\n",
        "        # Strict policy: exclude on any error\n",
        "        continue\n",
        "\n",
        "X = np.vstack(rows_X) if rows_X else np.zeros((0, len(FEATURE_ORDER)))\n",
        "y = np.array(rows_y) if rows_y else np.zeros((0,))\n",
        "\n",
        "print(f\"Prepared dataset: {X.shape[0]} rows √ó {X.shape[1]} features.\")\n",
        "\n",
        "# ---------- 5-Fold Cross-Validation ----------\n",
        "# We use KFold regression CV with two metrics:\n",
        "#  - MAE (lower is better): absolute error in points of the 0‚Äì100 score\n",
        "#  - R^2 (higher is better): variance explained\n",
        "if len(X) >= 5:\n",
        "    k = min(5, len(X))  # guard in case many rows were excluded\n",
        "    if k < 2:\n",
        "        print(\"\\nNot enough rows for CV; skipping cross-validation.\")\n",
        "    else:\n",
        "        kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "        # MAE (scikit returns negative MAE for loss; invert sign to report positive MAE)\n",
        "        mae_scores = cross_val_score(LinearRegression(), X, y, cv=kf, scoring=\"neg_mean_absolute_error\")\n",
        "        r2_scores  = cross_val_score(LinearRegression(), X, y, cv=kf, scoring=\"r2\")\n",
        "        mae_vals = -mae_scores  # turn to positive\n",
        "        print(f\"\\n{k}-fold CV results on {len(X)} rows:\")\n",
        "        print(f\"  MAE per fold: {np.round(mae_vals, 2)} | mean={mae_vals.mean():.2f}\")\n",
        "        print(f\"  R^2 per fold:  {np.round(r2_scores, 3)} | mean={r2_scores.mean():.3f}\")\n",
        "else:\n",
        "    print(\"\\nNot enough rows for CV; need ‚â•5 examples.\")\n",
        "\n",
        "# ---------- Fit Final Linear Regression on ALL available rows ----------\n",
        "linreg = LinearRegression()\n",
        "if len(X) >= 2:\n",
        "    linreg.fit(X, y)\n",
        "    print(\"\\nFinal model (trained on all available rows):\")\n",
        "    for kname, w in zip(FEATURE_ORDER, linreg.coef_):\n",
        "        print(f\"  {kname:>16s}: {w: .3f}\")\n",
        "    print(f\"  {'Intercept':>16s}: {linreg.intercept_: .3f}\")\n",
        "else:\n",
        "    print(\"\\nNot enough rows to train final model (need ‚â•2). Using neutral ML predictions.\")\n",
        "\n",
        "# ---------- Inference helpers ----------\n",
        "def predict_ml_score_from_eval(eval_obj: dict) -> float:\n",
        "    \"\"\"\n",
        "    Predict a 0‚Äì100 credibility score from features using the trained LinearRegression.\n",
        "    Falls back to 50.0 if not enough training data is available.\n",
        "    \"\"\"\n",
        "    if len(X) < 2:\n",
        "        return 50.0\n",
        "    feats = features_from_eval(eval_obj)\n",
        "    x = vectorize_features(feats).reshape(1, -1)\n",
        "    pred = linreg.predict(x)[0]\n",
        "    return float(np.clip(pred, 0.0, 100.0))\n",
        "\n",
        "def _stars(score_0_100: float) -> str:\n",
        "    stars = int(round(score_0_100 / 20))\n",
        "    stars = max(0, min(5, stars))\n",
        "    return \"‚òÖ\"*stars + \"‚òÜ\"*(5 - stars)\n",
        "\n",
        "def hybrid_score(url: str, alpha: float = 0.6) -> dict | None:\n",
        "    \"\"\"\n",
        "    Blend rule-based score with ML-predicted score:\n",
        "        final = alpha * rule_score + (1 - alpha) * ml_score\n",
        "\n",
        "    STRICT EXCLUDE:\n",
        "    - If fetch/evaluation fails, return None (do not score or include).\n",
        "    \"\"\"\n",
        "    ev = evaluate_url(url)\n",
        "    if ev is None:\n",
        "        return None  # STRICT EXCLUDE\n",
        "\n",
        "    details = ev.get(\"details\", {})\n",
        "    rule_score = float(ev.get(\"score\", 0.0))\n",
        "\n",
        "    # Normal path: combine rules + ML\n",
        "    ml_score = predict_ml_score_from_eval(ev)\n",
        "    final = float(np.clip(alpha * rule_score + (1 - alpha) * ml_score, 0.0, 100.0))\n",
        "\n",
        "    # Stars\n",
        "    stars = int(round(final / 20))\n",
        "    stars = max(0, min(5, stars))\n",
        "    star_str = \"‚òÖ\" * stars + \"‚òÜ\" * (5 - stars)\n",
        "\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"title\": details.get(\"title\"),\n",
        "        \"rule_score\": round(rule_score, 1),\n",
        "        \"ml_score\": round(ml_score, 1),\n",
        "        \"hybrid_score\": round(final, 1),\n",
        "        \"stars\": star_str,\n",
        "        \"explanation\": ev.get(\"explanation\", \"No detailed explanation available.\"),\n",
        "        \"details\": details,\n",
        "    }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BTtvEQ7VR8vn",
        "outputId": "f8507d19-cb78-4245-c663-626c83ea8c47",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared dataset: 13 rows √ó 7 features.\n",
            "\n",
            "5-fold CV results on 13 rows:\n",
            "  MAE per fold: [135.42 115.88  11.52  95.74  19.37] | mean=75.59\n",
            "  R^2 per fold:  [-1.160880e+02 -1.695836e+03  3.910000e-01 -4.500741e+03 -1.558600e+01] | mean=-1265.572\n",
            "\n",
            "Final model (trained on all available rows):\n",
            "             https:  0.000\n",
            "          inst_tld:  25.932\n",
            "        has_author: -5.099\n",
            "    num_paragraphs:  0.194\n",
            "       total_links:  0.013\n",
            "    external_links:  0.014\n",
            "    days_since_pub: -0.000\n",
            "         Intercept:  82.194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# --- Improved SerpAPI search (clean results, de-dupe, filter, robust errors) ---\n",
        "import os, re, requests\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "SERP_API_KEY = os.getenv(\"SERP_API_KEY\")\n",
        "\n",
        "# Common low-signal/social/video domains to exclude by default (tune as needed)\n",
        "_DEFAULT_EXCLUDE_DOMAINS = {\n",
        "    \"reddit.com\", \"www.reddit.com\", \"old.reddit.com\",\n",
        "    \"x.com\", \"twitter.com\", \"www.twitter.com\",\n",
        "    \"tiktok.com\", \"www.tiktok.com\",\n",
        "    \"pinterest.com\", \"www.pinterest.com\",\n",
        "    \"facebook.com\", \"www.facebook.com\",\n",
        "    \"instagram.com\", \"www.instagram.com\",\n",
        "    \"youtube.com\", \"www.youtube.com\", \"youtu.be\"\n",
        "}\n",
        "\n",
        "# Skip obvious non-article filetypes\n",
        "_SKIP_FILETYPES = re.compile(r\"\\.(pdf|pptx?|docx?|xlsx?|zip|rar)(?:$|\\?)\", re.I)\n",
        "\n",
        "def _host(url: str) -> str:\n",
        "    return urlparse(url).netloc.lower()\n",
        "\n",
        "def search_google(\n",
        "    query: str,\n",
        "    num_results: int = 5,\n",
        "    exclude_domains: set | None = None,\n",
        "    allow_news_results: bool = True,\n",
        "    timeout: int = 20,\n",
        ") -> list[dict]:\n",
        "    \"\"\"\n",
        "    Search Google via SerpAPI and return a clean list of results:\n",
        "        [{\"title\": str, \"link\": str, \"snippet\": str}, ...]\n",
        "    - De-duplicates by link and avoids flooding from the same host\n",
        "    - Skips social/UGC/video sites and non-HTML filetypes\n",
        "    - Optionally includes Google News results\n",
        "    \"\"\"\n",
        "    if not SERP_API_KEY:\n",
        "        raise RuntimeError(\"Missing SERP_API_KEY / SERPAPI_API_KEY. Set it in the environment first.\")\n",
        "\n",
        "    exclude = set(_DEFAULT_EXCLUDE_DOMAINS)\n",
        "    if exclude_domains:\n",
        "        exclude |= set(exclude_domains)\n",
        "\n",
        "    params = {\n",
        "        \"engine\": \"google\",\n",
        "        \"q\": query,\n",
        "        \"api_key\": SERP_API_KEY,\n",
        "        \"num\": 10,          # pull extra, then filter/trim to num_results\n",
        "        \"hl\": \"en\",\n",
        "        \"gl\": \"us\",\n",
        "        \"safe\": \"active\",   # optional: reduce NSFW\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        resp = requests.get(\"https://serpapi.com/search\", params=params, timeout=timeout)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "    except Exception as e:\n",
        "        # On failure, return empty list (caller can handle and message user)\n",
        "        print(f\"[SerpAPI] Error: {e}\")\n",
        "        return []\n",
        "\n",
        "    candidates: list[dict] = []\n",
        "\n",
        "    # 1) Organic results\n",
        "    for r in (data.get(\"organic_results\") or []):\n",
        "        link = r.get(\"link\")\n",
        "        title = r.get(\"title\")\n",
        "        snippet = r.get(\"snippet\") or \"\"\n",
        "        if not link or _SKIP_FILETYPES.search(link):\n",
        "            continue\n",
        "        host = _host(link)\n",
        "        if host in exclude:\n",
        "            continue\n",
        "        candidates.append({\"title\": title, \"link\": link, \"snippet\": snippet})\n",
        "\n",
        "    # 2) Optional: News results (helpful for timely topics)\n",
        "    if allow_news_results:\n",
        "        for r in (data.get(\"news_results\") or []):\n",
        "            link = r.get(\"link\")\n",
        "            title = r.get(\"title\")\n",
        "            snippet = r.get(\"snippet\") or \"\"\n",
        "            if not link or _SKIP_FILETYPES.search(link):\n",
        "                continue\n",
        "            host = _host(link)\n",
        "            if host in exclude:\n",
        "                continue\n",
        "            candidates.append({\"title\": title, \"link\": link, \"snippet\": snippet})\n",
        "\n",
        "    # De-duplicate by link; also avoid over-representing a single host\n",
        "    cleaned, seen_links, seen_hosts = [], set(), set()\n",
        "    for c in candidates:\n",
        "        link, host = c[\"link\"], _host(c[\"link\"])\n",
        "        if link in seen_links:\n",
        "            continue\n",
        "        # If we already have enough and this host is duplicate, skip\n",
        "        if host in seen_hosts and len(cleaned) >= num_results:\n",
        "            continue\n",
        "        seen_links.add(link)\n",
        "        seen_hosts.add(host)\n",
        "        cleaned.append(c)\n",
        "        if len(cleaned) >= num_results:\n",
        "            break\n",
        "\n",
        "    return cleaned"
      ],
      "metadata": {
        "id": "dMWhH62SWyQO",
        "cellView": "form"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Gemini Chat\n",
        "# --- Gemini Chat & Summarization Helpers (chat-first; robust + clean fallbacks) ---\n",
        "import os, re, google.generativeai as genai\n",
        "\n",
        "# ---- Safeguard: prevent wrong localhost routing ----\n",
        "for k in [\"GOOGLE_API_BASE_URL\", \"GOOGLE_AI_API_BASE\", \"GOOGLE_API_ENDPOINT\"]:\n",
        "    os.environ.pop(k, None)\n",
        "\n",
        "# ---- Configure Gemini API key + explicit endpoint ----\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "if not GEMINI_API_KEY:\n",
        "    raise RuntimeError(\"Missing GEMINI_API_KEY. Please set it first.\")\n",
        "genai.configure(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    client_options={\"api_endpoint\": \"https://generativelanguage.googleapis.com\"}\n",
        ")\n",
        "\n",
        "# ---- Pick a valid model across SDK versions ----\n",
        "_CANDIDATE_MODELS = [\"gemini-1.5-flash\", \"gemini-1.5-pro\", \"gemini-pro\"]\n",
        "\n",
        "def _get_model():\n",
        "    for name in _CANDIDATE_MODELS:\n",
        "        try:\n",
        "            m = genai.GenerativeModel(name)\n",
        "            _ = m.generate_content(\"ping\")\n",
        "            return m\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "_gemini = _get_model()\n",
        "\n",
        "def _gemini_text(prompt: str) -> str | None:\n",
        "    \"\"\"Call Gemini safely; never expose internal errors.\"\"\"\n",
        "    if _gemini is None:\n",
        "        return None\n",
        "    try:\n",
        "        resp = _gemini.generate_content(prompt)\n",
        "        out = getattr(resp, \"text\", None)\n",
        "        if not out and hasattr(resp, \"candidates\") and resp.candidates:\n",
        "            cand = resp.candidates[0]\n",
        "            if hasattr(cand, \"content\") and cand.content and getattr(cand.content, \"parts\", None):\n",
        "                out = cand.content.parts[0].text\n",
        "        return out.strip() if out else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---- Summarization (silent Gemini fallback) ----\n",
        "def summarize_text(text: str, query: str) -> str:\n",
        "    if not text or not text.strip():\n",
        "        return \"[No article text extracted to summarize.]\"\n",
        "\n",
        "    clipped = text[:5000]\n",
        "    prompt = (\n",
        "        \"You are a concise, factual assistant.\\n\"\n",
        "        f\"User topic: {query}\\n\\n\"\n",
        "        \"Write a 3‚Äì5 sentence factual summary using the content below.\\n\\n\"\n",
        "        f\"Content:\\n{clipped}\"\n",
        "    )\n",
        "    out = _gemini_text(prompt)\n",
        "    if out:\n",
        "        return out\n",
        "\n",
        "    sents = re.split(r'(?<=[.!?])\\s+', clipped)\n",
        "    return \" \".join(sents[:4]).strip()\n",
        "\n",
        "# ---- Intent / helper patterns ----\n",
        "_SEARCH_TRIGGERS = {\"find\",\"search\",\"sources\",\"articles\",\"look up\",\"get sources\",\"show links\"}\n",
        "_CONFIRM_YES = {\"yes\",\"y\",\"yep\",\"yeah\",\"sure\",\"ok\",\"okay\",\"do it\",\"go ahead\",\"please do\"}\n",
        "_CONFIRM_NO = {\"no\",\"n\",\"nope\",\"not now\",\"later\",\"stop\",\"cancel\",\"skip\"}\n",
        "_STOP_WORDS = {\"exit\",\"quit\",\"bye\"}\n",
        "_GREETING = re.compile(r\"\\b(hi|hello|hey|how are you|good (morning|afternoon|evening))\\b\", re.I)\n",
        "_VAGUE = re.compile(r\"^(what\\s+(is|do)|i\\s+don'?t\\s+know|help|huh|\\?)\", re.I)\n",
        "\n",
        "def extract_urls(t): return re.findall(r\"https?://\\S+\", t or \"\")\n",
        "\n",
        "def classify_intent(msg: str) -> str:\n",
        "    msg_l = (msg or \"\").lower()\n",
        "    if any(w in msg_l for w in _STOP_WORDS): return \"stop\"\n",
        "    if extract_urls(msg): return \"urls_pasted\"\n",
        "    if any(w in msg_l for w in _SEARCH_TRIGGERS): return \"trigger_search\"\n",
        "    return \"converse\"\n",
        "\n",
        "def is_confirmation_yes(t): return any(w in (t or \"\").lower() for w in _CONFIRM_YES)\n",
        "def is_confirmation_no(t):  return any(w in (t or \"\").lower() for w in _CONFIRM_NO)\n",
        "def _is_small_talk(t): return bool(_GREETING.search(t or \"\"))\n",
        "def _is_vague(t): return bool(_VAGUE.search((t or \"\").strip()))\n",
        "\n",
        "def get_topic_from_history(hist: list[dict]) -> str | None:\n",
        "    for m in reversed(hist):\n",
        "        if m.get(\"role\") != \"user\": continue\n",
        "        txt = (m.get(\"content\") or \"\").strip()\n",
        "        if not txt or extract_urls(txt) or _is_small_talk(txt) or _is_vague(txt):\n",
        "            continue\n",
        "        if txt.lower() in {\"yes\",\"no\",\"ok\",\"okay\",\"cool\",\"thanks\",\"thank you\"}:\n",
        "            continue\n",
        "        if len(re.findall(r\"[A-Za-z]{2,}\", txt)) >= 2:\n",
        "            return txt\n",
        "    return None\n",
        "\n",
        "def gemini_chat_reply(hist: list[dict], user_msg: str) -> str:\n",
        "    \"\"\"Friendly, contextual chat‚Äîno parroting.\"\"\"\n",
        "    if _is_small_talk(user_msg):\n",
        "        return \"I‚Äôm doing well‚Äîthanks for asking! What topic are you curious about today?\"\n",
        "    if _is_vague(user_msg):\n",
        "        return \"No worries‚Äîwhat subject would you like to explore? For example: ice cream, data science, or cars.\"\n",
        "\n",
        "    topic = get_topic_from_history(hist)\n",
        "    if topic and _gemini:\n",
        "        convo = \"\\n\".join(f\"{m['role'].capitalize()}: {m['content']}\" for m in hist[-6:])\n",
        "        prompt = (\n",
        "            \"You are CredBot: natural, concise, and curious. \"\n",
        "            \"If a topic exists, share one factual insight or ask one smart question about it. \"\n",
        "            \"Never echo the user's sentence. \"\n",
        "            f\"Known topic: {topic}\\n\\nConversation:\\n{convo}\\n\\nReply naturally in 1‚Äì2 sentences.\"\n",
        "        )\n",
        "        out = _gemini_text(prompt)\n",
        "        if out: return out\n",
        "    if topic:\n",
        "        return f\"{topic.split()[0].capitalize()} sounds interesting. Should I pull some sources for you?\"\n",
        "    return \"What would you like to talk about today?\"\n"
      ],
      "metadata": {
        "id": "IsrPre2Qfhte",
        "cellView": "form"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Conversational Credbot\n",
        "# ============================\n",
        "# Conversational CredBot (chat-first; memory + auto-search confirmation)\n",
        "# ============================\n",
        "import re\n",
        "\n",
        "chat_history = []\n",
        "_pending_links: list[str] = []\n",
        "\n",
        "AUTO_SEARCH_BEHAVIOR = \"ask\"   # or \"auto\"\n",
        "AUTO_SEARCH_AFTER = 3\n",
        "MEMORY_SUMMARY_EVERY = 4\n",
        "\n",
        "_turns_since_last_search = 0\n",
        "_last_summary_turn_idx = -1\n",
        "\n",
        "def display_response(title, url, summary, hybrid_dict):\n",
        "    score = hybrid_dict.get(\"hybrid_score\", 0.0)\n",
        "    stars = hybrid_dict.get(\"stars\") or (\"‚òÖ\" * int(round(score / 20)))\n",
        "    why = hybrid_dict.get(\"explanation\", \"\").strip()\n",
        "    print(f\"\\nüì∞ Title: {title or '[No Title]'}\")\n",
        "    print(f\"üîó Link: {url}\")\n",
        "    print(f\"üìÑ Summary: {summary}\")\n",
        "    print(f\"‚≠ê Credibility: {stars} ({score}/100)\")\n",
        "    if why: print(f\"üìù Why: {why}\\n\")\n",
        "\n",
        "def _score_and_summarize_url(link: str, user_query: str) -> bool:\n",
        "    hybrid = hybrid_score(link, alpha=0.6)\n",
        "    if hybrid is None: return False\n",
        "    html, err = fetch_html(link)\n",
        "    if err or not html: return False\n",
        "    try:\n",
        "        fields = extract_article_fields(html, link)\n",
        "        text = fields.get(\"text\") or \"\"\n",
        "        if not text.strip(): return False\n",
        "        summary = summarize_text(text, user_query)\n",
        "        title = fields.get(\"title\") or \"[No Title]\"\n",
        "        display_response(title, link, summary, hybrid)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _run_search_flow(topic: str) -> int:\n",
        "    print(\"üîç Searching for sources...\")\n",
        "    results = search_google(topic, num_results=5)\n",
        "    if not results:\n",
        "        print(\"‚ö†Ô∏è No relevant articles found.\")\n",
        "        return 0\n",
        "    shown = 0\n",
        "    for item in results:\n",
        "        link = item.get(\"link\")\n",
        "        if link and _score_and_summarize_url(link, topic): shown += 1\n",
        "    if not shown:\n",
        "        print(\"‚ö†Ô∏è I couldn‚Äôt access any promising sources for that query.\")\n",
        "    return shown\n",
        "\n",
        "def run_chatbot():\n",
        "    print(\"ü§ñ Hi, I‚Äôm CredBot. We can chat normally. I‚Äôll remember the conversation and the topic. (Type 'exit' to quit.)\")\n",
        "    global _pending_links, _turns_since_last_search, _last_summary_turn_idx\n",
        "    user_turn_index = 0\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \").strip()\n",
        "        if not user_input: continue\n",
        "        if user_input.lower() in {\"exit\",\"quit\",\"bye\"}:\n",
        "            print(\"üëã Goodbye!\"); break\n",
        "\n",
        "        user_turn_index += 1\n",
        "        chat_history.append({\"role\":\"user\",\"content\":user_input})\n",
        "\n",
        "        # ---- Handle confirmations ----\n",
        "        if _pending_links:\n",
        "            if is_confirmation_yes(user_input):\n",
        "                if len(_pending_links)==1 and _pending_links[0].startswith(\"AUTOSEARCH::\"):\n",
        "                    topic=_pending_links[0].split(\"::\",1)[1]\n",
        "                    print(f\"üîç Searching for ‚Äú{topic}‚Äù...\")\n",
        "                    shown=_run_search_flow(topic)\n",
        "                    chat_history.append({\"role\":\"bot\",\"content\":f\"[auto-confirmed] Returned {shown} results.\"})\n",
        "                    _pending_links=[]; _turns_since_last_search=0; continue\n",
        "                shown=0; topic=get_topic_from_history(chat_history) or user_input\n",
        "                for l in _pending_links:\n",
        "                    if not l.startswith(\"AUTOSEARCH::\"):\n",
        "                        shown+=1 if _score_and_summarize_url(l,topic) else 0\n",
        "                _pending_links=[]\n",
        "                if not shown: print(\"‚ö†Ô∏è I couldn‚Äôt access those links.\")\n",
        "                chat_history.append({\"role\":\"bot\",\"content\":f\"Processed {shown} links.\"})\n",
        "                _turns_since_last_search=0; continue\n",
        "            elif is_confirmation_no(user_input):\n",
        "                _pending_links=[]; print(\"üëç Okay, I won‚Äôt run that. What else would you like to do?\")\n",
        "                chat_history.append({\"role\":\"bot\",\"content\":\"User declined action.\"}); continue\n",
        "\n",
        "        # ---- Intent routing ----\n",
        "        intent=classify_intent(user_input)\n",
        "\n",
        "        if intent==\"urls_pasted\":\n",
        "            links=extract_urls(user_input)\n",
        "            seen=set(); _pending_links=[u for u in links if not (u in seen or seen.add(u))]\n",
        "            print(\"üîó I see you shared link(s). Do you want me to check their credibility? (yes/no)\")\n",
        "            chat_history.append({\"role\":\"bot\",\"content\":\"Asked to confirm scoring pasted links.\"}); continue\n",
        "\n",
        "        if intent==\"trigger_search\":\n",
        "            topic=get_topic_from_history(chat_history) or user_input\n",
        "            shown=_run_search_flow(topic)\n",
        "            chat_history.append({\"role\":\"bot\",\"content\":f\"Returned {shown} sources.\"})\n",
        "            _turns_since_last_search=0; continue\n",
        "\n",
        "        # ---- Default: converse normally ----\n",
        "        bot=gemini_chat_reply(chat_history,user_input)\n",
        "        print(f\"ü§ñ {bot}\")\n",
        "        chat_history.append({\"role\":\"bot\",\"content\":bot})\n",
        "        _turns_since_last_search+=1\n",
        "\n",
        "        # ---- Memory summary ----\n",
        "        if (MEMORY_SUMMARY_EVERY and user_turn_index%MEMORY_SUMMARY_EVERY==0 and not _pending_links):\n",
        "            if 'summarize_conversation' in globals():\n",
        "                summary=summarize_conversation(chat_history)\n",
        "                print(f\"\\nüß† So far, here‚Äôs our focus:\\n{summary}\")\n",
        "                chat_history.append({\"role\":\"bot\",\"content\":f\"[memory-summary]\\\\n{summary}\"})\n",
        "                _last_summary_turn_idx=user_turn_index\n",
        "\n",
        "        # ---- Auto-search offer ----\n",
        "        topic=get_topic_from_history(chat_history)\n",
        "        if topic and _turns_since_last_search>=AUTO_SEARCH_AFTER and not _pending_links:\n",
        "            if AUTO_SEARCH_BEHAVIOR==\"ask\":\n",
        "                print(f\"\\nüîé Want me to pull some sources on ‚Äú{topic}‚Äù? (yes/no)\")\n",
        "                chat_history.append({\"role\":\"bot\",\"content\":f\"Offer to search on: {topic}\"})\n",
        "                _pending_links=[f\"AUTOSEARCH::{topic}\"]\n",
        "            else:\n",
        "                shown=_run_search_flow(topic)\n",
        "                chat_history.append({\"role\":\"bot\",\"content\":f\"[auto] Returned {shown} results.\"})\n",
        "                _turns_since_last_search=0\n"
      ],
      "metadata": {
        "id": "Kxb0TjgGfm_K",
        "cellView": "form"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R0q5nlL3j8w3",
        "outputId": "3cf6fcea-8036-4919-e364-121fe51c920a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Hi, I‚Äôm CredBot. We can chat normally. I‚Äôll remember the conversation and the topic. (Type 'exit' to quit.)\n",
            "\n",
            "You: how are you doing\n",
            "ü§ñ I‚Äôm doing well‚Äîthanks for asking! What topic are you curious about today?\n",
            "\n",
            "You: i don't want to learn about anything\n",
            "ü§ñ I sounds interesting. Should I pull some sources for you?\n",
            "\n",
            "You: what\n",
            "ü§ñ I sounds interesting. Should I pull some sources for you?\n",
            "\n",
            "üîé Want me to pull some sources on ‚Äúi don't want to learn about anything‚Äù? (yes/no)\n",
            "\n",
            "You: no tell me about ice cream\n",
            "üëç Okay, I won‚Äôt run that. What else would you like to do?\n",
            "\n",
            "You: i want to learn about ice cream\n",
            "ü§ñ I sounds interesting. Should I pull some sources for you?\n",
            "\n",
            "üîé Want me to pull some sources on ‚Äúi want to learn about ice cream‚Äù? (yes/no)\n",
            "\n",
            "You: yes\n",
            "üîç Searching for ‚Äúi want to learn about ice cream‚Äù...\n",
            "üîç Searching for sources...\n",
            "\n",
            "üì∞ Title: The History of Ice Cream - IDFA\n",
            "üîó Link: https://www.idfa.org/the-history-of-ice-cream\n",
            "üìÑ Summary: Ice cream's origins are known to reach back as far as the second century B.C., although no specific date of origin nor inventor has been undisputably credited with its discovery. We know that Alexander the Great enjoyed snow and ice flavored with honey and nectar. Biblical references also show that King Solomon was fond of iced drinks during harvesting. During the Roman Empire, Nero Claudius Caesar (A.D.\n",
            "‚≠ê Credibility: ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (65.8/100)\n",
            "üìù Why: +12: uses HTTPS; -6: no clear author/byline; 0: no publication date found; +10: provides references (links: 39, external: 10); +2: moderate article length\n",
            "\n",
            "\n",
            "üì∞ Title: 15 Ice Cream Fun Facts That You Didn't Know - My/Mochi Blog\n",
            "üîó Link: https://www.mymochi.com/blog/fun-facts-about-ice-cream-that-you-didnt-know/\n",
            "üìÑ Summary: Do you love ice cream as much as we do? If you can‚Äôt go a day, a week, or even a month without the delicious flavors of ice cream, then we know how you feel. While you may be a pro at eating ice cream, did you know that there are some really interesting facts about ice cream? We have found some cool tidbits about ice cream that you may not have heard before.\n",
            "‚≠ê Credibility: ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (69.4/100)\n",
            "üìù Why: +12: uses HTTPS; +10: author/byline found; 0: could not parse publication date reliably; +4: some references (links: 31); +2: moderate article length\n",
            "\n",
            "\n",
            "You: I also want to learn about asbestos\n",
            "ü§ñ I sounds interesting. Should I pull some sources for you?\n",
            "\n",
            "You: yes\n",
            "ü§ñ I sounds interesting. Should I pull some sources for you?\n",
            "\n",
            "You: search for asbestos\n",
            "üîç Searching for sources...\n",
            "\n",
            "üì∞ Title: Asbestos Control Bureau | Department of Labor\n",
            "üîó Link: https://dol.ny.gov/asbestos-control-bureau\n",
            "üìÑ Summary: The Asbestos Control Bureau (ACB) oversees the abatement of toxic hazards associated with asbestos fiber by performing inspections during the rehabilitation, reconstruction, or demolition of buildings and other structures originally constructed with asbestos or asbestos-containing materials. The ACB also responds to complaints with regards to asbestos. As well as¬†enforcing¬†the New York State Labor Law and Industrial Code Rule 56. For general information about asbestos and asbestos rules, click on the button below.\n",
            "‚≠ê Credibility: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (88.4/100)\n",
            "üìù Why: +12: uses HTTPS; +14: institutional TLD (gov); -6: no clear author/byline; 0: no publication date found; +10: provides references (links: 41, external: 30); +6: substantive article length\n",
            "\n",
            "\n",
            "üì∞ Title: Asbestos Information for Homeowners & Renters\n",
            "üîó Link: https://www.health.ny.gov/environmental/indoors/asbestos/homeowners.htm\n",
            "üìÑ Summary: Asbestos may still be found in many different building materials and products, regardless of their age. Therefore, the only way to definitively know is to have samples of the material in question analyzed. See the information on Sampling and Testing for more information. Visit the U.S.\n",
            "‚≠ê Credibility: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (87.1/100)\n",
            "üìù Why: +12: uses HTTPS; +14: institutional TLD (gov); -6: no clear author/byline; 0: no publication date found; +10: provides references (links: 32, external: 32); +6: substantive article length\n",
            "\n",
            "\n",
            "üì∞ Title: Asbestos Abatement - DEP\n",
            "üîó Link: https://www.nyc.gov/site/dep/environment/asbestos-abatement.page\n",
            "üìÑ Summary: Asbestos is a naturally occurring fibrous mineral with a high tensile strength, the ability to be woven and is resistant to heat, electricity, and most chemicals. Because of these properties, asbestos fibers have been used in a variety of building materials since the late 1800s, such as roofing materials, ceiling and floor tiles, paper and cement products, wall plaster and thermal insulations. Asbestos can be found in almost all buildings constructed prior to 1989. When asbestos is disturbed and becomes airborne, it may enter the respiratory system if the appropriate Personal Protective Equipment (PPE) is not used.\n",
            "‚≠ê Credibility: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (87.3/100)\n",
            "üìù Why: +12: uses HTTPS; +14: institutional TLD (gov); -6: no clear author/byline; 0: no publication date found; +10: provides references (links: 12, external: 11); +6: substantive article length\n",
            "\n",
            "\n",
            "üì∞ Title: Certified Asbestos Investigator¬†¬∑ NYC311\n",
            "üîó Link: https://portal.311.nyc.gov/article/?kanumber=KA-02448\n",
            "üìÑ Summary: The Department of Environmental Protection (DEP) requires certification for those who handle or investigate asbestos. While DEP doesn't provide recommendations or referrals, you can find a list of currently certified asbestos investigators on DEP's website. Find a list of currently certified asbestos investigators. You also can find a full list of active asbestos contractors on the NY State Department of Labor's website.\n",
            "‚≠ê Credibility: ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (84.4/100)\n",
            "üìù Why: +12: uses HTTPS; +14: institutional TLD (gov); -6: no clear author/byline; 0: no publication date found; +10: provides references (links: 30, external: 29); +2: moderate article length\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2065920103.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_chatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-979801452.py\u001b[0m in \u001b[0;36mrun_chatbot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nYou: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"bye\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING FUNCTIONS (NO NEED TO RUN):"
      ],
      "metadata": {
        "id": "0iVv_O3-E7Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title\n",
        "# #Testing URL hybrind scoring function:\n",
        "# # def hybrid_score(url: str, alpha: float = 0.6)\n",
        "\n",
        "# # hybrid_score(\"https://www.health.harvard.edu/staying-healthy/what-is-intermittent-fasting\")\n",
        "\n",
        "# # ---------- Demo on a few URLs ----------\n",
        "# demo_urls = [\n",
        "#     \"https://www.cdc.gov/flu/about/index.html\",\n",
        "#     \"https://www.mayoclinic.org/diseases-conditions/dehydration/symptoms-causes/syc-20354086\",\n",
        "#     \"https://www.healthline.com/nutrition/green-tea-and-weight-loss\",\n",
        "#     \"https://www.reddit.com/r/icecreamery/comments/19elt19/looking_for_resources_to_learn_how_to_make_ice/\",\n",
        "#     \"https://www.buzzfeed.com/\",\n",
        "#     \"https://www.cnn.com/2023/07/16/business/ice-cream-consumption\",\n",
        "#     \"https://www.idfa.org/the-history-of-ice-cream\",\n",
        "#     \"https://pmc.ncbi.nlm.nih.gov/articles/PMC12261055/\",\n",
        "#     \"https://www.britannica.com/science/photosynthesis\",\n",
        "#     \"https://www.health.harvard.edu/staying-healthy/what-is-intermittent-fasting\",\n",
        "#     \"https://www.cheryls.com/articles/food-facts/facts-about-ice-cream?srsltid=AfmBOopmqWKbaUMsJs1YCI0XWNTyAU7otmNF9qtr_0CmT9nuQBDLmc4v\"\n",
        "\n",
        "# ]\n",
        "\n",
        "# for u in demo_urls:\n",
        "#     try:\n",
        "#         res = hybrid_score(u, alpha=0.6)  # 60% rules / 40% ML\n",
        "#         if res is None:  # STRICT EXCLUDE\n",
        "#             continue\n",
        "#         print(\"\\n\" + \"‚Äî\"*70)\n",
        "#         print(f\"üîó {res['url']}\")\n",
        "#         print(f\"üì∞ {res['title'] or '[No Title]'}\")\n",
        "#         print(f\"‚≠ê {res['stars']}  ({res['hybrid_score']}/100)  |  Rule: {res['rule_score']}  ML: {res['ml_score']}\")\n",
        "#         print(f\"üìù Why: {res['explanation']}\")\n",
        "#     except Exception:\n",
        "#         # For the demo,\n",
        "#         print ( [u] + \" - some error\")\n",
        "#         continue"
      ],
      "metadata": {
        "id": "kCTCkeMf_IT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title\n",
        "# import requests\n",
        "# url = \"https://www.ice.edu/blog/scoop-getting-know-ice-cream\"\n",
        "# r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=12)\n",
        "# print(r.status_code, r.headers.get(\"Content-Type\"))\n",
        "# print(r.text[:500])"
      ],
      "metadata": {
        "id": "8ydpfF3XZm-z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title\n",
        "# # ============================================================\n",
        "# # DELIVERABLE_2: Simple manual test on 5 ice cream URLs\n",
        "# # ============================================================\n",
        "\n",
        "# test_links = [\n",
        "#     \"https://en.wikipedia.org/wiki/Ice_cream\",\n",
        "#     \"https://www.dreamscoops.com/ice-cream-science/\",\n",
        "#     \"https://www.mymochi.com/blog/fun-facts-about-ice-cream-that-you-didnt-know/\",\n",
        "#     \"https://www.idfa.org/the-history-of-ice-cream\",\n",
        "#     \"https://www.ice.edu/blog/scoop-getting-know-ice-cream\",\n",
        "# ]\n",
        "\n",
        "# print(\"üß™ Running hybrid credibility scoring test on 5 URLs...\\n\")\n",
        "\n",
        "# for url in test_links:\n",
        "#     try:\n",
        "#         result = hybrid_score(url, alpha=0.6)  # uses rule + ML\n",
        "#         print(\"‚Äî\" * 70)\n",
        "#         print(f\"üîó {url}\")\n",
        "#         print(f\"‚≠ê {result['stars']}  ({result['hybrid_score']}/100)\")\n",
        "#         print(f\"   ‚Ä¢ Rule score: {result['rule_score']}\")\n",
        "#         print(f\"   ‚Ä¢ ML score:   {result['ml_score']}\")\n",
        "#         print(f\"üìù Why: {result['explanation']}\\n\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"‚ö†Ô∏è Error scoring {url}: {e}\")\n",
        "\n",
        "# print(\"‚úÖ Test complete!\")"
      ],
      "metadata": {
        "id": "oZ5o8JtYYLAa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}